{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Functionizaled Historical Tweets Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: All Libraries from the top\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import socket\n",
    "import sys\n",
    "import errno\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Libraries for the getting the latest file\n",
    "import glob\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Libraries to get the time of when the batch is imported \n",
    "#https://www.programiz.com/python-programming/datetime/current-datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/johnc.burns/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/johnc.burns/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Part 5: English Analysis Libraries needed:\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentianalyzer = SentimentIntensityAnalyzer()\n",
    "#This time we will add the individual sentiment to the tweet\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Google Maps API\n",
    "import googlemaps\n",
    "gmaps = googlemaps.Client(key = \"AIzaSyDntbz0oR6KLfWl_BYZj1LCTS_iQUWv1Cg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 6: Spanish Anaysis Libraries Needed:\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 7: French Libraries Needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 9: Arabic Libraries Needed\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 10: Japanese Libraries Needed\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 11: Korean Libraries Needed\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import csv\n",
    "#Import konlpy\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.utils import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 12: Library needed to append the new data file to the existing csv\n",
    "from csv import writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 13: Library Create a map with Folium\n",
    "#http://python-visualization.github.io/folium/quickstart.html#Getting-Started\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 14: Library needed to translate the locations to English\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0.5: Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_json_df = pd.read_json(r\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Tweets_Full/Full_File/Full_one.json\")\n",
    "#full_json_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_s2 = full_json_df.reset_index(drop = True)\n",
    "#df_s2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_json_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_json_df[\"cat\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_json_df[\"tag\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_json_df[\"scale\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_json_df[\"lang\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_json_df[\"created_at\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_json_df[\"text\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixes for the full file\n",
    "#fjd_fix = full_json_df[full_json_df[\"tag\"].isnull()]\n",
    "#fjd_fix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fjd_fix.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fjd_fix2 = fjd_fix.reset_index(drop = True)\n",
    "#fjd_fix2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fjd_fix2[\"cat\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the tag variable\n",
    "#fjd_fix2[\"tag2\"] = \"GN\"\n",
    "#fjd_fix2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop tag column\n",
    "#fjd_fix3 = fjd_fix2.drop(\"tag\", axis = 1)\n",
    "#fjd_fix3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "#fjd_fix4 = fjd_fix3.rename(columns = {\"tag2\": \"tag\"})\n",
    "#fjd_fix4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the nas\n",
    "#full_2 = full_json_df.dropna()\n",
    "#full_3 = full_2.reset_index(drop = True)\n",
    "#full_3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the fixed dataframe\n",
    "#full_4 = full_3.append(fjd_fix4)\n",
    "#full_5 = full_4.reset_index(drop = True)\n",
    "#full_5.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take random sample for testing\n",
    "#df_sample = full_json_df.sample(frac = 0.0001, replace = True, random_state = 1)\n",
    "#df_sample.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_s2 = df_sample.reset_index()\n",
    "#df_s2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_s3 = df_s2.dropna()\n",
    "#df_s4 = df_s3.reset_index()\n",
    "#df_s4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: The constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2.5: Import the English BERT Model\n",
    "#Multilingual BERT model, had 80.8% accuracy on Full Twitter Data\n",
    "#multi_sa_en = pipeline(model= \"cardiffnlp/twitter-xlm-roberta-base-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, None, 768)         116768000 \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 128)         98432     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 128)         0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 64)          37248     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 64)          0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 32)                9408      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116,913,121\n",
      "Trainable params: 145,121\n",
      "Non-trainable params: 116,768,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Part 3: Import RNN Spanish Model #GET CORRECT SPANISH MODEL\n",
    "RNN_sp_model = joblib.load('/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/RNN_Spanish_2tass.joblib')\n",
    "RNN_sp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, None, 768)         116768000 \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 128)         98432     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 128)         0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 128)               99072     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116,965,633\n",
      "Trainable params: 197,633\n",
      "Non-trainable params: 116,768,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Part 4: Import the French RNN Model\n",
    "RNN_fr_model = joblib.load('/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/RNN_French_1.joblib')\n",
    "RNN_fr_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, None, 768)         116768000 \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 128)         98432     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 128)         0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 64)          37248     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 64)          0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, None, 64)          24960     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, 64)          0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 32)                9408      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116,938,081\n",
      "Trainable params: 170,081\n",
      "Non-trainable params: 116,768,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Part 5: Import Portuguese RNN Model\n",
    "RNN_pt_model = joblib.load('/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/RNN_pt_2.joblib')\n",
    "RNN_pt_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 6: Import the Arabic Sentiment Analysis Hugging Face Model\n",
    "#https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment\n",
    "sa_ar = pipeline(\"sentiment-analysis\", model = 'CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 7: NER Arabic Hugging Face Model\n",
    "#https://huggingface.co/hatmimoha/arabic-ner\n",
    "#https://github.com/hatmimoha/arabic-ner\n",
    "nlp_ar_2 = pipeline(\"ner\", model = 'hatmimoha/arabic-ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, None, 768)         116768000 \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 256)         196864    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 256)         0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 256)        296448    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 256)              296448    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,558,017\n",
      "Trainable params: 790,017\n",
      "Non-trainable params: 116,768,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Part 8: Import RNN Japanese Model\n",
    "RNN_ja_model = joblib.load('/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/RNN_Japanese_1.joblib')\n",
    "RNN_ja_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, None, 768)         116768000 \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 128)         98432     \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 128)         99072     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 128)         0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, None, 64)          37248     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 64)          0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 32)                9408      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,012,193\n",
      "Trainable params: 244,193\n",
      "Non-trainable params: 116,768,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Step 9: Import RNN Korean Model\n",
    "RNN_ko_model = joblib.load('/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 7/Twitter Language Samples/Korean/Korean RNN Test/RNN_ko.joblib')\n",
    "RNN_ko_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 9: Import Korean Stop Words:\n",
    "#ko_stopwords = pd.read_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/stopwords-ko.txt\", delimiter = \"\\t\", quoting=csv.QUOTE_NONE, encoding = \"utf-8\")\n",
    "#ko_stopwords.columns = [\"word\"]\n",
    "#ko_stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>alpha2</th>\n",
       "      <th>alpha3</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>288</td>\n",
       "      <td>가나</td>\n",
       "      <td>gh</td>\n",
       "      <td>gha</td>\n",
       "      <td>['가나']</td>\n",
       "      <td>가나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>266</td>\n",
       "      <td>가봉</td>\n",
       "      <td>ga</td>\n",
       "      <td>gab</td>\n",
       "      <td>['가', '##봉']</td>\n",
       "      <td>가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>328</td>\n",
       "      <td>가이아나</td>\n",
       "      <td>gy</td>\n",
       "      <td>guy</td>\n",
       "      <td>['가이', '##아나']</td>\n",
       "      <td>가이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>270</td>\n",
       "      <td>감비아</td>\n",
       "      <td>gm</td>\n",
       "      <td>gmb</td>\n",
       "      <td>['감', '##비아']</td>\n",
       "      <td>감</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>831</td>\n",
       "      <td>건지섬</td>\n",
       "      <td>gg</td>\n",
       "      <td>ggy</td>\n",
       "      <td>['건지', '##섬']</td>\n",
       "      <td>건지</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   id  name alpha2 alpha3       tokenized token\n",
       "0           0  288    가나     gh    gha          ['가나']    가나\n",
       "1           1  266    가봉     ga    gab    ['가', '##봉']     가\n",
       "2           2  328  가이아나     gy    guy  ['가이', '##아나']    가이\n",
       "3           3  270   감비아     gm    gmb   ['감', '##비아']     감\n",
       "4           4  831   건지섬     gg    ggy   ['건지', '##섬']    건지"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part 10: Import the Korean World Data Set\n",
    "df_ko_world = pd.read_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/Ko_World_Token.csv\")\n",
    "df_ko_world.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 11: Import the Korean MNB Token List #CAN DELETE WON'T NEED\n",
    "#ko_mnb_tokens = pd.read_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/Korean_MNB_Token2.csv\")\n",
    "#ko_mnb_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 12: Define the okt function from KoNLPy\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 13: Create list of topics\n",
    "topic_list = [\"Goldstein Positive\", \"Goldstein Negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 14: Import Country - Production Excel File\n",
    "#country_product = pd.read_excel(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/continents2.xls\")\n",
    "#country_product_2 = country_product.rename(columns = {\"name\":\"textloc_en\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 15: Import Topic - Industry Excel File\n",
    "#Topic_Product = pd.read_excel(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/Topic_Industry.xlsx\")\n",
    "#Topic_Product_2 = Topic_Product.rename(columns = {\"Topic_Label\":\"tag\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 16: Import the Topic - Financial Data\n",
    "#Topic_Finance = pd.read_excel(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/Topic_Financial.xlsx\")\n",
    "#Topic_Finance_2 = Topic_Finance.rename(columns = {\"Topic_Label\":\"Topics\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 17: Counter Token\n",
    "#counter_2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 11: Create the Date Function\n",
    "def date_func(df):\n",
    "    df[\"Date\"] = \"\"\n",
    "    lendf = len(df)\n",
    "    for i in range(lendf):\n",
    "        df[\"Date\"][i] = str(df[\"created_at\"][i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 12: Create the Year Function\n",
    "def year_func(df):\n",
    "    df[\"Year\"] = \"\"\n",
    "    lendf = len(df)\n",
    "    for i in range(lendf):\n",
    "        df[\"Year\"][i] = int(df[\"Date\"][i][0:4]) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 13: Create the Month Function\n",
    "def month_func(df):\n",
    "    df[\"Month\"] = \"\"\n",
    "    lendf = len(df)\n",
    "    for i in range(lendf):\n",
    "        df[\"Month\"][i] = int(df[\"Date\"][i][5:7])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 14: Create the Day Function\n",
    "def day_func(df):\n",
    "    df[\"Day\"] = \"\"\n",
    "    lendf = len(df)\n",
    "    for i in range(lendf):\n",
    "        df[\"Day\"][i] = int(df[\"Date\"][i][8:10])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 17.5: Data Combos Function\n",
    "def date_combos(df):\n",
    "    #df[\"Year_Month_Day\"] = 0\n",
    "    df[\"Month_Day\"] = 0\n",
    "    #df[\"Day_Hour\"] = 0\n",
    "    #df[\"Month_Day_Hour\"] = 0 \n",
    "    lendf = len(df)\n",
    "    for i in range(0, lendf):\n",
    "        #df[\"Year_Month_Day\"][i] = ((((df[\"Year\"][i]))*365) + ((((df[\"Month\"]))*30) - 1) + ((df[\"Day\"][i])))\n",
    "        df[\"Month_Day\"][i] = (((df[\"Month\"][i])*30) + df[\"Day\"][i])\n",
    "        #df[\"Day_Hour\"][i] = (((df[\"Day\"][i])*24) + df[\"Hour\"][i])\n",
    "        #df[\"Month_Day_Hour\"][i] = (((df[\"Month\"][i])*730) + ((df[\"Day\"][i])*24) + (df[\"Hour\"][i]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 18: Sort Chronologically Function Sort by the Year, Month, Day, Hour, Minute, Second\n",
    "def sort_chrono(df):\n",
    "    df2 = df.sort_values(by = ['Year', 'Month', 'Day'], ascending = ['False', 'False', 'False'], na_position = 'first')\n",
    "    df3 = df2.reset_index(drop = True)\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 6: Break into English Tweets\n",
    "def English_Tweets_Func(df_gt):\n",
    "    English_Tweets = df_gt[df_gt['lang'] == \"en\"]\n",
    "    English_Tweets.reset_index(drop = True, inplace = True)\n",
    "    English_Tweets['TweetNumber'] = np.arange(len(English_Tweets))\n",
    "    return English_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 7: Break into Spanish Tweets\n",
    "def Spanish_Tweets_Func(df_gt):\n",
    "    Spanish_Tweets = df_gt[df_gt['lang'] == \"es\"]\n",
    "    Spanish_Tweets.reset_index(drop = True, inplace = True)\n",
    "    Spanish_Tweets['TweetNumber'] = np.arange(len(Spanish_Tweets))\n",
    "    return Spanish_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 8: Break into French Tweets\n",
    "def French_Tweets_Func(df_gt):\n",
    "    French_Tweets = df_gt[df_gt['lang'] == \"fr\"]\n",
    "    French_Tweets.reset_index(drop = True, inplace = True)\n",
    "    French_Tweets['TweetNumber'] = np.arange(len(French_Tweets))\n",
    "    return French_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 9: Break into Portuguese Tweets\n",
    "def Portuguese_Tweets_Func(df_gt):\n",
    "    Portuguese_Tweets = df_gt[df_gt['lang'] == \"pt\"]\n",
    "    Portuguese_Tweets.reset_index(drop = True, inplace = True)\n",
    "    Portuguese_Tweets['TweetNumber'] = np.arange(len(Portuguese_Tweets))\n",
    "    return Portuguese_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 10: Break into Arabic Tweets\n",
    "def Arabic_Tweets_Func(df_gt):\n",
    "    Arabic_Tweets = df_gt[df_gt['lang'] == \"ar\"]\n",
    "    Arabic_Tweets.reset_index(drop = True, inplace = True)\n",
    "    Arabic_Tweets['TweetNumber'] = np.arange(len(Arabic_Tweets))\n",
    "    return Arabic_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 11: Break into Japanese Tweets\n",
    "def Japanese_Tweets_Func(df_gt):\n",
    "    Japanese_Tweets = df_gt[df_gt['lang'] == \"ja\"]\n",
    "    Japanese_Tweets.reset_index(drop = True, inplace = True)\n",
    "    Japanese_Tweets['TweetNumber'] = np.arange(len(Japanese_Tweets)) \n",
    "    return Japanese_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 12: Break into Korean Tweets\n",
    "def Korean_Tweets_Func(df_gt):\n",
    "    Korean_Tweets = df_gt[df_gt['lang'] == \"ko\"]\n",
    "    Korean_Tweets.reset_index(drop = True, inplace = True)\n",
    "    Korean_Tweets['TweetNumber'] = np.arange(len(Korean_Tweets))\n",
    "    return Korean_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_analysis(df):\n",
    "    #Length of the English Tweets\n",
    "    LenEnT = len(df.index)\n",
    "    LenEnT\n",
    "\n",
    "    #Create a new dataframe\n",
    "    EnTCompound = pd.DataFrame([])\n",
    "    EnTSentence = pd.DataFrame([])\n",
    "\n",
    "    #This loop takes the contents from each article, tokenizes each sentence in the article, counts the number of \n",
    "    #sentences in the article and then gets the sentiments of each sentence and exports those sentiments to a dataframe\n",
    "    for i in np.arange(0, LenEnT):\n",
    "        dtestent = df[\"text\"][i]\n",
    "        dictionary = sentianalyzer.polarity_scores(dtestent)\n",
    "        c1 = dictionary.get('compound')\n",
    "        EnTCompound = EnTCompound.append(pd.DataFrame({'Compound': c1}, index = [0]), ignore_index = True)\n",
    "    \n",
    "    #Add the Sentiment to the DataFrame\n",
    "    frames = [df, EnTCompound]\n",
    "    finalent = pd.concat(frames, axis = 1)\n",
    "    \n",
    "    #Normalize the Sentiment\n",
    "    #1. Conditions\n",
    "    conditions_eng = [\n",
    "        (finalent['Compound'] <= -0.25),\n",
    "        (finalent['Compound'] > -0.25) & (finalent['Compound'] < 0.25),\n",
    "        (finalent['Compound'] >= 0.25)\n",
    "    ]\n",
    "    \n",
    "    #2. Values\n",
    "    values_eng = [-1, 0, 1]\n",
    "    \n",
    "    #3. New Variable\n",
    "    finalent['Sentiment'] = np.select(conditions_eng, values_eng)\n",
    "    \n",
    "    #Matching Patterns for countries for GeoCoding\n",
    "    lenentw = len(finalent[\"text\"])\n",
    "\n",
    "    #Create a new dataframe\n",
    "    ettext = pd.DataFrame([])\n",
    "    etlabel = pd.DataFrame([])\n",
    "    ettweet = pd.DataFrame([])\n",
    "\n",
    "    #Initialize the NLP in Spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for i in np.arange(0, lenentw):\n",
    "        etspacy = finalent[\"text\"][i]\n",
    "        doc = nlp(etspacy)\n",
    "        for ent in doc.ents:\n",
    "            ettext = ettext.append(pd.DataFrame({'textloc': ent.text}, index = [0]), ignore_index = True)\n",
    "            etlabel = etlabel.append(pd.DataFrame({'label': ent.label_}, index = [0]), ignore_index = True)\n",
    "            ettweet = ettweet.append(pd.DataFrame({'TweetNumber': [i]}, index = [0]), ignore_index = True)\n",
    "    \n",
    "    #Combining the Spacy Findings\n",
    "    frames_spacy = [ettweet, ettext, etlabel]\n",
    "    finalent_spacy = pd.concat(frames_spacy, axis = 1)\n",
    "    \n",
    "    #Keep only the GPE entities\n",
    "    gpedf = finalent_spacy[finalent_spacy['label'] == \"GPE\"]\n",
    "    \n",
    "    #Left Merge Back the GPE labels to the tweets\n",
    "    etlocations = pd.merge(finalent, gpedf, on = \"TweetNumber\", how = \"left\")\n",
    "\n",
    "    #Left Merge The Lat and Long into to the tweets\n",
    "    etcoord = etlocations\n",
    "    return etcoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 14: Spanish Convert Sentiment Results to -1, 1 \n",
    "#CHANGE THE CONVERSION RULES\n",
    "def convert_sp(df, i):\n",
    "    if (df[\"results\"][i][0]) > 0.935:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 15: THIS IS THE SPANISH ANALYSIS FUNCTION\n",
    "#https://stackoverflow.com/questions/38895768/python-pandas-dataframe-is-it-pass-by-value-or-pass-by-reference\n",
    "def spanish_analysis(df):\n",
    "    \n",
    "    #Part 1: Sentiment Analysis with Spanish RNN\n",
    "    \n",
    "    #Step 1: Transform the pandas data frame into a tensor frame dataframe for text\n",
    "    \n",
    "    twsp_ran = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "    \n",
    "        (\n",
    "            tf.cast(df['text'].values, tf.string),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    #Step 2: Batch the data\n",
    "    batch_size = 2\n",
    "    twsp_ran_set = twsp_ran.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    #Step 3: Predict the Sentiment for the data\n",
    "    twsp_predict = RNN_sp_model.predict(twsp_ran_set)\n",
    "    \n",
    "    #Step 4: Append the predictions to the pandas dataframe \n",
    "    df[\"results\"] = twsp_predict.tolist()\n",
    "    \n",
    "    #Step 5: Convert Results to -1, 1\n",
    "    lendtt = len(df)\n",
    "    df[\"Sentiment\"] = 0\n",
    "    for i in range(0, lendtt):\n",
    "        df[\"Sentiment\"][i] = convert_sp(df, i)\n",
    "    \n",
    "    \n",
    "    #Part 2: Get the NER, Location Text, and Coordinates \n",
    "    \n",
    "    #Get the length of the finalspt dataframe\n",
    "    lensptw = len(df[\"text\"])\n",
    "\n",
    "    #First: Create the new dataframes to capture the different NER parts\n",
    "    spttext = pd.DataFrame([])\n",
    "    sptlabel = pd.DataFrame([])\n",
    "    spttweet = pd.DataFrame([])\n",
    "\n",
    "    #Second: Initialize the NLP in Spacy\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    for i in np.arange(0, lensptw):\n",
    "        sptspacy = df[\"text\"][i]\n",
    "        doc_sp = nlp(sptspacy)\n",
    "        for ent in doc_sp.ents:\n",
    "            spttext = spttext.append(pd.DataFrame({'textloc': ent.text}, index = [0]), ignore_index = True)\n",
    "            sptlabel = sptlabel.append(pd.DataFrame({'label': ent.label_}, index = [0]), ignore_index = True)\n",
    "            spttweet = spttweet.append(pd.DataFrame({'TweetNumber': [i]}, index = [0]), ignore_index = True)\n",
    "    \n",
    "    #Third: Combining the Spacy Spanish Findings\n",
    "    frames_spacy_sp = [spttweet, spttext, sptlabel]\n",
    "    finalent_spacy_sp = pd.concat(frames_spacy_sp, axis = 1)\n",
    "    \n",
    "    #Fourth: Keep only the GPE parts \n",
    "    gpedf_sp = finalent_spacy_sp[finalent_spacy_sp['label'] == \"LOC\"]\n",
    "    \n",
    "    #Fifth: Merge the GPE on the Main Data Frame\n",
    "    sptlocations = pd.merge(df, gpedf_sp, on = \"TweetNumber\", how = \"left\")\n",
    "    \n",
    "    #Left Merge The Lat and Long into to the tweets\n",
    "    sptcoord = sptlocations\n",
    "    return sptcoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 16: French Convert Results to -1, 1\n",
    "def convert_fr(df, i):\n",
    "    if (df[\"results\"][i][0]) > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 17: French Analysis Function Update\n",
    "def french_analysis(df):\n",
    "    \n",
    "    #Part 1: Sentiment Analysis\n",
    "    \n",
    "    #Step 1: Transform the pandas data frame into a tensor frame dataframe for text\n",
    "    twfr_ran = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "    \n",
    "        (\n",
    "            tf.cast(df['text'].values, tf.string),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    #Step 2: Batch the data, hopefully\n",
    "    batch_size = 128\n",
    "    twfr_ran_set = twfr_ran.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    #Step 3: Predict the Sentiment for the data\n",
    "    twfr_predict = RNN_fr_model.predict(twfr_ran_set)\n",
    "    \n",
    "    #Step 4: Append the predictions to the pandas dataframe \n",
    "    df[\"results\"] = twfr_predict.tolist()\n",
    "    \n",
    "    #Step 5: Convert Results to -1, 1\n",
    "    lendfr = len(df)\n",
    "    df[\"Sentiment\"] = 0\n",
    "    for i in range(0, lendfr):\n",
    "        df[\"Sentiment\"][i] = convert_fr(df, i)\n",
    "    \n",
    "    #Part 2: Get the NER, Location Text, and Coordinates, \n",
    "    \n",
    "    #Step 4: Put the Index Number as Tweet Number\n",
    "    #https://stackoverflow.com/questions/20461165/how-to-convert-index-of-a-pandas-dataframe-into-a-column\n",
    "    df[\"TweetNumber\"] = df.index\n",
    "    \n",
    "    #Step 5: NER Spacy for French \n",
    "    lendfr2 = len(df[\"text\"])\n",
    "\n",
    "    #Create a new dataframe\n",
    "    frtext = pd.DataFrame([])\n",
    "    frlabel = pd.DataFrame([])\n",
    "    frtweet = pd.DataFrame([])\n",
    "\n",
    "    #Initialize the NLP in Spacy\n",
    "    nlp_fr_sp = spacy.load(\"fr_core_news_sm\")\n",
    "    for i in np.arange(0, lendfr2):\n",
    "        frspacy = df[\"text\"][i]\n",
    "        doc = nlp_fr_sp(frspacy)\n",
    "        for ent in doc.ents:\n",
    "            frtext = frtext.append(pd.DataFrame({'textloc': ent.text}, index = [0]), ignore_index = True)\n",
    "            frlabel = frlabel.append(pd.DataFrame({'label': ent.label_}, index = [0]), ignore_index = True)\n",
    "            frtweet = frtweet.append(pd.DataFrame({'TweetNumber': [i]}, index = [0]), ignore_index = True)\n",
    "    \n",
    "    #Step 6: Combining the Spacy Findings\n",
    "    frames_spacy_fr = [frtweet, frtext, frlabel]\n",
    "    finalent_spacy_fr = pd.concat(frames_spacy_fr, axis = 1)\n",
    "    \n",
    "    #Step 7: Keep only the LOC entities\n",
    "    gpedf_fr = finalent_spacy_fr[finalent_spacy_fr['label'] == \"LOC\"]\n",
    "    \n",
    "    #Step 8: Left Merge Back the GPE labels to the tweets\n",
    "    frlocations = pd.merge(df, gpedf_fr, on = \"TweetNumber\", how = \"left\")\n",
    "    \n",
    "    #Step 12: Left Merge The Lat and Long into to the tweets\n",
    "    frcoord = frlocations \n",
    "    return frcoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 18: Portuguese Convert Sentiment Results to 0, 1\n",
    "def convert_pt_rnn(df, i):\n",
    "    if df[\"results\"][i][0] > 0.8:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 19: Main Portuguese Function\n",
    "def portuguese_analysis(df):\n",
    "    \n",
    "    #Part 1: Sentiment Analysis\n",
    "    #Step 1: Transform the pandas data frame into a tensor frame dataframe for text\n",
    "    twpt_ran = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "    \n",
    "        (\n",
    "            tf.cast(df['text'].values, tf.string),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    #Step 2: Batch the data, hopefully\n",
    "    batch_size = 4\n",
    "    twpt_ran_set = twpt_ran.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    #Step 3: Predict the Sentiment for the data\n",
    "    twpt_predict = RNN_pt_model.predict(twpt_ran_set)\n",
    "    \n",
    "    #Step 4: Append the predictions to the pandas dataframe \n",
    "    df[\"results\"] = twpt_predict.tolist()\n",
    "    \n",
    "    #Step 5: Convert Results to 0, 1\n",
    "    lendtt = len(df)\n",
    "    df[\"Sentiment\"] = 0\n",
    "    for i in range(0, lendtt):\n",
    "        df[\"Sentiment\"][i] = convert_pt_rnn(df, i)\n",
    "    \n",
    "    #Part 2: Named Entity Recognition and Geocoding for Coordinates\n",
    "    \n",
    "    #Step 1: Length of Dataframe\n",
    "    lenpttw = len(df[\"text\"])\n",
    "\n",
    "    #Step 2: Create a new dataframe for Appending\n",
    "    pttext = pd.DataFrame([])\n",
    "    ptlabel = pd.DataFrame([])\n",
    "    pttweet = pd.DataFrame([])\n",
    "\n",
    "    #Step 3: Initialize the NLP in Spacy\n",
    "    nlp_pt = spacy.load(\"pt_core_news_sm\")\n",
    "    for i in np.arange(0, lenpttw):\n",
    "        ptspacy = df[\"text\"][i]\n",
    "        doc = nlp_pt(ptspacy)\n",
    "        for ent in doc.ents:\n",
    "            pttext = pttext.append(pd.DataFrame({'textloc': ent.text}, index = [0]), ignore_index = True)\n",
    "            ptlabel = ptlabel.append(pd.DataFrame({'label': ent.label_}, index = [0]), ignore_index = True)\n",
    "            pttweet = pttweet.append(pd.DataFrame({'TweetNumber': [i]}, index = [0]), ignore_index = True)\n",
    "            \n",
    "    #Step 4: Combining the Spacy Findings\n",
    "    frames_spacy_pt = [pttweet, pttext, ptlabel]\n",
    "    finalpt_spacy = pd.concat(frames_spacy_pt, axis = 1)\n",
    "    \n",
    "    #Step 5: Keep only the LOC entities\n",
    "    gpedf_pt = finalpt_spacy[finalpt_spacy['label'] == \"LOC\"]\n",
    "    \n",
    "    #Step 6: Left Merge Back the GPE labels to the tweets\n",
    "    ptlocations = pd.merge(df, gpedf_pt, on = \"TweetNumber\", how = \"left\")\n",
    "    \n",
    "    #Step 10: Left Merge The Lat and Long into to the tweets\n",
    "    ptcoord = ptlocations\n",
    "    return ptcoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 20: Arabic Convert Sentiment Results to -1, 0, 1\n",
    "def convert_ar(df, i):\n",
    "    if df[\"Sentiment_label\"][i] == \"positive\":\n",
    "        return 1\n",
    "    elif df[\"Sentiment_label\"][i] == \"negative\":\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 21: Main Arabic Analysis Function\n",
    "def arabic_analysis(df):\n",
    "    \n",
    "    #Part 1: Sentiment Analysis\n",
    "    \n",
    "    #Step 1: Use the tranformer to get the sentiment \n",
    "    lendar2 = len(df)\n",
    "    df[\"Sentiment_label\"] = \"\"\n",
    "    for i in range(0, lendar2):\n",
    "        ar_sa_test = sa_ar(df[\"text\"][i])\n",
    "        df[\"Sentiment_label\"][i] = ar_sa_test[0]['label']\n",
    "    \n",
    "    #Step 2: Convert the Results\n",
    "    df[\"Sentiment\"] = 0\n",
    "    for i in range(0, lendar2):\n",
    "        df[\"Sentiment\"][i] = convert_ar(df, i)\n",
    "            \n",
    "    #Part 2: Use the Second Tranformer for the NER Locations\n",
    "    \n",
    "    #Step 1: Get the list of locations named in tweets\n",
    "    df[\"Location\"] = \"\"\n",
    "    lennat2 = len(df)\n",
    "    for i in range(0, lennat2):\n",
    "        named_test1 = nlp_ar_2(df[\"text\"][i])\n",
    "        if not named_test1:\n",
    "            df[\"Location\"][i] = \"---\"\n",
    "        else:\n",
    "            entity_list = []\n",
    "            word_list = []\n",
    "            for j in range(0, len(named_test1)):\n",
    "                entity_list.append(named_test1[j][\"entity\"])\n",
    "                word_list.append(named_test1[j][\"word\"])\n",
    "                entity_index = [m for m, x in enumerate(entity_list) if x == \"LABEL_4\"]\n",
    "                location_list = [word_list[k] for k in entity_index]\n",
    "            df[\"Location\"][i] = location_list\n",
    "            \n",
    "    #Step 2: Get a Single Location\n",
    "    #This might be changed in the future if I can make multiple copies of a tweet for each location\n",
    "    df[\"textloc\"] = \"\"\n",
    "    for i in range(0, len(df[\"textloc\"])):\n",
    "        if not df[\"Location\"][i]:\n",
    "            df[\"textloc\"][i] = \"---\"\n",
    "        else:\n",
    "            single_loc = df[\"Location\"][i][0]\n",
    "            df[\"textloc\"][i] = single_loc\n",
    "            \n",
    "    #Step 3: Drop the All Locations\n",
    "    df2 = df.drop([\"Location\"], axis = 1)\n",
    "    \n",
    "    #Step 4: Reform the data\n",
    "    arcoord = df2.reset_index(drop = True)\n",
    "    return arcoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 22: Convert Japanese Sentiment Analysis Results to 0, 1\n",
    "def convert_japanese(df, i):\n",
    "    if (df[\"results\"][i][0]) > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 23: The Main Japanese Function\n",
    "def japanese_analysis(df):\n",
    "    \n",
    "    #Part 1: Sentiment Analysis\n",
    "    \n",
    "    #Step 1: Transform the pandas data frame into a tensor frame dataframe for text\n",
    "    twja_ran = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "    \n",
    "        (\n",
    "            tf.cast(df['text'].values, tf.string),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    #Step 2: Batch the data, hopefully\n",
    "    batch_size = 32\n",
    "    twja_ran_set = twja_ran.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    #Step 3: Predict the Sentiment for the data\n",
    "    twja_predict = RNN_ja_model.predict(twja_ran_set)\n",
    "    \n",
    "    #Step 4: Append the predictions to the pandas dataframe \n",
    "    df[\"results\"] = twja_predict.tolist()\n",
    "    \n",
    "    #Step 5: Convert Results to -1, 0, 1\n",
    "    lendttja = len(df)\n",
    "    df[\"Sentiment\"] = 0\n",
    "    for i in range(0, lendttja):\n",
    "        df[\"Sentiment\"][i] = convert_japanese(df, i)\n",
    "            \n",
    "    #Part 2: Named Entity Recognition Japanese\n",
    "    \n",
    "    #Step 1: Length of Dataframe\n",
    "    lenjatw = len(df[\"text\"])\n",
    "    \n",
    "    #Step 2: Create a new dataframe for Appending\n",
    "    jatext = pd.DataFrame([])\n",
    "    jalabel = pd.DataFrame([])\n",
    "    jatweet = pd.DataFrame([])\n",
    "    \n",
    "    #Step 3: Initialize the NLP in Spacy\n",
    "    nlp_ja = spacy.load(\"ja_core_news_sm\")\n",
    "    for i in np.arange(0, lenjatw):\n",
    "        jaspacy = df[\"text\"][i]\n",
    "        doc = nlp_ja(jaspacy)\n",
    "        if not doc.ents:\n",
    "            jatext = jatext.append(pd.DataFrame({'textloc': np.nan}, index = [0]), ignore_index = True)\n",
    "            jalabel = jalabel.append(pd.DataFrame({'label': np.nan}, index = [0]), ignore_index = True)\n",
    "            jatweet = jatweet.append(pd.DataFrame({'TweetNumber': [i]}, index = [0]), ignore_index = True)\n",
    "        else:\n",
    "            for ent in doc.ents:\n",
    "                jatext = jatext.append(pd.DataFrame({'textloc': ent.text}, index = [0]), ignore_index = True)\n",
    "                jalabel = jalabel.append(pd.DataFrame({'label': ent.label_}, index = [0]), ignore_index = True)\n",
    "                jatweet = jatweet.append(pd.DataFrame({'TweetNumber': [i]}, index = [0]), ignore_index = True)\n",
    "            \n",
    "    #Step 4: Combining the Spacy Findings\n",
    "    frames_spacy_ja = [jatweet, jatext, jalabel]\n",
    "    finalja_spacy = pd.concat(frames_spacy_ja, axis = 1)\n",
    "    \n",
    "    #Step 5: Keep only the GPE entities\n",
    "    gpedf_ja = finalja_spacy[finalja_spacy[\"label\"] == \"GPE\"]\n",
    "    \n",
    "    #Step 6: Left Merge Back the GPE labels to the tweets\n",
    "    jalocations = pd.merge(df, gpedf_ja, on = \"TweetNumber\", how = \"left\")\n",
    "    jacoord = jalocations\n",
    "    return jacoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4.5: Convert Results to 0, 1\n",
    "def convert_korean(df, i):\n",
    "    if (df[\"results\"][i][0]) > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 25: Korean Analysis Function\n",
    "def korean_analysis(df):\n",
    "\n",
    "    #Part 1: Sentiment Analysis with Korean RNN\n",
    "    \n",
    "    #Step 1: Transform the pandas data frame into a tensor frame dataframe for text\n",
    "    twko_ran = (\n",
    "        tf.data.Dataset.from_tensor_slices(\n",
    "            \n",
    "            (\n",
    "                tf.cast(df['text'].values, tf.string),\n",
    "            )\n",
    "            \n",
    "            )\n",
    "    )\n",
    "    \n",
    "    #Step 2: Batch the data, hopefully\n",
    "    batch_size = 4\n",
    "    twko_ran_set = twko_ran.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    #Step 3: Predict the Sentiment for the data\n",
    "    twko_predict = RNN_ko_model.predict(twko_ran_set)\n",
    "    \n",
    "    #Step 4: Append the predictions to the pandas dataframe \n",
    "    df[\"results\"] = twko_predict.tolist()\n",
    "    \n",
    "    #Step 5: Convert Results to -1, 0, 1\n",
    "    lendttko = len(df[\"text\"])\n",
    "    df[\"Sentiment\"] = 0\n",
    "    for i in range(0, lendttko):\n",
    "        df[\"Sentiment\"][i] = convert_korean(df, i)\n",
    "  \n",
    "    #Part 2: Text Country and Coordinates  from the text\n",
    "    \n",
    "    #Step 1: Create the text_country variable\n",
    "    df[\"textloc\"] = \"\"\n",
    "    lenk2test = len(df[\"textloc\"])\n",
    "    for i in range(0, lenk2test):\n",
    "        try:\n",
    "            #Step 2: Get the text\n",
    "            text_2 = df[\"text\"][i]\n",
    "            #Step 3: Using Konlpy get the tokenization of the text and \n",
    "            #Transform it into a pandas dataframe\n",
    "            kt_df_2 = pd.DataFrame(okt.morphs(text_2, norm = True, stem = True))\n",
    "            kt_df_2.columns = ['name']\n",
    "            #Step 3: Merge the Token DataFrame on the Tokenized Sentiment Lexicon\n",
    "            token_text_country_test = pd.merge(kt_df_2, df_ko_world, on = \"name\", how = \"inner\")\n",
    "            #Step 4: Get the first token and put in as the text country, not perfect but good enough\n",
    "            df[\"textloc\"][i] = token_text_country_test[\"token\"][0]\n",
    "        except IndexError:\n",
    "            df[\"textloc\"][i] = \"\"\n",
    "    \n",
    "    #Step 8: Left Merge The Lat and Long into to the tweets\n",
    "    kocoord = df\n",
    "    return kocoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the Analyzer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 26: The English Analyzer Function\n",
    "def english_df(English_Tweets):\n",
    "    try:\n",
    "        english_test_df = english_analysis(English_Tweets)\n",
    "    except KeyError:\n",
    "        english_test_df = pd.DataFrame()\n",
    "    \n",
    "    return english_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 27: The Spanish Analyzer Function\n",
    "def spanish_df(Spanish_Tweets):\n",
    "    try:\n",
    "        spanish_test_df = spanish_analysis(Spanish_Tweets)\n",
    "    except ValueError:\n",
    "        spanish_test_df = pd.DataFrame()\n",
    "        \n",
    "    return spanish_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 28: The French Analyzer Function\n",
    "def french_df(French_Tweets):\n",
    "    try:\n",
    "        french_test_df = french_analysis(French_Tweets)\n",
    "    except ValueError:\n",
    "        french_test_df = pd.DataFrame()\n",
    "        \n",
    "    return french_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 29: The Portuguese Analyzer Function\n",
    "def portuguese_df(Portuguese_Tweets):\n",
    "    try:\n",
    "        portuguese_test_df = portuguese_analysis(Portuguese_Tweets)\n",
    "    except:\n",
    "        portuguese_test_df = pd.DataFrame()\n",
    "        \n",
    "    return portuguese_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 30: Call the Arabic Analyzer Function\n",
    "def arabic_df(Arabic_Tweets):\n",
    "    try:\n",
    "        arabic_test_df_2 = arabic_analysis(Arabic_Tweets)\n",
    "        arabic_test_df = arabic_test_df_2.loc[:, ~arabic_test_df_2.columns.duplicated()]\n",
    "    except ValueError:\n",
    "        arabic_test_df = pd.DataFrame()\n",
    "\n",
    "    return arabic_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 31: Call the Japanese Analyzer Function\n",
    "def japanese_df(Japanese_Tweets):\n",
    "    try:\n",
    "        japanese_test_df = japanese_analysis(Japanese_Tweets)\n",
    "    except ValueError:\n",
    "        japanese_test_df = pd.DataFrame()\n",
    "    \n",
    "    return japanese_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 32: Call the Korean Analyzer Function\n",
    "def korean_df(Korean_Tweets):\n",
    "    try:\n",
    "        korean_test_df = korean_analysis(Korean_Tweets)\n",
    "    except ValueError:\n",
    "        korean_test_df = pd.DataFrame()\n",
    "        \n",
    "    return korean_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 33: Combine the Data Frames Function:\n",
    "def create_full(english_test_df, spanish_test_df, french_test_df, portuguese_test_df, arabic_test_df, japanese_test_df, korean_test_df):\n",
    "    en_sp_df = english_test_df.append(spanish_test_df)\n",
    "    en_sp_df_2 = en_sp_df.reset_index(drop = True)\n",
    "    en_sp_fr_df = en_sp_df_2.append(french_test_df)\n",
    "    en_sp_fr_df_2 = en_sp_fr_df.reset_index(drop = True)\n",
    "    en_sp_fr_pt_df = en_sp_fr_df_2.append(portuguese_test_df)\n",
    "    en_sp_fr_pt_df_2 = en_sp_fr_pt_df.reset_index(drop = True)\n",
    "    en_sp_fr_pt_ar_df = en_sp_fr_pt_df_2.append(arabic_test_df)\n",
    "    en_sp_fr_pt_ar_df_2 = en_sp_fr_pt_ar_df.reset_index(drop = True)\n",
    "    en_sp_fr_pt_ar_ja_df = en_sp_fr_pt_ar_df.append(japanese_test_df)\n",
    "    en_sp_fr_pt_ar_ja_df_2 = en_sp_fr_pt_ar_ja_df.reset_index(drop = True)\n",
    "    full_df_1 = en_sp_fr_pt_ar_ja_df.append(korean_test_df)\n",
    "    full_df = full_df_1.drop([\"Sentiment_label\", \"results\"], axis = 1)\n",
    "    full_df.reset_index(drop = True, inplace = True)\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 43: Translate the text location to English Function\n",
    "def translate_to_english(full_df):\n",
    "    full_dflen = len(full_df[\"textloc\"])\n",
    "    full_df[\"textloc_en\"] = \"\"\n",
    "    for i in range(0, full_dflen):\n",
    "        try:\n",
    "            text = full_df[\"textloc\"][i]\n",
    "            translated = GoogleTranslator(source = \"auto\", to_lang = \"english\").translate(text=text)\n",
    "            full_df[\"textloc_en\"][i] = translated\n",
    "        except:\n",
    "            full_df[\"textloc_en\"][i] = \"\"\n",
    "            \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 46: Construct the Main Function, this will get all the tweets sentiment and dates in order for comparsion\n",
    "def main_full(full_json_df):\n",
    "    #counter_2 = 0\n",
    "    df = full_json_df\n",
    "    tmfull3 = date_func(df)\n",
    "    tmfull4 = year_func(tmfull3)\n",
    "    tmfull5 = month_func(tmfull4)\n",
    "    tmfull6 = day_func(tmfull5)\n",
    "    df3 = date_combos(tmfull6)\n",
    "    English_Tweets_2 = English_Tweets_Func(df3)\n",
    "    Spanish_Tweets_2 = Spanish_Tweets_Func(df3)\n",
    "    French_Tweets_2 = French_Tweets_Func(df3)\n",
    "    Portuguese_Tweets_2 = Portuguese_Tweets_Func(df3)\n",
    "    Arabic_Tweets_2 = Arabic_Tweets_Func(df3)\n",
    "    Japanese_Tweets_2 = Japanese_Tweets_Func(df3)\n",
    "    Korean_Tweets_2 = Korean_Tweets_Func(df3)\n",
    "    entweet_coord = english_df(English_Tweets_2)\n",
    "    sptweet_coord = spanish_df(Spanish_Tweets_2)\n",
    "    frtweet_coord = french_df(French_Tweets_2)\n",
    "    pttweet_coord = portuguese_df(Portuguese_Tweets_2)\n",
    "    artweet_coord = arabic_df(Arabic_Tweets_2)\n",
    "    jatweet_coord = japanese_df(Japanese_Tweets_2)\n",
    "    kotweet_coord = korean_df(Korean_Tweets_2)\n",
    "    full_df_2 = create_full(entweet_coord, sptweet_coord, frtweet_coord, pttweet_coord, artweet_coord, jatweet_coord, kotweet_coord)\n",
    "    return full_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_df_no_dups(full_df):\n",
    "    full_df_no_dups = full_df.drop_duplicates(subset=['text', 'created_at'])\n",
    "    return full_df_no_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset \n",
    "#df_hist_full = main_full(full_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_hist_full.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_hist_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_hist_full[\"tag\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need Section to Group by tweets for analysis\n",
    "#https://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts\n",
    "#hist_grouped = df_hist_full.groupby([\"lang\", \"Sentiment\"]).size()\n",
    "#hist_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translate the location\n",
    "#hist_dff2 = translate_to_english(df_hist_full)\n",
    "#hist_dff2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need Section to Group by tweets for analysis\n",
    "#https://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts\n",
    "#hist_grouped = df_hist_full.groupby([\"lang\", \"textloc_en\"]).size()\n",
    "#hist_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 500)\n",
    "#hgdf = hist_grouped.to_frame()\n",
    "#hgdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: The Topic Modelling, this could wind up being a maybe as well just given the size of the data and the \n",
    "#time it takes to run the topic modelling programs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0: Libraries\n",
    "#Step 3: Topics over Time Libraries\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.test.utils\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tqdm\n",
    "#Random other libraries\n",
    "import requests\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Pretty Printing Library\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Making the Plot Library\n",
    "#Graphing in Jupiter Notebook\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Importing The Stopwords\n",
    "#Step 19: English Stop Words Vector\n",
    "def stopwords_en_func():\n",
    "    stop_words_en = stopwords.words('english')\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_en = stop_words_en + custom_stop_words\n",
    "    return final_stop_words_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 20: Spanish Stop Words Vector\n",
    "def stopwords_sp_func():\n",
    "    stop_words_sp = stopwords.words('spanish')\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_sp = stop_words_sp + custom_stop_words\n",
    "    return final_stop_words_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 21: French Stop Words Vector\n",
    "def stopwords_fr_func():\n",
    "    stop_words_fr = stopwords.words('french')\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_fr = stop_words_fr + custom_stop_words\n",
    "    return final_stop_words_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 22: Portuguese Stop Words Vector\n",
    "def stopwords_pt_func():\n",
    "    stop_words_pt = stopwords.words('portuguese')\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_pt = stop_words_pt + custom_stop_words\n",
    "    return final_stop_words_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 23: Arabic Stop Words Vector\n",
    "def stopwords_ar_func():\n",
    "    stop_words_ar = stopwords.words('arabic')\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_ar = stop_words_ar + custom_stop_words\n",
    "    return final_stop_words_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 24: Get the Japanese Stopwords Vector\n",
    "def stopwords_ja_func():\n",
    "    stopwords_ja_df = pd.read_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/stopwords-ja.txt\", delimiter = \"\\t\", encoding = \"utf-8\")\n",
    "    stopwords_ja_df.columns = [\"word\"]\n",
    "    stopwords_ja = stopwords_ja_df[\"word\"].values.tolist()\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_ja = stopwords_ja + custom_stop_words\n",
    "    return final_stop_words_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 25: Import Korean Stop Words\n",
    "def stopwords_ko_func():\n",
    "    stopwords_ko_df = pd.read_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/stopwords-ko.txt\", delimiter = \"\\t\", quoting=csv.QUOTE_NONE, encoding = \"utf-8\")\n",
    "    stopwords_ko_df.columns = [\"word\"]\n",
    "    stopwords_ko = stopwords_ko_df[\"word\"].values.tolist()\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_ko = stopwords_ko + custom_stop_words\n",
    "    return final_stop_words_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Removing the Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 26: Remove Stop Words English\n",
    "def stopwords_en(texts, final_stop_words_en):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_en] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 27: Remove Stop Words Spanish\n",
    "def stopwords_sp(texts, final_stop_words_sp):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_sp] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 28: Remove Stop Words French\n",
    "def stopwords_fr(texts, final_stop_words_fr):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_fr] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 29: Remove Stop Words Portuguese\n",
    "def stopwords_pt(texts, final_stop_words_pt):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_pt] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 30: Remove Stop Words Arabic\n",
    "def stopwords_ar(texts, final_stop_words_ar):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_ar] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 31: Remove Stop Words Japanese\n",
    "def stopwords_ja(texts, final_stop_words_ja):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_ja] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 32: Remove Stop Words Korean\n",
    "def stopwords_ko(texts, final_stop_words_ko):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_ko] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Make the bigram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 33: Make bigrams of the words\n",
    "#Make sure do call this function 7 times for each of the 7 languages\n",
    "def bigrams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count = 5, threshold = 100)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Lemma the data for the languages you can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 34: Turn the words into lemmas English\n",
    "def data_lemmatization_en(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])\n",
    "    nlp.max_length = 15000000\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 35: Turn the words into lemmas Spanish\n",
    "def data_lemmatization_sp(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    nlp = spacy.load(\"es_core_news_sm\", disable = ['parser', 'ner'])\n",
    "    nlp.max_length = 15000000\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 36: Turn the words into lemmas French\n",
    "def data_lemmatization_fr(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    nlp = spacy.load(\"fr_core_news_sm\", disable = ['parser', 'ner'])\n",
    "    nlp.max_length = 15000000\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 37: Turn the words into lemmas Portuguese\n",
    "def data_lemmatization_pt(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    nlp = spacy.load(\"pt_core_news_sm\", disable = ['parser', 'ner'])\n",
    "    nlp.max_length = 15000000\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 38: Turn the words into lemmas Portuguese\n",
    "def data_lemmatization_ja(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    nlp = spacy.load(\"ja_core_news_sm\", disable = ['parser', 'ner'])\n",
    "    nlp.max_length = 15000000\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Create the createlist function\n",
    "#Step 41: Create topic_id numbers, change dynamically\n",
    "#https://www.delftstack.com/howto/python/python-list-from-1-to-n/\n",
    "#topic_id = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "def createList(n):\n",
    "    lst1 = []\n",
    "    for i in range(n):\n",
    "        lst1.append(i)\n",
    "    return(lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Create the computing coherence function\n",
    "#Step 42: Detour to focus on hyperparameter tunning of the LDA Model\n",
    "#https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "#First build the function to test the hyperparameters of the number of topics (k), the document - topic density\n",
    "#(alpha) and the Word - topic density (beta), we get the coherence score for each model using the 'c_v' which is \n",
    "#measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation \n",
    "#measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
    "\n",
    "def compute_coherence_values(corpus, texts, dictionary, k, a, b):\n",
    "\n",
    "    lda_model_cv = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                         id2word = dictionary,\n",
    "                                         num_topics = k,\n",
    "                                         random_state = 101,\n",
    "                                         chunksize = 100,\n",
    "                                         passes = 10,\n",
    "                                         alpha = a,\n",
    "                                         eta = b,\n",
    "                                          per_word_topics = True,\n",
    "                                          minimum_probability = 0)\n",
    "    \n",
    "    #coherence_model_lda = CoherenceModel(model = lda_model_cv, texts = data_lemma, dictionary = id2word,\n",
    "    #                                    coherence = 'c_v')\n",
    "    coherence_model_lda = CoherenceModel(model = lda_model_cv, texts = texts, \n",
    "                                         dictionary = dictionary, coherence = 'c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 43: LDA Hyperparameter Finding function for English\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "def lda_hyperparameter_generating_en(df_en, final_stop_words_en):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_en(df_en['text'], final_stop_words_en)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram_hf = bigrams(data_words_nostops_hf)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma_hf = data_lemmatization_en(data_words_bigram_hf, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word_hf = corpora.Dictionary(data_lemma_hf)\n",
    "    #Corpus\n",
    "    texts_hf = data_lemma_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 9\n",
    "    max_topics = 10\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_lemma_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_en = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 44: Hyper Parameter Defining for English\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_en(mr_en):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_en.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_en['Topics'] == top_opt\n",
    "    mr_to = mr_en[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_en = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_en)\n",
    "    return hyper_list_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 45: Implement the stopwords, bigrams, and lemma functions English\n",
    "def build_lda_en(df_en, final_stop_words_en, hyper_list_en):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_en(df_en['text'], final_stop_words_en)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram = bigrams(data_words_nostops)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma = data_lemmatization_en(data_words_bigram, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemma)\n",
    "    #Corpus\n",
    "    texts = data_lemma\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_en = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_en[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_en[1],\n",
    "                                              eta = hyper_list_en[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_en[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_en\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Date', 'Year', 'Month', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_en.print_topics()\n",
    "    lenpt2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenpt2):\n",
    "        pt_list = printtopics2[i][1].split('*')\n",
    "        pt_list_words = []\n",
    "        lenptl = len(pt_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 4):\n",
    "            t_1 = pt_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            pt_list_words.append(t_3)\n",
    "        topic_label_list.append(pt_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_en = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 46: Visualization of Topics over Time English\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_en(df, hyper_list_en):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs_en = {}\n",
    "    topic_label_list_en = []\n",
    "    for i in range(0, hyper_list_en[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list_en.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs_en[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors_en = hyper_list_en[0]\n",
    "    \n",
    "    color_en = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors_en)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs_en.keys():\n",
    "        plt.plot(topic_dfs_en[i], color = color_en[i])\n",
    "    plt.xlim(14, 21)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in English Topics\")\n",
    "    plt.xlabel(\"Day\") \n",
    "    plt.ylabel(\"Average Day Topic Weight\")\n",
    "    plt.legend((topic_label_list_en))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Test_Output/Test_Output_EN_Day_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 47: LDA Hyperparameter Finding function for Spanish\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "def lda_hyperparameter_generating_sp(df_sp, final_stop_words_sp):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_sp(df_sp['text'], final_stop_words_sp)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram_hf = bigrams(data_words_nostops_hf)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma_hf = data_lemmatization_sp(data_words_bigram_hf, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word_hf = corpora.Dictionary(data_lemma_hf)\n",
    "    #Corpus\n",
    "    texts_hf = data_lemma_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 9\n",
    "    max_topics = 10\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_lemma_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_sp = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 48: Hyper Parameter Defining for Spanish\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_sp(mr_sp):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_sp.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_sp['Topics'] == top_opt\n",
    "    mr_to = mr_sp[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_sp = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_sp)\n",
    "    return hyper_list_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 50: Implement the stopwords, bigrams, and lemma functions Spanish\n",
    "def build_lda_sp(df_sp, final_stop_words_sp, hyper_list_sp):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_sp(df_sp['text'], final_stop_words_sp)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram = bigrams(data_words_nostops)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma = data_lemmatization_sp(data_words_bigram, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemma)\n",
    "    #Corpus\n",
    "    texts = data_lemma\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_sp = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_sp[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_sp[1],\n",
    "                                              eta = hyper_list_sp[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_sp[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_sp\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Date', 'Year', 'Month', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_sp.print_topics()\n",
    "    lenpt2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenpt2):\n",
    "        pt_list = printtopics2[i][1].split('*')\n",
    "        pt_list_words = []\n",
    "        lenptl = len(pt_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 4):\n",
    "            t_1 = pt_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            pt_list_words.append(t_3)\n",
    "        topic_label_list.append(pt_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Translate the Topic Label \n",
    "    tll_translate = []\n",
    "    lentll = len(topic_label_list)\n",
    "    for i in range(0, lentll):\n",
    "        tll_topic_line = []\n",
    "        lent_t_l = len(topic_label_list[i])\n",
    "        for j in range(0, lent_t_l):\n",
    "            text = topic_label_list[i][j]\n",
    "            translated = GoogleTranslator(source = \"spanish\", to_lang = \"english\").translate(text=text)\n",
    "            tll_topic_line.append(translated)\n",
    "        tll_translate.append(tll_topic_line)\n",
    "        \n",
    "    #Topic Label Translated \n",
    "    topic_labels_translate = tll_translate\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels_translate))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_sp = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 51: Visualization of Topics over Time Spanish\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_sp(df, hyper_list_sp):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs_sp = {}\n",
    "    topic_label_list_sp = []\n",
    "    for i in range(0, hyper_list_sp[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list_sp.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs_sp[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors_sp = hyper_list_sp[0]\n",
    "    \n",
    "    color_sp = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors_sp)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs_sp.keys():\n",
    "        plt.plot(topic_dfs_sp[i], color = color_sp[i])\n",
    "    plt.xlim(14, 21)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in Spanish Topics\")\n",
    "    plt.xlabel(\"Day\") \n",
    "    plt.ylabel(\"Average Day Topic Weight\")\n",
    "    plt.legend((topic_label_list_sp))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Test_Output/Test_Output_SP_Day_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 52: LDA Hyperparameter Finding function for French\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "def lda_hyperparameter_generating_fr(df_fr, final_stop_words_fr):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_fr(df_fr['text'], final_stop_words_fr)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram_hf = bigrams(data_words_nostops_hf)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma_hf = data_lemmatization_fr(data_words_bigram_hf, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word_hf = corpora.Dictionary(data_lemma_hf)\n",
    "    #Corpus\n",
    "    texts_hf = data_lemma_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 9\n",
    "    max_topics = 10\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_lemma_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_fr = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 53: Hyper Parameter Defining for French\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_fr(mr_fr):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_fr.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_fr['Topics'] == top_opt\n",
    "    mr_to = mr_fr[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_fr = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_fr)\n",
    "    return hyper_list_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 54: Implement the stopwords, bigrams, and lemma functions Spanish\n",
    "def build_lda_fr(df_fr, final_stop_words_fr, hyper_list_fr):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_fr(df_fr['text'], final_stop_words_fr)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram = bigrams(data_words_nostops)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma = data_lemmatization_fr(data_words_bigram, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemma)\n",
    "    #Corpus\n",
    "    texts = data_lemma\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_fr = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_fr[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_fr[1],\n",
    "                                              eta = hyper_list_fr[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_fr[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_fr\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Date', 'Year', 'Month', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_fr.print_topics()\n",
    "    lenpt2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenpt2):\n",
    "        pt_list = printtopics2[i][1].split('*')\n",
    "        pt_list_words = []\n",
    "        lenptl = len(pt_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 4):\n",
    "            t_1 = pt_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            pt_list_words.append(t_3)\n",
    "        topic_label_list.append(pt_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Translate the Topic Label \n",
    "    tll_translate = []\n",
    "    lentll = len(topic_label_list)\n",
    "    for i in range(0, lentll):\n",
    "        tll_topic_line = []\n",
    "        lent_t_l = len(topic_label_list[i])\n",
    "        for j in range(0, lent_t_l):\n",
    "            text = topic_label_list[i][j]\n",
    "            translated = GoogleTranslator(source = \"french\", to_lang = \"english\").translate(text=text)\n",
    "            tll_topic_line.append(translated)\n",
    "        tll_translate.append(tll_topic_line)\n",
    "        \n",
    "    #Topic Label Translated \n",
    "    topic_labels_translate = tll_translate\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels_translate))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_fr = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_fr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 55: Visualization of Topics over Time French\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_fr(df, hyper_list_fr):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs_fr = {}\n",
    "    topic_label_list_fr = []\n",
    "    for i in range(0, hyper_list_fr[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list_fr.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs_fr[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors_fr = hyper_list_fr[0]\n",
    "    \n",
    "    color_fr = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors_fr)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs_fr.keys():\n",
    "        plt.plot(topic_dfs_fr[i], color = color_fr[i])\n",
    "    plt.xlim(14, 21)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in French Topics\")\n",
    "    plt.xlabel(\"Day\") \n",
    "    plt.ylabel(\"Average Hour Topic Weight\")\n",
    "    plt.legend((topic_label_list_fr))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Test_Output/Test_Output_FR_Day_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 56: LDA Hyperparameter Finding function for Portuguese\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "def lda_hyperparameter_generating_pt(df_pt, final_stop_words_pt):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_pt(df_pt['text'], final_stop_words_pt)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram_hf = bigrams(data_words_nostops_hf)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma_hf = data_lemmatization_pt(data_words_bigram_hf, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word_hf = corpora.Dictionary(data_lemma_hf)\n",
    "    #Corpus\n",
    "    texts_hf = data_lemma_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 9\n",
    "    max_topics = 10\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_lemma_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_pt = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 57: Hyper Parameter Defining for Portuguese\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_pt(mr_pt):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_pt.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_pt['Topics'] == top_opt\n",
    "    mr_to = mr_pt[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_pt = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_pt)\n",
    "    return hyper_list_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 58: Implement the stopwords, bigrams, and lemma functions Portuguese\n",
    "def build_lda_pt(df_pt, final_stop_words_pt, hyper_list_pt):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_pt(df_pt['text'], final_stop_words_pt)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram = bigrams(data_words_nostops)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma = data_lemmatization_pt(data_words_bigram, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemma)\n",
    "    #Corpus\n",
    "    texts = data_lemma\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_pt = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_pt[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_pt[1],\n",
    "                                              eta = hyper_list_pt[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_pt[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_pt\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Date', 'Year', 'Month', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_pt.print_topics()\n",
    "    lenpt2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenpt2):\n",
    "        pt_list = printtopics2[i][1].split('*')\n",
    "        pt_list_words = []\n",
    "        lenptl = len(pt_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 4):\n",
    "            t_1 = pt_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            pt_list_words.append(t_3)\n",
    "        topic_label_list.append(pt_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Translate the Topic Label \n",
    "    tll_translate = []\n",
    "    lentll = len(topic_label_list)\n",
    "    for i in range(0, lentll):\n",
    "        tll_topic_line = []\n",
    "        lent_t_l = len(topic_label_list[i])\n",
    "        for j in range(0, lent_t_l):\n",
    "            text = topic_label_list[i][j]\n",
    "            translated = GoogleTranslator(source = \"portuguese\", to_lang = \"english\").translate(text=text)\n",
    "            tll_topic_line.append(translated)\n",
    "        tll_translate.append(tll_topic_line)\n",
    "        \n",
    "    #Topic Label Translated \n",
    "    topic_labels_translate = tll_translate\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels_translate))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_pt = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 59: Visualization of Topics over Time Portuguese\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_pt(df, hyper_list_pt):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_pt[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors = hyper_list_pt[0]\n",
    "    \n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs.keys():\n",
    "        plt.plot(topic_dfs[i], color = color[i])\n",
    "    plt.xlim(14, 21)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in Portuguese Topics\")\n",
    "    plt.xlabel(\"Day\") \n",
    "    plt.ylabel(\"Average Hour Topic Weight\")\n",
    "    plt.legend((topic_label_list))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Test_Output/Test_Output_PT_Day_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 60: LDA Hyperparameter Finding function for Arabic\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "#https://stackoverflow.com/questions/33229360/gensim-typeerror-doc2bow-expects-an-array-of-unicode-tokens-on-input-not-a-si\n",
    "def lda_hyperparameter_generating_ar(df_ar, final_stop_words_ar):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_ar(df_ar['text'], final_stop_words_ar)\n",
    "    #Create the bigram from the non stop words\n",
    "    #data_words_bigram_hf = str(bigrams(data_words_nostops_hf))\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    #data_lemma_hf = data_tokenization_ar(data_words_bigram_hf)\n",
    "    #data_lemma_hf = data_tokenization_ar(data_words_nostops_hf)\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    #id2word_hf = corpora.Dictionary([a.split(' ') for a in data_lemma_hf])\n",
    "    #id2word_hf = corpora.Dictionary([str(data_lemma_hf).split(' ')])\n",
    "    #id2word_hf = corpora.Dictionary([str(data_words_bigram_hf).split()])\n",
    "    id2word_hf = corpora.Dictionary(data_words_nostops_hf)\n",
    "    #Corpus\n",
    "    #texts_hf = [str(data_lemma_hf).split(' ')]\n",
    "    #texts_hf = [str(data_words_bigram_hf).split()]\n",
    "    texts_hf = data_words_nostops_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 9\n",
    "    max_topics = 10\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_words_nostops_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_ar = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_ar\n",
    "    #print(mr_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 61: Hyper Parameter Defining for Arabic\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_ar(mr_ar):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_ar.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_ar['Topics'] == top_opt\n",
    "    mr_to = mr_ar[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_ar = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_ar)\n",
    "    return hyper_list_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 62: Implement the stopwords, bigrams, and lemma functions Arabic\n",
    "def build_lda_ar(df_ar, final_stop_words_ar, hyper_list_ar):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_pt(df_ar['text'], final_stop_words_ar)\n",
    "    #Create the bigram from the non stop words\n",
    "    #data_words_bigram = str(bigrams(data_words_nostops))\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    #data_lemma = data_tokenization_ar(data_words_bigram)\n",
    "    #data_lemma = data_tokenization_ar(data_words_nostops)\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    #id2word = corpora.Dictionary([a.split(' ') for a in data_lemma])\n",
    "    id2word = corpora.Dictionary(data_words_nostops)\n",
    "    #Corpus\n",
    "    texts = data_words_nostops\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_ar = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_ar[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_ar[1],\n",
    "                                              eta = hyper_list_ar[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_ar[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_ar\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Date', 'Year', 'Month', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_ar.print_topics()\n",
    "    lenar2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenar2):\n",
    "        ar_list = printtopics2[i][1].split('*')\n",
    "        ar_list_words = []\n",
    "        lenarl = len(ar_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 4):\n",
    "            t_1 = ar_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            ar_list_words.append(t_3)\n",
    "        topic_label_list.append(ar_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Translate the Topic Label \n",
    "    tll_translate = []\n",
    "    lentll = len(topic_label_list)\n",
    "    for i in range(0, lentll):\n",
    "        tll_topic_line = []\n",
    "        lent_t_l = len(topic_label_list[i])\n",
    "        for j in range(0, lent_t_l):\n",
    "            text = topic_label_list[i][j]\n",
    "            translated = GoogleTranslator(source = \"arabic\", to_lang = \"english\").translate(text=text)\n",
    "            tll_topic_line.append(translated)\n",
    "        tll_translate.append(tll_topic_line)\n",
    "        \n",
    "    #Topic Label Translated \n",
    "    topic_labels_translate = tll_translate\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels_translate))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_ar = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 63: Visualization of Topics over Time Arabic\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_ar(df, hyper_list_ar):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_ar[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors = hyper_list_ar[0]\n",
    "    \n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs.keys():\n",
    "        plt.plot(topic_dfs[i], color = color[i])\n",
    "    plt.xlim(14, 21)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in Arabic Topics\")\n",
    "    plt.xlabel(\"Day\") \n",
    "    plt.ylabel(\"Average Hour Topic Weight\")\n",
    "    plt.legend((topic_label_list))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Test_Output/Test_Output_AR_Day_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 64: LDA Hyperparameter Finding function for Japanese\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "def lda_hyperparameter_generating_ja(df_ja, final_stop_words_ja):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_ja(df_ja['text'], final_stop_words_ja)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram_hf = bigrams(data_words_nostops_hf)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma_hf = data_lemmatization_ja(data_words_bigram_hf, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word_hf = corpora.Dictionary(data_lemma_hf)\n",
    "    #Corpus\n",
    "    texts_hf = data_lemma_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 9\n",
    "    max_topics = 10\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_lemma_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_ja = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 65: Hyper Parameter Defining for Japanese\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_ja(mr_ja):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_ja.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    #print(mr3)\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_ja['Topics'] == top_opt\n",
    "    mr_to = mr_ja[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_ja = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_ja)\n",
    "    return hyper_list_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 66: Implement the stopwords, bigrams, and lemma functions Japanese\n",
    "def build_lda_ja(df_ja, final_stop_words_ja, hyper_list_ja):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_ja(df_ja['text'], final_stop_words_ja)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram = bigrams(data_words_nostops)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma = data_lemmatization_ja(data_words_bigram, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemma)\n",
    "    #Corpus\n",
    "    texts = data_lemma\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_ja = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_ja[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_ja[1],\n",
    "                                              eta = hyper_list_ja[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_ja[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_ja\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Date', 'Year', 'Month', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_ja.print_topics()\n",
    "    lenja2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenja2):\n",
    "        ja_list = printtopics2[i][1].split('*')\n",
    "        ja_list_words = []\n",
    "        lenjal = len(ja_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 4):\n",
    "            t_1 = ja_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            ja_list_words.append(t_3)\n",
    "        topic_label_list.append(ja_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Translate the Topic Label \n",
    "    tll_translate = []\n",
    "    lentll = len(topic_label_list)\n",
    "    for i in range(0, lentll):\n",
    "        tll_topic_line = []\n",
    "        lent_t_l = len(topic_label_list[i])\n",
    "        for j in range(0, lent_t_l):\n",
    "            text = topic_label_list[i][j]\n",
    "            translated = GoogleTranslator(source = \"japanese\", to_lang = \"english\").translate(text=text)\n",
    "            tll_topic_line.append(translated)\n",
    "        tll_translate.append(tll_topic_line)\n",
    "        \n",
    "    #Topic Label Translated \n",
    "    topic_labels_translate = tll_translate\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels_translate))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_ja = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 67: Visualization of Topics over Time Japanese\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_ja(df, hyper_list_ja):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_ja[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors = hyper_list_ja[0]\n",
    "    \n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs.keys():\n",
    "        plt.plot(topic_dfs[i], color = color[i])\n",
    "    plt.xlim(14, 21)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in Japanese Topics\")\n",
    "    plt.xlabel(\"Day\") \n",
    "    plt.ylabel(\"Average Hour Topic Weight\")\n",
    "    plt.legend((topic_label_list))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Test_Output/Test_Output_JA_Day_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 64: LDA Hyperparameter Finding function for Korean\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "def lda_hyperparameter_generating_ko(df_ko, final_stop_words_ko):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_ko(df_ko['text'], final_stop_words_ko)\n",
    "    #Create the bigram from the non stop words\n",
    "    #data_words_bigram_hf = bigrams(data_words_nostops_hf)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    #data_lemma_hf = data_lemmatization_ko(data_words_bigram_hf)\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word_hf = corpora.Dictionary(data_words_nostops_hf)\n",
    "    #id2word_hf = corpora.Dictionary([str(data_lemma_hf).split(' ')])\n",
    "    #Corpus\n",
    "    #texts_hf = [str(data_lemma_hf).split(' ')]\n",
    "    texts_hf = data_words_nostops_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 9\n",
    "    max_topics = 10\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_words_nostops_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_ko = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 65: Hyper Parameter Defining for Japanese\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_ko(mr_ko):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_ko.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_ko['Topics'] == top_opt\n",
    "    mr_to = mr_ko[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_ko = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_ko)\n",
    "    return hyper_list_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 66: Implement the stopwords, bigrams, and lemma functions Japanese\n",
    "def build_lda_ko(df_ko, final_stop_words_ko, hyper_list_ko):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_ko(df_ko['text'], final_stop_words_ko)\n",
    "    #Create the bigram from the non stop words\n",
    "    #data_words_bigram = bigrams(data_words_nostops)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    #data_lemma = data_lemmatization_ko(data_words_bigram)\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    #id2word = corpora.Dictionary([str(data_lemma).split(' ')])\n",
    "    id2word = corpora.Dictionary(data_words_nostops)\n",
    "    #Corpus\n",
    "    texts = data_words_nostops\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_ko = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_ko[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_ko[1],\n",
    "                                              eta = hyper_list_ko[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_ko[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_ko\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Date', 'Year', 'Month', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_ko.print_topics()\n",
    "    lenko2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenko2):\n",
    "        ko_list = printtopics2[i][1].split('*')\n",
    "        ko_list_words = []\n",
    "        lenkol = len(ko_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 4):\n",
    "            t_1 = ko_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            ko_list_words.append(t_3)\n",
    "        topic_label_list.append(ko_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Translate the Topic Label \n",
    "    tll_translate = []\n",
    "    lentll = len(topic_label_list)\n",
    "    for i in range(0, lentll):\n",
    "        tll_topic_line = []\n",
    "        lent_t_l = len(topic_label_list[i])\n",
    "        for j in range(0, lent_t_l):\n",
    "            text = topic_label_list[i][j]\n",
    "            translated = GoogleTranslator(source = \"korean\", to_lang = \"english\").translate(text=text)\n",
    "            tll_topic_line.append(translated)\n",
    "        tll_translate.append(tll_topic_line)\n",
    "        \n",
    "    #Topic Label Translated \n",
    "    topic_labels_translate = tll_translate\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels_translate))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_ko = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 67: Visualization of Topics over Time Japanese\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_ko(df, hyper_list_ko):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_ko[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors = hyper_list_ko[0]\n",
    "    \n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs.keys():\n",
    "        plt.plot(topic_dfs[i], color = color[i])\n",
    "    plt.xlim(14, 21)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in Korean Topics\")\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Average Hour Topic Weight\")\n",
    "    plt.legend((topic_label_list))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Test_Output/Test_Output_KO_Day_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 54: The Main Function\n",
    "def main_tm(full_json_file):\n",
    "    tmfull = full_json_file\n",
    "    tmfull3 = date_func(tmfull)\n",
    "    tmfull4 = year_func(tmfull3)\n",
    "    tmfull5 = month_func(tmfull4)\n",
    "    tmfull6 = day_func(tmfull5)\n",
    "    tmfull10 = date_combos(tmfull6)\n",
    "    eng_tweet = English_Tweets_Func(tmfull10)\n",
    "    esp_tweet = Spanish_Tweets_Func(tmfull10)\n",
    "    frn_tweet = French_Tweets_Func(tmfull10)\n",
    "    prt_tweet = Portuguese_Tweets_Func(tmfull10)\n",
    "    arb_tweet = Arabic_Tweets_Func(tmfull10)\n",
    "    jpn_tweet = Japanese_Tweets_Func(tmfull10)\n",
    "    kor_tweet = Korean_Tweets_Func(tmfull10)\n",
    "    eng_t2 = sort_chrono(eng_tweet)\n",
    "    esp_t2 = sort_chrono(esp_tweet)\n",
    "    frn_t2 = sort_chrono(frn_tweet)\n",
    "    prt_t2 = sort_chrono(prt_tweet)\n",
    "    arb_t2 = sort_chrono(arb_tweet)\n",
    "    jpn_t2 = sort_chrono(jpn_tweet)\n",
    "    kor_t2 = sort_chrono(kor_tweet)\n",
    "    final_stop_words_en = stopwords_en_func()\n",
    "    final_stop_words_sp = stopwords_sp_func()\n",
    "    final_stop_words_fr = stopwords_fr_func()\n",
    "    final_stop_words_pt = stopwords_pt_func()\n",
    "    final_stop_words_ar = stopwords_ar_func()\n",
    "    final_stop_words_ja = stopwords_ja_func()\n",
    "    final_stop_words_ko = stopwords_ko_func()\n",
    "    mr_en = lda_hyperparameter_generating_en(eng_t2, final_stop_words_en)\n",
    "    mr_sp = lda_hyperparameter_generating_sp(esp_t2, final_stop_words_sp)\n",
    "    mr_fr = lda_hyperparameter_generating_fr(frn_t2, final_stop_words_fr)\n",
    "    mr_pt = lda_hyperparameter_generating_pt(prt_t2, final_stop_words_pt)\n",
    "    mr_ar = lda_hyperparameter_generating_ar(arb_t2, final_stop_words_ar)\n",
    "    mr_ja = lda_hyperparameter_generating_ja(jpn_t2, final_stop_words_ja)\n",
    "    mr_ko = lda_hyperparameter_generating_ko(kor_t2, final_stop_words_ko)\n",
    "    hyper_list_en = lda_hyper_define_en(mr_en)\n",
    "    hyper_list_sp = lda_hyper_define_sp(mr_sp)\n",
    "    hyper_list_fr = lda_hyper_define_fr(mr_fr)\n",
    "    hyper_list_pt = lda_hyper_define_pt(mr_pt)\n",
    "    hyper_list_ar = lda_hyper_define_ar(mr_ar)\n",
    "    hyper_list_ja = lda_hyper_define_ja(mr_ja)\n",
    "    hyper_list_ko = lda_hyper_define_ko(mr_ko)\n",
    "    en_tm_final = build_lda_en(eng_t2, final_stop_words_en, hyper_list_en)\n",
    "    sp_tm_final = build_lda_sp(esp_t2, final_stop_words_sp, hyper_list_sp)\n",
    "    fr_tm_final = build_lda_fr(frn_t2, final_stop_words_fr, hyper_list_fr)\n",
    "    pt_tm_final = build_lda_pt(prt_t2, final_stop_words_pt, hyper_list_pt)\n",
    "    ar_tm_final = build_lda_ar(arb_t2, final_stop_words_ar, hyper_list_ar)\n",
    "    ja_tm_final = build_lda_ja(jpn_t2, final_stop_words_ja, hyper_list_ja)\n",
    "    ko_tm_final = build_lda_ko(kor_t2, final_stop_words_ko, hyper_list_ko)\n",
    "    viz_topic_time_en(en_tm_final, hyper_list_en)\n",
    "    viz_topic_time_sp(sp_tm_final, hyper_list_sp)\n",
    "    viz_topic_time_fr(fr_tm_final, hyper_list_fr)\n",
    "    viz_topic_time_pt(pt_tm_final, hyper_list_pt)\n",
    "    viz_topic_time_ar(ar_tm_final, hyper_list_ar)\n",
    "    viz_topic_time_ja(ja_tm_final, hyper_list_ja)\n",
    "    viz_topic_time_ko(ko_tm_final, hyper_list_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the Main_tm function\n",
    "main_tm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Day 1: Test Data\n",
    "#full_5.tail()\n",
    "#REPLACE CODE EACH TIME WITH DIFFERENT INDEX VALUES, IT TAKES ROUGHLY 15 MINUTES TO GO THROUGH AND PROCESS\n",
    "#10,000 FILES, SO IF I AM WORKING TOMORROW THEN 15,000*4*6 = 360,000, SO SET 10000:369,999\n",
    "#NOTE: FINAL INDEX IS 3634952\n",
    "#NOTE: SINCE THIS HISTORICAL STUDY INFORMS THE REAL TIME STUDY THIS ONE NEEDS TO BE COMPLETED FIRST, HOWEVER, \n",
    "#I CAN WRITE UP THE ANALYSIS OF THE HISTORICAL STUDY WHILE GATHERING THE REAL TIME DATA, WHICH SHOULD TAKE ROUGHLY,\n",
    "#THE SAME AMOUNT OF TIME, SO I AM LOOKING AT MARCH FOR COMPLETION OF GATHERING THE REAL TIME DATA, AND APRIL BY HAVING\n",
    "#DRAFTS COMPLETED OF ALL MY PAPERS. \n",
    "#NOTE: NEED TO FIGURE OUT IF I WILL DO TOPIC MODELLING FOR THE HISTORICAL MODELS, SEE HOW LONG A WEEK OF DATA TAKES\n",
    "#TO ANALYZE THROUGH TOPIC MODELLING, WE KNOW THAT ROUGHLY 10,000 TWEETS TAKES 30 MINUTES, IF POSSIBLE I WILL GATHER\n",
    "#THE DATA FROM THAT, IT COULD BE USEFUL, ALTERNATIVELY,IF IT TAKES TOO LONG, I CAN DO IT FOR THE WEEKS SURROUNDING\n",
    "#THE KEY DATES OF THE URKAINE INVASION, WOULD CUT DOWN ON TIME THAT WAY. ONE WEEK PER MONTH FOR EXAMPLE WOULD ONLY BE \n",
    "#5 SESSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_5.to_json(r\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Tweets_Full/Full_File/Full_Five.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3634948</th>\n",
       "      <td>RT @RoyalAirForce: The Swiss Air Force has arr...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-01 00:43:26</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634949</th>\n",
       "      <td>@Louis_0004 @ADH423 @gladrew7 @DefiantLs I’m s...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-01 00:42:54</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634950</th>\n",
       "      <td>There was so much hostility between the sexes ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-01 00:38:17</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634951</th>\n",
       "      <td>RT @Daily_MSFight: Celestial being's Gundams a...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-01 00:30:28</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634952</th>\n",
       "      <td>RT @Daily_MSFight: Celestial being's Gundams a...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-01 00:21:18</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text lang  \\\n",
       "3634948  RT @RoyalAirForce: The Swiss Air Force has arr...   en   \n",
       "3634949  @Louis_0004 @ADH423 @gladrew7 @DefiantLs I’m s...   en   \n",
       "3634950  There was so much hostility between the sexes ...   en   \n",
       "3634951  RT @Daily_MSFight: Celestial being's Gundams a...   en   \n",
       "3634952  RT @Daily_MSFight: Celestial being's Gundams a...   en   \n",
       "\n",
       "                 created_at tag             cat  scale Date  \n",
       "3634948 2021-12-01 00:43:26  GN  force exercise   -7.6  NaT  \n",
       "3634949 2021-12-01 00:42:54  GN  force exercise   -7.6  NaT  \n",
       "3634950 2021-12-01 00:38:17  GN  force exercise   -7.6  NaT  \n",
       "3634951 2021-12-01 00:30:28  GN  force exercise   -7.6  NaT  \n",
       "3634952 2021-12-01 00:21:18  GN  force exercise   -7.6  NaT  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_5 = pd.read_json(r\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Tweets_Full/Full_File/Full_Five.json\")\n",
    "full_5.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>RT @PacificMarines: Birds of a Feather…\\n \\n@u...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-02-15 23:53:25</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>Two US Air Force F-16 assigned to 18th Aggress...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-02-15 23:51:01</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>RT @PacificMarines: Birds of a Feather…\\n \\n@u...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-02-15 23:42:38</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>RT @CISTpRecruiting: If it’s not raining it’s ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-02-15 23:31:05</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>RT @UKMCCMiddleEast: We're working alongside 6...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-02-15 23:30:56</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang  \\\n",
       "99995  RT @PacificMarines: Birds of a Feather…\\n \\n@u...   en   \n",
       "99996  Two US Air Force F-16 assigned to 18th Aggress...   en   \n",
       "99997  RT @PacificMarines: Birds of a Feather…\\n \\n@u...   en   \n",
       "99998  RT @CISTpRecruiting: If it’s not raining it’s ...   en   \n",
       "99999  RT @UKMCCMiddleEast: We're working alongside 6...   en   \n",
       "\n",
       "               created_at tag             cat  scale Date  \n",
       "99995 2022-02-15 23:53:25  GN  force exercise   -7.6  NaT  \n",
       "99996 2022-02-15 23:51:01  GN  force exercise   -7.6  NaT  \n",
       "99997 2022-02-15 23:42:38  GN  force exercise   -7.6  NaT  \n",
       "99998 2022-02-15 23:31:05  GN  force exercise   -7.6  NaT  \n",
       "99999 2022-02-15 23:30:56  GN  force exercise   -7.6  NaT  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3510,000 - 3610,000\n",
    "full_3510_3610 = full_5.loc[3510000:3609999]\n",
    "full_3510_3610_two = full_3510_3610.reset_index(drop = True)\n",
    "full_3510_3610_two.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 14:32:34.122121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 663ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 4s 337ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 15:29:16.883084\n",
      "0:56:42.760963\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(start)\n",
    "hist_3510_3610 = main_full(full_3510_3610_two)\n",
    "end = datetime.now()\n",
    "print(end)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123222</th>\n",
       "      <td>RT @sakimori8821: 昔見た時は何かと思ったけど、単純にリグを前後逆につけてる...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-02-22 01:46:11</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2022-02-22 01:46:11</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>82</td>\n",
       "      <td>330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123223</th>\n",
       "      <td>RT @sakimori8821: 昔見た時は何かと思ったけど、単純にリグを前後逆につけてる...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-02-21 14:53:19</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2022-02-21 14:53:19</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>81</td>\n",
       "      <td>331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123224</th>\n",
       "      <td>昔見た時は何かと思ったけど、単純にリグを前後逆につけてるだけだった\\n\\nhttps://t...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-02-21 13:24:29</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2022-02-21 13:24:29</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>81</td>\n",
       "      <td>332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123225</th>\n",
       "      <td>実力者っは誤訳。\\n\\nstrongman/ˈstrɒŋman/noun:\\n1. a ma...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-02-21 05:28:05</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2022-02-21 05:28:05</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>81</td>\n",
       "      <td>333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123226</th>\n",
       "      <td>미국 사시는 분들 참조:\\nCoronavirus Aid, Relief and Eco...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-01-12 00:13:59</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2022-01-12 00:13:59</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>미국</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text lang  \\\n",
       "123222  RT @sakimori8821: 昔見た時は何かと思ったけど、単純にリグを前後逆につけてる...   ja   \n",
       "123223  RT @sakimori8821: 昔見た時は何かと思ったけど、単純にリグを前後逆につけてる...   ja   \n",
       "123224  昔見た時は何かと思ったけど、単純にリグを前後逆につけてるだけだった\\n\\nhttps://t...   ja   \n",
       "123225  実力者っは誤訳。\\n\\nstrongman/ˈstrɒŋman/noun:\\n1. a ma...   ja   \n",
       "123226  미국 사시는 분들 참조:\\nCoronavirus Aid, Relief and Eco...   ko   \n",
       "\n",
       "                created_at tag             cat  scale                 Date  \\\n",
       "123222 2022-02-22 01:46:11  GN  force exercise   -7.6  2022-02-22 01:46:11   \n",
       "123223 2022-02-21 14:53:19  GN  force exercise   -7.6  2022-02-21 14:53:19   \n",
       "123224 2022-02-21 13:24:29  GN  force exercise   -7.6  2022-02-21 13:24:29   \n",
       "123225 2022-02-21 05:28:05  GN  force exercise   -7.6  2022-02-21 05:28:05   \n",
       "123226 2022-01-12 00:13:59  GP    economic aid    7.4  2022-01-12 00:13:59   \n",
       "\n",
       "        Year Month Day  Month_Day  TweetNumber  Compound  Sentiment textloc  \\\n",
       "123222  2022     2  22         82          330       NaN          1     NaN   \n",
       "123223  2022     2  21         81          331       NaN          1     NaN   \n",
       "123224  2022     2  21         81          332       NaN          1     NaN   \n",
       "123225  2022     2  21         81          333       NaN         -1     NaN   \n",
       "123226  2022     1  12         42            0       NaN          1      미국   \n",
       "\n",
       "       label  \n",
       "123222   NaN  \n",
       "123223   NaN  \n",
       "123224   NaN  \n",
       "123225   NaN  \n",
       "123226   NaN  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So full data can give us the count of text locations over time \n",
    "hist_3510_3610.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need Section to Group by tweets for analysis\n",
    "#https://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts\n",
    "#hist_grouped = hist_0_10000.groupby([\"lang\", \"Sentiment\"]).size()\n",
    "#hist_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need Section to Group by tweets for analysis\n",
    "#https://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts\n",
    "pd.set_option('display.max_rows', 500)\n",
    "hist_grouped_2 = hist_3510_3610.groupby([\"lang\", \"textloc\"]).size()\n",
    "#hist_grouped_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count_ner</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th>textloc</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ko</th>\n",
       "      <th>미국</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">pt</th>\n",
       "      <th>JURO</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT World News</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taiwan</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count_ner\n",
       "lang textloc                 \n",
       "ko   미국                     1\n",
       "pt   JURO                   1\n",
       "     RT World News          1\n",
       "     Taiwan                 1\n",
       "     US                     1"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series(hist_grouped_2, name = \"count_ner\")\n",
    "hgdf = s.to_frame()\n",
    "hgdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 1646 entries, ('ar', '-') to ('pt', 'US')\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   count_ner  1646 non-null   int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 30.6+ KB\n"
     ]
    }
   ],
   "source": [
    "hgdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>textloc</th>\n",
       "      <th>count_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>en</td>\n",
       "      <td>Mercy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>en</td>\n",
       "      <td>Metel-2022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>en</td>\n",
       "      <td>Micronesia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>en</td>\n",
       "      <td>Midwest States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>pt</td>\n",
       "      <td>US</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lang         textloc  count_ner\n",
       "1641   en           Mercy          1\n",
       "1642   en      Metel-2022          1\n",
       "1643   en      Micronesia          1\n",
       "1644   en  Midwest States          1\n",
       "1645   pt              US          1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgdf2 = hgdf.sort_values('count_ner', ascending = False)\n",
    "hgdf3 = hgdf2.reset_index()\n",
    "hgdf3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 15:29:17.046314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 15:36:35.189688\n",
      "0:07:18.143374\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(start)\n",
    "hgdf4 = translate_to_english(hgdf3)\n",
    "end = datetime.now()\n",
    "print(end)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>textloc</th>\n",
       "      <th>count_ner</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>14405</td>\n",
       "      <td>Ukraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>6741</td>\n",
       "      <td>U.S.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>Russia</td>\n",
       "      <td>6464</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>China</td>\n",
       "      <td>4765</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>US</td>\n",
       "      <td>4359</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>en</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>3201</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>Canada</td>\n",
       "      <td>3091</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>2465</td>\n",
       "      <td>Ethiopia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>UK</td>\n",
       "      <td>2202</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>2185</td>\n",
       "      <td>Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang      textloc  count_ner   textloc_en\n",
       "0   en      Ukraine      14405      Ukraine\n",
       "1   en         U.S.       6741         U.S.\n",
       "2   en       Russia       6464       Russia\n",
       "3   en        China       4765        China\n",
       "4   en           US       4359           US\n",
       "5   en  Afghanistan       3201  Afghanistan\n",
       "6   en       Canada       3091       Canada\n",
       "7   en     Ethiopia       2465     Ethiopia\n",
       "8   en           UK       2202           UK\n",
       "9   en       Taiwan       2185       Taiwan"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgdf4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>lang_y</th>\n",
       "      <th>count_ner</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132370</th>\n",
       "      <td>RT @sakimori8821: 昔見た時は何かと思ったけど、単純にリグを前後逆につけてる...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-02-22 01:46:11</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2022-02-22 01:46:11</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>82</td>\n",
       "      <td>330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132371</th>\n",
       "      <td>RT @sakimori8821: 昔見た時は何かと思ったけど、単純にリグを前後逆につけてる...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-02-21 14:53:19</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2022-02-21 14:53:19</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>81</td>\n",
       "      <td>331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132372</th>\n",
       "      <td>昔見た時は何かと思ったけど、単純にリグを前後逆につけてるだけだった\\n\\nhttps://t...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-02-21 13:24:29</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2022-02-21 13:24:29</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>81</td>\n",
       "      <td>332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132373</th>\n",
       "      <td>実力者っは誤訳。\\n\\nstrongman/ˈstrɒŋman/noun:\\n1. a ma...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-02-21 05:28:05</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2022-02-21 05:28:05</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>81</td>\n",
       "      <td>333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132374</th>\n",
       "      <td>미국 사시는 분들 참조:\\nCoronavirus Aid, Relief and Eco...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-01-12 00:13:59</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2022-01-12 00:13:59</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>미국</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ko</td>\n",
       "      <td>1.0</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text lang_x  \\\n",
       "132370  RT @sakimori8821: 昔見た時は何かと思ったけど、単純にリグを前後逆につけてる...     ja   \n",
       "132371  RT @sakimori8821: 昔見た時は何かと思ったけど、単純にリグを前後逆につけてる...     ja   \n",
       "132372  昔見た時は何かと思ったけど、単純にリグを前後逆につけてるだけだった\\n\\nhttps://t...     ja   \n",
       "132373  実力者っは誤訳。\\n\\nstrongman/ˈstrɒŋman/noun:\\n1. a ma...     ja   \n",
       "132374  미국 사시는 분들 참조:\\nCoronavirus Aid, Relief and Eco...     ko   \n",
       "\n",
       "                created_at tag             cat  scale                 Date  \\\n",
       "132370 2022-02-22 01:46:11  GN  force exercise   -7.6  2022-02-22 01:46:11   \n",
       "132371 2022-02-21 14:53:19  GN  force exercise   -7.6  2022-02-21 14:53:19   \n",
       "132372 2022-02-21 13:24:29  GN  force exercise   -7.6  2022-02-21 13:24:29   \n",
       "132373 2022-02-21 05:28:05  GN  force exercise   -7.6  2022-02-21 05:28:05   \n",
       "132374 2022-01-12 00:13:59  GP    economic aid    7.4  2022-01-12 00:13:59   \n",
       "\n",
       "        Year Month Day  Month_Day  TweetNumber  Compound  Sentiment textloc  \\\n",
       "132370  2022     2  22         82          330       NaN          1     NaN   \n",
       "132371  2022     2  21         81          331       NaN          1     NaN   \n",
       "132372  2022     2  21         81          332       NaN          1     NaN   \n",
       "132373  2022     2  21         81          333       NaN         -1     NaN   \n",
       "132374  2022     1  12         42            0       NaN          1      미국   \n",
       "\n",
       "       label lang_y  count_ner textloc_en  \n",
       "132370   NaN    NaN        NaN        NaN  \n",
       "132371   NaN    NaN        NaN        NaN  \n",
       "132372   NaN    NaN        NaN        NaN  \n",
       "132373   NaN    NaN        NaN        NaN  \n",
       "132374   NaN     ko        1.0        USA  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgdf5 = pd.merge(hist_3510_3610, hgdf4, on = \"textloc\", how = \"left\")\n",
    "hgdf5.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgdf5.to_json(r\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Processed_Data/hgdf3510_3610.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the data from memory\n",
    "del hgdf5, hist_3510_3610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This can give us the sentiment over time#\n",
    "#hist_0_10000_nodups = full_df_no_dups(hgdf5)\n",
    "#h0_10 = hist_0_10000_nodups.reset_index(drop = True)\n",
    "#h0_10.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h0_10[\"textloc_en\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h0_10.to_json(r\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Processed_Data/h0_10.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h0_10[\"tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h0_10[\"scale\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h0_10[\"Sentiment\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24948</th>\n",
       "      <td>RT @RoyalAirForce: The Swiss Air Force has arr...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-01 00:43:26</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24949</th>\n",
       "      <td>@Louis_0004 @ADH423 @gladrew7 @DefiantLs I’m s...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-01 00:42:54</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24950</th>\n",
       "      <td>There was so much hostility between the sexes ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-01 00:38:17</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24951</th>\n",
       "      <td>RT @Daily_MSFight: Celestial being's Gundams a...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-01 00:30:28</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24952</th>\n",
       "      <td>RT @Daily_MSFight: Celestial being's Gundams a...</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-12-01 00:21:18</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang  \\\n",
       "24948  RT @RoyalAirForce: The Swiss Air Force has arr...   en   \n",
       "24949  @Louis_0004 @ADH423 @gladrew7 @DefiantLs I’m s...   en   \n",
       "24950  There was so much hostility between the sexes ...   en   \n",
       "24951  RT @Daily_MSFight: Celestial being's Gundams a...   en   \n",
       "24952  RT @Daily_MSFight: Celestial being's Gundams a...   en   \n",
       "\n",
       "               created_at tag             cat  scale Date  \n",
       "24948 2021-12-01 00:43:26  GN  force exercise   -7.6  NaT  \n",
       "24949 2021-12-01 00:42:54  GN  force exercise   -7.6  NaT  \n",
       "24950 2021-12-01 00:38:17  GN  force exercise   -7.6  NaT  \n",
       "24951 2021-12-01 00:30:28  GN  force exercise   -7.6  NaT  \n",
       "24952 2021-12-01 00:21:18  GN  force exercise   -7.6  NaT  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Second Batch\n",
    "#3610,000 - 3109,999\n",
    "full_3610_end = full_5.loc[3610000:3634953]\n",
    "full_3610_end_two = full_3610_end.reset_index(drop = True)\n",
    "full_3610_end_two.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:06:36.069728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 185ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 519ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:15:24.927369\n",
      "0:08:48.857641\n"
     ]
    }
   ],
   "source": [
    "#Run the second batch\n",
    "start = datetime.now()\n",
    "print(start)\n",
    "hist_3610_end = main_full(full_3610_end_two)\n",
    "end = datetime.now()\n",
    "print(end)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27471</th>\n",
       "      <td>RT @Aviation_Assets: アメリカ陸軍第7歩兵師団および陸上自衛隊がワシント...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-16 16:31:28</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2021-12-16 16:31:28</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>376</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>ワシントン州ヤキマ・トレーニング・センター</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27472</th>\n",
       "      <td>アメリカ陸軍第7歩兵師団および陸上自衛隊がワシントン州ヤキマ・トレーニング・センターで実施し...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-16 14:58:58</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2021-12-16 14:58:58</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>376</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>ワシントン州ヤキマ・トレーニング・センター</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27473</th>\n",
       "      <td>こっちの方が注目されてるじゃないか。 https://t.co/kRRtL0INrR</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-11 12:29:00</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2021-12-11 12:29:00</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>371</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27474</th>\n",
       "      <td>#TOEIC 頻フレ247\\n104. us- cau-\\n注意する, 用心する\\n\\n解\\...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-04 03:06:00</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2021-12-04 03:06:00</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>364</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27475</th>\n",
       "      <td>Dior force jisoo to exercise😆 \\n\\n블랙핑크 지수 #KIM...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-01-17 08:27:49</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2022-01-17 08:27:49</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang  \\\n",
       "27471  RT @Aviation_Assets: アメリカ陸軍第7歩兵師団および陸上自衛隊がワシント...   ja   \n",
       "27472  アメリカ陸軍第7歩兵師団および陸上自衛隊がワシントン州ヤキマ・トレーニング・センターで実施し...   ja   \n",
       "27473         こっちの方が注目されてるじゃないか。 https://t.co/kRRtL0INrR   ja   \n",
       "27474  #TOEIC 頻フレ247\\n104. us- cau-\\n注意する, 用心する\\n\\n解\\...   ja   \n",
       "27475  Dior force jisoo to exercise😆 \\n\\n블랙핑크 지수 #KIM...   ko   \n",
       "\n",
       "               created_at tag             cat  scale                 Date  \\\n",
       "27471 2021-12-16 16:31:28  GN  force exercise   -7.6  2021-12-16 16:31:28   \n",
       "27472 2021-12-16 14:58:58  GN  force exercise   -7.6  2021-12-16 14:58:58   \n",
       "27473 2021-12-11 12:29:00  GN  force exercise   -7.6  2021-12-11 12:29:00   \n",
       "27474 2021-12-04 03:06:00  GN  force exercise   -7.6  2021-12-04 03:06:00   \n",
       "27475 2022-01-17 08:27:49  GN  force exercise   -7.6  2022-01-17 08:27:49   \n",
       "\n",
       "       Year Month Day  Month_Day  TweetNumber  Compound  Sentiment  \\\n",
       "27471  2021    12  16        376           26       NaN         -1   \n",
       "27472  2021    12  16        376           27       NaN         -1   \n",
       "27473  2021    12  11        371           28       NaN         -1   \n",
       "27474  2021    12   4        364           29       NaN         -1   \n",
       "27475  2022     1  17         47            0       NaN          1   \n",
       "\n",
       "                     textloc label  \n",
       "27471  ワシントン州ヤキマ・トレーニング・センター   GPE  \n",
       "27472  ワシントン州ヤキマ・トレーニング・センター   GPE  \n",
       "27473                    NaN   NaN  \n",
       "27474                    NaN   NaN  \n",
       "27475                          NaN  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So full data can give us the count of text locations over time \n",
    "hist_3610_end.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need Section to Group by tweets for analysis\n",
    "#https://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "hist_grouped_2 = hist_3610_end.groupby([\"lang\", \"textloc\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count_ner</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th>textloc</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ja</th>\n",
       "      <th>アメリカ</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ワシントン州ヤキマ・トレーニング・センター</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>日本</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>陸自</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ko</th>\n",
       "      <th></th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            count_ner\n",
       "lang textloc                         \n",
       "ja   アメリカ                          20\n",
       "     ワシントン州ヤキマ・トレーニング・センター          2\n",
       "     日本                             1\n",
       "     陸自                             1\n",
       "ko                                  1"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series(hist_grouped_2, name = \"count_ner\")\n",
    "hgdf = s.to_frame()\n",
    "hgdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 561 entries, ('en', '#Korea') to ('ko', '')\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   count_ner  561 non-null    int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 10.5+ KB\n"
     ]
    }
   ],
   "source": [
    "hgdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>textloc</th>\n",
       "      <th>count_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>en</td>\n",
       "      <td>Martín</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>en</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>en</td>\n",
       "      <td>Manila</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>en</td>\n",
       "      <td>Mamata</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>ko</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang    textloc  count_ner\n",
       "556   en     Martín          1\n",
       "557   en  Marseille          1\n",
       "558   en     Manila          1\n",
       "559   en     Mamata          1\n",
       "560   ko                     1"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgdf2 = hgdf.sort_values('count_ner', ascending = False)\n",
    "hgdf3 = hgdf2.reset_index()\n",
    "hgdf3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:15:25.044023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:17:48.850046\n",
      "0:02:23.806023\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(start)\n",
    "hgdf4 = translate_to_english(hgdf3)\n",
    "hgdf4.head(10)\n",
    "end = datetime.now()\n",
    "print(end)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>lang_y</th>\n",
       "      <th>count_ner</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28348</th>\n",
       "      <td>RT @Aviation_Assets: アメリカ陸軍第7歩兵師団および陸上自衛隊がワシント...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-16 16:31:28</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2021-12-16 16:31:28</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>376</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>ワシントン州ヤキマ・トレーニング・センター</td>\n",
       "      <td>GPE</td>\n",
       "      <td>ja</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yakima Training Center, Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28349</th>\n",
       "      <td>アメリカ陸軍第7歩兵師団および陸上自衛隊がワシントン州ヤキマ・トレーニング・センターで実施し...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-16 14:58:58</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2021-12-16 14:58:58</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>376</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>ワシントン州ヤキマ・トレーニング・センター</td>\n",
       "      <td>GPE</td>\n",
       "      <td>ja</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Yakima Training Center, Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28350</th>\n",
       "      <td>こっちの方が注目されてるじゃないか。 https://t.co/kRRtL0INrR</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-11 12:29:00</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2021-12-11 12:29:00</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>371</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28351</th>\n",
       "      <td>#TOEIC 頻フレ247\\n104. us- cau-\\n注意する, 用心する\\n\\n解\\...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2021-12-04 03:06:00</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2021-12-04 03:06:00</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>364</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28352</th>\n",
       "      <td>Dior force jisoo to exercise😆 \\n\\n블랙핑크 지수 #KIM...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-01-17 08:27:49</td>\n",
       "      <td>GN</td>\n",
       "      <td>force exercise</td>\n",
       "      <td>-7.6</td>\n",
       "      <td>2022-01-17 08:27:49</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>ko</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang_x  \\\n",
       "28348  RT @Aviation_Assets: アメリカ陸軍第7歩兵師団および陸上自衛隊がワシント...     ja   \n",
       "28349  アメリカ陸軍第7歩兵師団および陸上自衛隊がワシントン州ヤキマ・トレーニング・センターで実施し...     ja   \n",
       "28350         こっちの方が注目されてるじゃないか。 https://t.co/kRRtL0INrR     ja   \n",
       "28351  #TOEIC 頻フレ247\\n104. us- cau-\\n注意する, 用心する\\n\\n解\\...     ja   \n",
       "28352  Dior force jisoo to exercise😆 \\n\\n블랙핑크 지수 #KIM...     ko   \n",
       "\n",
       "               created_at tag             cat  scale                 Date  \\\n",
       "28348 2021-12-16 16:31:28  GN  force exercise   -7.6  2021-12-16 16:31:28   \n",
       "28349 2021-12-16 14:58:58  GN  force exercise   -7.6  2021-12-16 14:58:58   \n",
       "28350 2021-12-11 12:29:00  GN  force exercise   -7.6  2021-12-11 12:29:00   \n",
       "28351 2021-12-04 03:06:00  GN  force exercise   -7.6  2021-12-04 03:06:00   \n",
       "28352 2022-01-17 08:27:49  GN  force exercise   -7.6  2022-01-17 08:27:49   \n",
       "\n",
       "       Year Month Day  Month_Day  TweetNumber  Compound  Sentiment  \\\n",
       "28348  2021    12  16        376           26       NaN         -1   \n",
       "28349  2021    12  16        376           27       NaN         -1   \n",
       "28350  2021    12  11        371           28       NaN         -1   \n",
       "28351  2021    12   4        364           29       NaN         -1   \n",
       "28352  2022     1  17         47            0       NaN          1   \n",
       "\n",
       "                     textloc label lang_y  count_ner  \\\n",
       "28348  ワシントン州ヤキマ・トレーニング・センター   GPE     ja        2.0   \n",
       "28349  ワシントン州ヤキマ・トレーニング・センター   GPE     ja        2.0   \n",
       "28350                    NaN   NaN    NaN        NaN   \n",
       "28351                    NaN   NaN    NaN        NaN   \n",
       "28352                          NaN     ko        1.0   \n",
       "\n",
       "                               textloc_en  \n",
       "28348  Yakima Training Center, Washington  \n",
       "28349  Yakima Training Center, Washington  \n",
       "28350                                 NaN  \n",
       "28351                                 NaN  \n",
       "28352                                      "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgdf5 = pd.merge(hist_3610_end, hgdf4, on = \"textloc\", how = \"left\")\n",
    "hgdf5.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgdf5.to_json(r\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Processed_Data/hgdf3610_end.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the data from memory\n",
    "del hgdf5, hist_3610_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>RT @SandaBlueDeux: Russia asked China for mili...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-03-14 00:33:41</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>RT @lapatina_: Russia allegedly asking China f...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-03-14 00:33:39</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>Russia asked China for military, economic aid ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-03-14 00:33:31</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>RT @lapatina_: Russia allegedly asking China f...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-03-14 00:33:07</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>🇺🇸 faced a similar choice in 1917: give econom...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-03-14 00:33:05</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang  \\\n",
       "99995  RT @SandaBlueDeux: Russia asked China for mili...   en   \n",
       "99996  RT @lapatina_: Russia allegedly asking China f...   en   \n",
       "99997  Russia asked China for military, economic aid ...   en   \n",
       "99998  RT @lapatina_: Russia allegedly asking China f...   en   \n",
       "99999  🇺🇸 faced a similar choice in 1917: give econom...   en   \n",
       "\n",
       "               created_at tag           cat  scale Date  \n",
       "99995 2022-03-14 00:33:41  GP  economic aid    7.4  NaT  \n",
       "99996 2022-03-14 00:33:39  GP  economic aid    7.4  NaT  \n",
       "99997 2022-03-14 00:33:31  GP  economic aid    7.4  NaT  \n",
       "99998 2022-03-14 00:33:07  GP  economic aid    7.4  NaT  \n",
       "99999 2022-03-14 00:33:05  GP  economic aid    7.4  NaT  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3410,000 - 3509,999\n",
    "full_3410_3510 = full_5.loc[3410000:3509999]\n",
    "full_3410_3510_two = full_3410_3510.reset_index(drop = True)\n",
    "full_3410_3510_two.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 12:24:59.765548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 130ms/step\n",
      "5/5 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 13:22:55.118953\n",
      "0:57:55.353405\n"
     ]
    }
   ],
   "source": [
    "#Run the third batch\n",
    "start = datetime.now()\n",
    "print(start)\n",
    "hist_3410_3510 = main_full(full_3410_3510_two)\n",
    "end = datetime.now()\n",
    "print(end)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137476</th>\n",
       "      <td>RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-04-16 01:52:50</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2022-04-16 01:52:50</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>136</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>독일</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137477</th>\n",
       "      <td>RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-04-16 01:28:19</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2022-04-16 01:28:19</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>136</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>독일</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137478</th>\n",
       "      <td>RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-04-16 01:09:56</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2022-04-16 01:09:56</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>136</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>독일</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137479</th>\n",
       "      <td>RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-04-16 00:55:39</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2022-04-16 00:55:39</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>136</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>독일</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137480</th>\n",
       "      <td>독일 연방 경제 기후부는 2025년까지 전기차 보조금을 단계적으로 폐지하고 당장 올...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-04-16 00:49:03</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2022-04-16 00:49:03</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>136</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>독일</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text lang  \\\n",
       "137476  RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...   ko   \n",
       "137477  RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...   ko   \n",
       "137478  RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...   ko   \n",
       "137479  RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...   ko   \n",
       "137480  독일 연방 경제 기후부는 2025년까지 전기차 보조금을 단계적으로 폐지하고 당장 올...   ko   \n",
       "\n",
       "                created_at tag           cat  scale                 Date  \\\n",
       "137476 2022-04-16 01:52:50  GP  economic aid    7.4  2022-04-16 01:52:50   \n",
       "137477 2022-04-16 01:28:19  GP  economic aid    7.4  2022-04-16 01:28:19   \n",
       "137478 2022-04-16 01:09:56  GP  economic aid    7.4  2022-04-16 01:09:56   \n",
       "137479 2022-04-16 00:55:39  GP  economic aid    7.4  2022-04-16 00:55:39   \n",
       "137480 2022-04-16 00:49:03  GP  economic aid    7.4  2022-04-16 00:49:03   \n",
       "\n",
       "        Year Month Day  Month_Day  TweetNumber  Compound  Sentiment textloc  \\\n",
       "137476  2022     4  16        136           13       NaN         -1      독일   \n",
       "137477  2022     4  16        136           14       NaN         -1      독일   \n",
       "137478  2022     4  16        136           15       NaN         -1      독일   \n",
       "137479  2022     4  16        136           16       NaN         -1      독일   \n",
       "137480  2022     4  16        136           17       NaN         -1      독일   \n",
       "\n",
       "       label  \n",
       "137476   NaN  \n",
       "137477   NaN  \n",
       "137478   NaN  \n",
       "137479   NaN  \n",
       "137480   NaN  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So full data can give us the count of text locations over time \n",
    "hist_3410_3510.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need Section to Group by tweets for analysis\n",
    "#https://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "hist_grouped_2 = hist_3410_3510.groupby([\"lang\", \"textloc\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count_ner</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th>textloc</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">pt</th>\n",
       "      <th>Russia</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Russia in</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rússia</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ucrânia</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                count_ner\n",
       "lang textloc             \n",
       "pt   Russia             1\n",
       "     Russia in          1\n",
       "     Rússia             2\n",
       "     US                 1\n",
       "     Ucrânia            4"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series(hist_grouped_2, name = \"count_ner\")\n",
    "hgdf = s.to_frame()\n",
    "hgdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 971 entries, ('ar', '-') to ('pt', 'Ucrânia')\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   count_ner  971 non-null    int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 18.1+ KB\n"
     ]
    }
   ],
   "source": [
    "hgdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>textloc</th>\n",
       "      <th>count_ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>en</td>\n",
       "      <td>Pkaistan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>en</td>\n",
       "      <td>Playoffs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>en</td>\n",
       "      <td>Arsenal-Kiev'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>en</td>\n",
       "      <td>Politico</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>en</td>\n",
       "      <td>Notary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang        textloc  count_ner\n",
       "966   en       Pkaistan          1\n",
       "967   en       Playoffs          1\n",
       "968   en  Arsenal-Kiev'          1\n",
       "969   en       Politico          1\n",
       "970   en         Notary          1"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgdf2 = hgdf.sort_values('count_ner', ascending = False)\n",
    "hgdf3 = hgdf2.reset_index()\n",
    "hgdf3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 13:22:55.269462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-02 13:27:14.110911\n",
      "0:04:18.841449\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(start)\n",
    "hgdf4 = translate_to_english(hgdf3)\n",
    "end = datetime.now()\n",
    "print(end)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>textloc</th>\n",
       "      <th>count_ner</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>en</td>\n",
       "      <td>Pkaistan</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm packing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>en</td>\n",
       "      <td>Playoffs</td>\n",
       "      <td>1</td>\n",
       "      <td>Playoffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>en</td>\n",
       "      <td>Arsenal-Kiev'</td>\n",
       "      <td>1</td>\n",
       "      <td>Arsenal-Kiev'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>en</td>\n",
       "      <td>Politico</td>\n",
       "      <td>1</td>\n",
       "      <td>Politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>en</td>\n",
       "      <td>Notary</td>\n",
       "      <td>1</td>\n",
       "      <td>Notary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang        textloc  count_ner     textloc_en\n",
       "966   en       Pkaistan          1    I'm packing\n",
       "967   en       Playoffs          1       Playoffs\n",
       "968   en  Arsenal-Kiev'          1  Arsenal-Kiev'\n",
       "969   en       Politico          1       Politico\n",
       "970   en         Notary          1         Notary"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgdf4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>textloc</th>\n",
       "      <th>count_ner</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>21176</td>\n",
       "      <td>Ukraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>Russia</td>\n",
       "      <td>12665</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>China</td>\n",
       "      <td>12125</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>US</td>\n",
       "      <td>4394</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>UK</td>\n",
       "      <td>3935</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>en</td>\n",
       "      <td>Ethiopia</td>\n",
       "      <td>3142</td>\n",
       "      <td>Ethiopia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>2889</td>\n",
       "      <td>U.S.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en</td>\n",
       "      <td>Japan</td>\n",
       "      <td>1241</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>😪</td>\n",
       "      <td>1011</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en</td>\n",
       "      <td>India</td>\n",
       "      <td>866</td>\n",
       "      <td>India</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang   textloc  count_ner textloc_en\n",
       "0   en   Ukraine      21176    Ukraine\n",
       "1   en    Russia      12665     Russia\n",
       "2   en     China      12125      China\n",
       "3   en        US       4394         US\n",
       "4   en        UK       3935         UK\n",
       "5   en  Ethiopia       3142   Ethiopia\n",
       "6   en      U.S.       2889       U.S.\n",
       "7   en     Japan       1241      Japan\n",
       "8   en         😪       1011       None\n",
       "9   en     India        866      India"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgdf4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>lang_y</th>\n",
       "      <th>count_ner</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217811</th>\n",
       "      <td>RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-04-16 01:52:50</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2022-04-16 01:52:50</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>136</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>독일</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ko</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217812</th>\n",
       "      <td>RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-04-16 01:28:19</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2022-04-16 01:28:19</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>136</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>독일</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ko</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217813</th>\n",
       "      <td>RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-04-16 01:09:56</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2022-04-16 01:09:56</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>136</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>독일</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ko</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217814</th>\n",
       "      <td>RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-04-16 00:55:39</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2022-04-16 00:55:39</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>136</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>독일</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ko</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217815</th>\n",
       "      <td>독일 연방 경제 기후부는 2025년까지 전기차 보조금을 단계적으로 폐지하고 당장 올...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-04-16 00:49:03</td>\n",
       "      <td>GP</td>\n",
       "      <td>economic aid</td>\n",
       "      <td>7.4</td>\n",
       "      <td>2022-04-16 00:49:03</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>136</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>독일</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ko</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text lang_x  \\\n",
       "217811  RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...     ko   \n",
       "217812  RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...     ko   \n",
       "217813  RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...     ko   \n",
       "217814  RT @ecolibertas: 독일 연방 경제 기후부는 2025년까지 전기차 보조금...     ko   \n",
       "217815  독일 연방 경제 기후부는 2025년까지 전기차 보조금을 단계적으로 폐지하고 당장 올...     ko   \n",
       "\n",
       "                created_at tag           cat  scale                 Date  \\\n",
       "217811 2022-04-16 01:52:50  GP  economic aid    7.4  2022-04-16 01:52:50   \n",
       "217812 2022-04-16 01:28:19  GP  economic aid    7.4  2022-04-16 01:28:19   \n",
       "217813 2022-04-16 01:09:56  GP  economic aid    7.4  2022-04-16 01:09:56   \n",
       "217814 2022-04-16 00:55:39  GP  economic aid    7.4  2022-04-16 00:55:39   \n",
       "217815 2022-04-16 00:49:03  GP  economic aid    7.4  2022-04-16 00:49:03   \n",
       "\n",
       "        Year Month Day  Month_Day  TweetNumber  Compound  Sentiment textloc  \\\n",
       "217811  2022     4  16        136           13       NaN         -1      독일   \n",
       "217812  2022     4  16        136           14       NaN         -1      독일   \n",
       "217813  2022     4  16        136           15       NaN         -1      독일   \n",
       "217814  2022     4  16        136           16       NaN         -1      독일   \n",
       "217815  2022     4  16        136           17       NaN         -1      독일   \n",
       "\n",
       "       label lang_y  count_ner textloc_en  \n",
       "217811   NaN     ko        9.0    Germany  \n",
       "217812   NaN     ko        9.0    Germany  \n",
       "217813   NaN     ko        9.0    Germany  \n",
       "217814   NaN     ko        9.0    Germany  \n",
       "217815   NaN     ko        9.0    Germany  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgdf5 = pd.merge(hist_3410_3510, hgdf4, on = \"textloc\", how = \"left\")\n",
    "hgdf5.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgdf5.to_json(r\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Processed_Data/hgdf3410_3510.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the data from memory\n",
    "del hgdf5, hist_3410_3510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COmbine all the files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Create different pathways\n",
    "path_to_json_hist = \"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Processed_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Get all files from Json_file output\n",
    "def all_files(path_to_json):\n",
    "    json_pattern = os.path.join(path_to_json, '*.json')\n",
    "    file_list = glob.glob(json_pattern)\n",
    "    #Import the jsons and convert them to pandas dataframes\n",
    "    dfs = []\n",
    "    for file in file_list:\n",
    "        data = pd.read_json(file)\n",
    "        dfs.append(data)\n",
    "    #Concat the individual data frames into one dataframe\n",
    "    full_tm = pd.concat(dfs, ignore_index = True)\n",
    "    #Reset Index\n",
    "    ftm2 = full_tm.reset_index(drop = True)\n",
    "    return ftm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>lang_y</th>\n",
       "      <th>count_ner</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8173484</th>\n",
       "      <td>さて\\nｵﾏｲ達の健康に\\n影響のある事をオイラは一応、氣にしてやっているノ\\n\\nなの?(...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-08 02:22:47</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022-03-08 02:22:47</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173485</th>\n",
       "      <td>RT @melt_myself: ミャンマー国連大使 暗殺計画の続報\\nさすがの Myanm...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 21:32:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022-03-07 21:32:20</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173486</th>\n",
       "      <td>【画像・動画】ロシア軍、隠していた戦車を火炎瓶攻撃により全滅させられる衝撃映像がこちら・・・...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 06:48:17</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022-03-07 06:48:17</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173487</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:28:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022-03-09 04:28:20</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>ko</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173488</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:20:48</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022-03-09 04:20:48</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>ko</td>\n",
       "      <td>2.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text lang_x  \\\n",
       "8173484  さて\\nｵﾏｲ達の健康に\\n影響のある事をオイラは一応、氣にしてやっているノ\\n\\nなの?(...     ja   \n",
       "8173485  RT @melt_myself: ミャンマー国連大使 暗殺計画の続報\\nさすがの Myanm...     ja   \n",
       "8173486  【画像・動画】ロシア軍、隠していた戦車を火炎瓶攻撃により全滅させられる衝撃映像がこちら・・・...     ja   \n",
       "8173487  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "8173488  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "\n",
       "                 created_at tag              cat  scale                Date  \\\n",
       "8173484 2022-03-08 02:22:47  GN  military attack  -10.0 2022-03-08 02:22:47   \n",
       "8173485 2022-03-07 21:32:20  GN  military attack  -10.0 2022-03-07 21:32:20   \n",
       "8173486 2022-03-07 06:48:17  GN  military attack  -10.0 2022-03-07 06:48:17   \n",
       "8173487 2022-03-09 04:28:20  GN  military attack  -10.0 2022-03-09 04:28:20   \n",
       "8173488 2022-03-09 04:20:48  GN  military attack  -10.0 2022-03-09 04:20:48   \n",
       "\n",
       "         Year  Month  Day  Month_Day  TweetNumber  Compound  Sentiment  \\\n",
       "8173484  2022      3    8         98           57       NaN         -1   \n",
       "8173485  2022      3    7         97           58       NaN         -1   \n",
       "8173486  2022      3    7         97           59       NaN          1   \n",
       "8173487  2022      3    9         99            0       NaN         -1   \n",
       "8173488  2022      3    9         99            1       NaN          1   \n",
       "\n",
       "        textloc label lang_y  count_ner textloc_en  \n",
       "8173484    None  None   None        NaN       None  \n",
       "8173485    None  None   None        NaN       None  \n",
       "8173486    None  None   None        NaN       None  \n",
       "8173487          None     ko        2.0             \n",
       "8173488          None     ko        2.0             "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3: Combine the files for French Negative\n",
    "tmfull_hist = all_files(path_to_json_hist)\n",
    "tmfull_hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8173489 entries, 0 to 8173488\n",
      "Data columns (total 19 columns):\n",
      " #   Column       Dtype         \n",
      "---  ------       -----         \n",
      " 0   text         object        \n",
      " 1   lang_x       object        \n",
      " 2   created_at   datetime64[ns]\n",
      " 3   tag          object        \n",
      " 4   cat          object        \n",
      " 5   scale        float64       \n",
      " 6   Date         datetime64[ns]\n",
      " 7   Year         int64         \n",
      " 8   Month        int64         \n",
      " 9   Day          int64         \n",
      " 10  Month_Day    int64         \n",
      " 11  TweetNumber  int64         \n",
      " 12  Compound     float64       \n",
      " 13  Sentiment    int64         \n",
      " 14  textloc      object        \n",
      " 15  label        object        \n",
      " 16  lang_y       object        \n",
      " 17  count_ner    float64       \n",
      " 18  textloc_en   object        \n",
      "dtypes: datetime64[ns](2), float64(3), int64(6), object(8)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "tmfull_hist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmfull_hist[\"Year\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Compound</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>lang_y</th>\n",
       "      <th>count_ner</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @caitoz: Still hilarious that the United St...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-02 06:24:13</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022-04-02 06:24:13</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>1</td>\n",
       "      <td>the United States of America</td>\n",
       "      <td>GPE</td>\n",
       "      <td>en</td>\n",
       "      <td>393.0</td>\n",
       "      <td>the United States of America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @olliecarroll: New: Ukraine says Russia is ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-02 06:24:04</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022-04-02 06:24:04</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>GPE</td>\n",
       "      <td>en</td>\n",
       "      <td>56043.0</td>\n",
       "      <td>Ukraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @olliecarroll: New: Ukraine says Russia is ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-02 06:24:04</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022-04-02 06:24:04</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>GPE</td>\n",
       "      <td>fr</td>\n",
       "      <td>158.0</td>\n",
       "      <td>Ukraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @olliecarroll: New: Ukraine says Russia is ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-02 06:24:04</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022-04-02 06:24:04</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>GPE</td>\n",
       "      <td>ja</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Ukraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @olliecarroll: New: Ukraine says Russia is ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-02 06:24:04</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022-04-02 06:24:04</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>GPE</td>\n",
       "      <td>es</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ukraine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text lang_x  \\\n",
       "0  RT @caitoz: Still hilarious that the United St...     en   \n",
       "1  RT @olliecarroll: New: Ukraine says Russia is ...     en   \n",
       "2  RT @olliecarroll: New: Ukraine says Russia is ...     en   \n",
       "3  RT @olliecarroll: New: Ukraine says Russia is ...     en   \n",
       "4  RT @olliecarroll: New: Ukraine says Russia is ...     en   \n",
       "\n",
       "           created_at tag                cat  scale                Date  Year  \\\n",
       "0 2022-04-02 06:24:13  GN  military invasion  -11.0 2022-04-02 06:24:13  2022   \n",
       "1 2022-04-02 06:24:04  GN  military invasion  -11.0 2022-04-02 06:24:04  2022   \n",
       "2 2022-04-02 06:24:04  GN  military invasion  -11.0 2022-04-02 06:24:04  2022   \n",
       "3 2022-04-02 06:24:04  GN  military invasion  -11.0 2022-04-02 06:24:04  2022   \n",
       "4 2022-04-02 06:24:04  GN  military invasion  -11.0 2022-04-02 06:24:04  2022   \n",
       "\n",
       "   Month  Day  Month_Day  TweetNumber  Compound  Sentiment  \\\n",
       "0      4    2        122            0    0.7003          1   \n",
       "1      4    2        122            1    0.0000          0   \n",
       "2      4    2        122            1    0.0000          0   \n",
       "3      4    2        122            1    0.0000          0   \n",
       "4      4    2        122            1    0.0000          0   \n",
       "\n",
       "                        textloc label lang_y  count_ner  \\\n",
       "0  the United States of America   GPE     en      393.0   \n",
       "1                       Ukraine   GPE     en    56043.0   \n",
       "2                       Ukraine   GPE     fr      158.0   \n",
       "3                       Ukraine   GPE     ja        2.0   \n",
       "4                       Ukraine   GPE     es        1.0   \n",
       "\n",
       "                     textloc_en  \n",
       "0  the United States of America  \n",
       "1                       Ukraine  \n",
       "2                       Ukraine  \n",
       "3                       Ukraine  \n",
       "4                       Ukraine  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmfull_hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8173484</th>\n",
       "      <td>さて\\nｵﾏｲ達の健康に\\n影響のある事をオイラは一応、氣にしてやっているノ\\n\\nなの?(...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-08 02:22:47</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173485</th>\n",
       "      <td>RT @melt_myself: ミャンマー国連大使 暗殺計画の続報\\nさすがの Myanm...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 21:32:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173486</th>\n",
       "      <td>【画像・動画】ロシア軍、隠していた戦車を火炎瓶攻撃により全滅させられる衝撃映像がこちら・・・...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 06:48:17</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173487</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:28:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173488</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:20:48</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text lang_x  \\\n",
       "8173484  さて\\nｵﾏｲ達の健康に\\n影響のある事をオイラは一応、氣にしてやっているノ\\n\\nなの?(...     ja   \n",
       "8173485  RT @melt_myself: ミャンマー国連大使 暗殺計画の続報\\nさすがの Myanm...     ja   \n",
       "8173486  【画像・動画】ロシア軍、隠していた戦車を火炎瓶攻撃により全滅させられる衝撃映像がこちら・・・...     ja   \n",
       "8173487  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "8173488  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "\n",
       "                 created_at tag              cat  scale  Year  Month  Day  \\\n",
       "8173484 2022-03-08 02:22:47  GN  military attack  -10.0  2022      3    8   \n",
       "8173485 2022-03-07 21:32:20  GN  military attack  -10.0  2022      3    7   \n",
       "8173486 2022-03-07 06:48:17  GN  military attack  -10.0  2022      3    7   \n",
       "8173487 2022-03-09 04:28:20  GN  military attack  -10.0  2022      3    9   \n",
       "8173488 2022-03-09 04:20:48  GN  military attack  -10.0  2022      3    9   \n",
       "\n",
       "         Month_Day  TweetNumber  Sentiment textloc label textloc_en  \n",
       "8173484         98           57         -1    None  None       None  \n",
       "8173485         97           58         -1    None  None       None  \n",
       "8173486         97           59          1    None  None       None  \n",
       "8173487         99            0         -1          None             \n",
       "8173488         99            1          1          None             "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove unneed columns\n",
    "tmfull_hist_2 = tmfull_hist.drop(columns = [\"lang_y\", \"Compound\", \"count_ner\", \"Date\"])\n",
    "tmfull_hist_2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8173484</th>\n",
       "      <td>さて\\nｵﾏｲ達の健康に\\n影響のある事をオイラは一応、氣にしてやっているノ\\n\\nなの?(...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-08 02:22:47</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173485</th>\n",
       "      <td>RT @melt_myself: ミャンマー国連大使 暗殺計画の続報\\nさすがの Myanm...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 21:32:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173486</th>\n",
       "      <td>【画像・動画】ロシア軍、隠していた戦車を火炎瓶攻撃により全滅させられる衝撃映像がこちら・・・...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 06:48:17</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173487</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:28:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173488</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:20:48</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text lang_x  \\\n",
       "8173484  さて\\nｵﾏｲ達の健康に\\n影響のある事をオイラは一応、氣にしてやっているノ\\n\\nなの?(...     ja   \n",
       "8173485  RT @melt_myself: ミャンマー国連大使 暗殺計画の続報\\nさすがの Myanm...     ja   \n",
       "8173486  【画像・動画】ロシア軍、隠していた戦車を火炎瓶攻撃により全滅させられる衝撃映像がこちら・・・...     ja   \n",
       "8173487  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "8173488  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "\n",
       "                 created_at tag              cat  scale  Year  Month  Day  \\\n",
       "8173484 2022-03-08 02:22:47  GN  military attack  -10.0  2022      3    8   \n",
       "8173485 2022-03-07 21:32:20  GN  military attack  -10.0  2022      3    7   \n",
       "8173486 2022-03-07 06:48:17  GN  military attack  -10.0  2022      3    7   \n",
       "8173487 2022-03-09 04:28:20  GN  military attack  -10.0  2022      3    9   \n",
       "8173488 2022-03-09 04:20:48  GN  military attack  -10.0  2022      3    9   \n",
       "\n",
       "         Month_Day  TweetNumber  Sentiment textloc label textloc_en  \n",
       "8173484         98           57         -1    None  None       None  \n",
       "8173485         97           58         -1    None  None       None  \n",
       "8173486         97           59          1    None  None       None  \n",
       "8173487         99            0         -1          None             \n",
       "8173488         99            1          1          None             "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop the duplicates for the sentiment analysis\n",
    "tmfull_hist_no_dups = tmfull_hist_2.drop_duplicates(subset = [\"text\", \"created_at\"], keep = \"last\")\n",
    "tmfull_hist_no_dups.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3314041</th>\n",
       "      <td>さて\\nｵﾏｲ達の健康に\\n影響のある事をオイラは一応、氣にしてやっているノ\\n\\nなの?(...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-08 02:22:47</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314042</th>\n",
       "      <td>RT @melt_myself: ミャンマー国連大使 暗殺計画の続報\\nさすがの Myanm...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 21:32:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314043</th>\n",
       "      <td>【画像・動画】ロシア軍、隠していた戦車を火炎瓶攻撃により全滅させられる衝撃映像がこちら・・・...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 06:48:17</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314044</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:28:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314045</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:20:48</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text lang_x  \\\n",
       "3314041  さて\\nｵﾏｲ達の健康に\\n影響のある事をオイラは一応、氣にしてやっているノ\\n\\nなの?(...     ja   \n",
       "3314042  RT @melt_myself: ミャンマー国連大使 暗殺計画の続報\\nさすがの Myanm...     ja   \n",
       "3314043  【画像・動画】ロシア軍、隠していた戦車を火炎瓶攻撃により全滅させられる衝撃映像がこちら・・・...     ja   \n",
       "3314044  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "3314045  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "\n",
       "                 created_at tag              cat  scale  Year  Month  Day  \\\n",
       "3314041 2022-03-08 02:22:47  GN  military attack  -10.0  2022      3    8   \n",
       "3314042 2022-03-07 21:32:20  GN  military attack  -10.0  2022      3    7   \n",
       "3314043 2022-03-07 06:48:17  GN  military attack  -10.0  2022      3    7   \n",
       "3314044 2022-03-09 04:28:20  GN  military attack  -10.0  2022      3    9   \n",
       "3314045 2022-03-09 04:20:48  GN  military attack  -10.0  2022      3    9   \n",
       "\n",
       "         Month_Day  TweetNumber  Sentiment textloc label textloc_en  \n",
       "3314041         98           57         -1    None  None       None  \n",
       "3314042         97           58         -1    None  None       None  \n",
       "3314043         97           59          1    None  None       None  \n",
       "3314044         99            0         -1          None             \n",
       "3314045         99            1          1          None             "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reset index\n",
    "tmfull_hist_no_dups_2 = tmfull_hist_no_dups.reset_index(drop = True)\n",
    "tmfull_hist_no_dups_2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output the dataframe\n",
    "tmfull_hist_no_dups_2.to_json(r\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Processed_Full/Hist_Full_No_Dups.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8173484</th>\n",
       "      <td>さて\\nｵﾏｲ達の健康に\\n影響のある事をオイラは一応、氣にしてやっているノ\\n\\nなの?(...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-08 02:22:47</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173485</th>\n",
       "      <td>RT @melt_myself: ミャンマー国連大使 暗殺計画の続報\\nさすがの Myanm...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 21:32:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173486</th>\n",
       "      <td>【画像・動画】ロシア軍、隠していた戦車を火炎瓶攻撃により全滅させられる衝撃映像がこちら・・・...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 06:48:17</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173487</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:28:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173488</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:20:48</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text lang_x  \\\n",
       "8173484  さて\\nｵﾏｲ達の健康に\\n影響のある事をオイラは一応、氣にしてやっているノ\\n\\nなの?(...     ja   \n",
       "8173485  RT @melt_myself: ミャンマー国連大使 暗殺計画の続報\\nさすがの Myanm...     ja   \n",
       "8173486  【画像・動画】ロシア軍、隠していた戦車を火炎瓶攻撃により全滅させられる衝撃映像がこちら・・・...     ja   \n",
       "8173487  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "8173488  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "\n",
       "                 created_at tag              cat  scale  Year  Month  Day  \\\n",
       "8173484 2022-03-08 02:22:47  GN  military attack  -10.0  2022      3    8   \n",
       "8173485 2022-03-07 21:32:20  GN  military attack  -10.0  2022      3    7   \n",
       "8173486 2022-03-07 06:48:17  GN  military attack  -10.0  2022      3    7   \n",
       "8173487 2022-03-09 04:28:20  GN  military attack  -10.0  2022      3    9   \n",
       "8173488 2022-03-09 04:20:48  GN  military attack  -10.0  2022      3    9   \n",
       "\n",
       "         Month_Day  TweetNumber  Sentiment textloc label textloc_en  \n",
       "8173484         98           57         -1    None  None       None  \n",
       "8173485         97           58         -1    None  None       None  \n",
       "8173486         97           59          1    None  None       None  \n",
       "8173487         99            0         -1          None             \n",
       "8173488         99            1          1          None             "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmfull_hist_2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GN', 'GP'], dtype=object)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmfull_hist_2[\"tag\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8173484</th>\n",
       "      <td>さて\\nｵﾏｲ達の健康に\\n影響のある事をオイラは一応、氣にしてやっているノ\\n\\nなの?(...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-08 02:22:47</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173485</th>\n",
       "      <td>RT @melt_myself: ミャンマー国連大使 暗殺計画の続報\\nさすがの Myanm...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 21:32:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173486</th>\n",
       "      <td>【画像・動画】ロシア軍、隠していた戦車を火炎瓶攻撃により全滅させられる衝撃映像がこちら・・・...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 06:48:17</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173487</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:28:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173488</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:20:48</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text lang_x  \\\n",
       "8173484  さて\\nｵﾏｲ達の健康に\\n影響のある事をオイラは一応、氣にしてやっているノ\\n\\nなの?(...     ja   \n",
       "8173485  RT @melt_myself: ミャンマー国連大使 暗殺計画の続報\\nさすがの Myanm...     ja   \n",
       "8173486  【画像・動画】ロシア軍、隠していた戦車を火炎瓶攻撃により全滅させられる衝撃映像がこちら・・・...     ja   \n",
       "8173487  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "8173488  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "\n",
       "                 created_at tag              cat  scale  Year  Month  Day  \\\n",
       "8173484 2022-03-08 02:22:47  GN  military attack  -10.0  2022      3    8   \n",
       "8173485 2022-03-07 21:32:20  GN  military attack  -10.0  2022      3    7   \n",
       "8173486 2022-03-07 06:48:17  GN  military attack  -10.0  2022      3    7   \n",
       "8173487 2022-03-09 04:28:20  GN  military attack  -10.0  2022      3    9   \n",
       "8173488 2022-03-09 04:20:48  GN  military attack  -10.0  2022      3    9   \n",
       "\n",
       "         Month_Day  TweetNumber  Sentiment textloc label textloc_en  \n",
       "8173484         98           57         -1    None  None       None  \n",
       "8173485         97           58         -1    None  None       None  \n",
       "8173486         97           59          1    None  None       None  \n",
       "8173487         99            0         -1          None             \n",
       "8173488         99            1          1          None             "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmfull_hist_3 = tmfull_hist_2.reset_index(drop = True)\n",
    "tmfull_hist_3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Json File\n",
    "tmfull_hist_3.to_json(r\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Processed_Full/Hist_Full_With_Dups.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
