{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Historical Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/johnc.burns/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/johnc.burns/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Part 1: All Libraries from the top\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import socket\n",
    "import sys\n",
    "import errno\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "#Part 2: Libraries for the getting the latest file\n",
    "import glob\n",
    "import os.path\n",
    "\n",
    "#Part 3: Libraries to get the time of when the batch is imported \n",
    "#https://www.programiz.com/python-programming/datetime/current-datetime\n",
    "from datetime import datetime\n",
    "\n",
    "#Part 5: English Analysis Libraries needed:\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentianalyzer = SentimentIntensityAnalyzer()\n",
    "#This time we will add the individual sentiment to the tweet\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#Import Google Maps API\n",
    "import googlemaps\n",
    "gmaps = googlemaps.Client(key = \"AIzaSyDntbz0oR6KLfWl_BYZj1LCTS_iQUWv1Cg\")\n",
    "\n",
    "#Part 6: Spanish Anaysis Libraries Needed:\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import joblib\n",
    "\n",
    "#Part 7: French Libraries Needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Part 9: Arabic Libraries Needed\n",
    "from transformers import pipeline\n",
    "\n",
    "#Part 10: Japanese Libraries Needed\n",
    "import csv\n",
    "\n",
    "#Part 11: Korean Libraries Needed\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import csv\n",
    "#Import konlpy\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "#Part 12: Library needed to append the new data file to the existing csv\n",
    "from csv import writer\n",
    "\n",
    "#Part 13: Library Create a map with Folium\n",
    "#http://python-visualization.github.io/folium/quickstart.html#Getting-Started\n",
    "import folium\n",
    "\n",
    "#Step 14: Library needed to translate the locations to English\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0: Libraries\n",
    "#Step 3: Topics over Time Libraries\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.test.utils\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tqdm\n",
    "#Random other libraries\n",
    "import requests\n",
    "import math\n",
    "\n",
    "#Step 4: Pretty Printing Library\n",
    "from pprint import pprint\n",
    "\n",
    "#Step 5: Making the Plot Library\n",
    "#Graphing in Jupiter Notebook\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: The Glob Libraries\n",
    "#https://stackoverflow.com/questions/57067551/how-to-read-multiple-json-files-into-pandas-dataframe\n",
    "#/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 9/foo5/json_file\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import math\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#Part 2: Lang Detect Libraries\n",
    "#Get the language labels for the dataset\n",
    "#Enforce consistent results \n",
    "from langdetect import DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "#Import the detect function\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Set the path to the json files\n",
    "counter_tm = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3314041</th>\n",
       "      <td>„Åï„Å¶\\nÔΩµÔæèÔΩ≤ÈÅî„ÅÆÂÅ•Â∫∑„Å´\\nÂΩ±Èüø„ÅÆ„ÅÇ„Çã‰∫ã„Çí„Ç™„Ç§„É©„ÅØ‰∏ÄÂøú„ÄÅÊ∞£„Å´„Åó„Å¶„ÇÑ„Å£„Å¶„ÅÑ„Çã„Éé\\n\\n„Å™„ÅÆ?(...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-08 02:22:47</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314042</th>\n",
       "      <td>RT @melt_myself: „Éü„É£„É≥„Éû„ÉºÂõΩÈÄ£Â§ß‰Ωø ÊöóÊÆ∫Ë®àÁîª„ÅÆÁ∂öÂ†±\\n„Åï„Åô„Åå„ÅÆ Myanm...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 21:32:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314043</th>\n",
       "      <td>„ÄêÁîªÂÉè„ÉªÂãïÁîª„Äë„É≠„Ç∑„Ç¢Ëªç„ÄÅÈö†„Åó„Å¶„ÅÑ„ÅüÊà¶Ëªä„ÇíÁÅ´ÁÇéÁì∂ÊîªÊíÉ„Å´„Çà„ÇäÂÖ®ÊªÖ„Åï„Åõ„Çâ„Çå„ÇãË°ùÊíÉÊò†ÂÉè„Åå„Åì„Å°„Çâ„Éª„Éª„Éª...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 06:48:17</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314044</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:28:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314045</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:20:48</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text lang_x  \\\n",
       "3314041  „Åï„Å¶\\nÔΩµÔæèÔΩ≤ÈÅî„ÅÆÂÅ•Â∫∑„Å´\\nÂΩ±Èüø„ÅÆ„ÅÇ„Çã‰∫ã„Çí„Ç™„Ç§„É©„ÅØ‰∏ÄÂøú„ÄÅÊ∞£„Å´„Åó„Å¶„ÇÑ„Å£„Å¶„ÅÑ„Çã„Éé\\n\\n„Å™„ÅÆ?(...     ja   \n",
       "3314042  RT @melt_myself: „Éü„É£„É≥„Éû„ÉºÂõΩÈÄ£Â§ß‰Ωø ÊöóÊÆ∫Ë®àÁîª„ÅÆÁ∂öÂ†±\\n„Åï„Åô„Åå„ÅÆ Myanm...     ja   \n",
       "3314043  „ÄêÁîªÂÉè„ÉªÂãïÁîª„Äë„É≠„Ç∑„Ç¢Ëªç„ÄÅÈö†„Åó„Å¶„ÅÑ„ÅüÊà¶Ëªä„ÇíÁÅ´ÁÇéÁì∂ÊîªÊíÉ„Å´„Çà„ÇäÂÖ®ÊªÖ„Åï„Åõ„Çâ„Çå„ÇãË°ùÊíÉÊò†ÂÉè„Åå„Åì„Å°„Çâ„Éª„Éª„Éª...     ja   \n",
       "3314044  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "3314045  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "\n",
       "                 created_at tag              cat  scale  Year  Month  Day  \\\n",
       "3314041 2022-03-08 02:22:47  GN  military attack  -10.0  2022      3    8   \n",
       "3314042 2022-03-07 21:32:20  GN  military attack  -10.0  2022      3    7   \n",
       "3314043 2022-03-07 06:48:17  GN  military attack  -10.0  2022      3    7   \n",
       "3314044 2022-03-09 04:28:20  GN  military attack  -10.0  2022      3    9   \n",
       "3314045 2022-03-09 04:20:48  GN  military attack  -10.0  2022      3    9   \n",
       "\n",
       "         Month_Day  TweetNumber  Sentiment textloc label textloc_en  \n",
       "3314041         98           57         -1    None  None       None  \n",
       "3314042         97           58         -1    None  None       None  \n",
       "3314043         97           59          1    None  None       None  \n",
       "3314044         99            0         -1          None             \n",
       "3314045         99            1          1          None             "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Output the dataframe\n",
    "df = pd.read_json(r\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Processed_Full/Hist_Full_No_Dups.json\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the first week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3099481</th>\n",
       "      <td>„Åï„Å¶\\nÔΩµÔæèÔΩ≤ÈÅî„ÅÆÂÅ•Â∫∑„Å´\\nÂΩ±Èüø„ÅÆ„ÅÇ„Çã‰∫ã„Çí„Ç™„Ç§„É©„ÅØ‰∏ÄÂøú„ÄÅÊ∞£„Å´„Åó„Å¶„ÇÑ„Å£„Å¶„ÅÑ„Çã„Éé\\n\\n„Å™„ÅÆ?(...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-08 02:22:47</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>98</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099482</th>\n",
       "      <td>RT @melt_myself: „Éü„É£„É≥„Éû„ÉºÂõΩÈÄ£Â§ß‰Ωø ÊöóÊÆ∫Ë®àÁîª„ÅÆÁ∂öÂ†±\\n„Åï„Åô„Åå„ÅÆ Myanm...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 21:32:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099483</th>\n",
       "      <td>„ÄêÁîªÂÉè„ÉªÂãïÁîª„Äë„É≠„Ç∑„Ç¢Ëªç„ÄÅÈö†„Åó„Å¶„ÅÑ„ÅüÊà¶Ëªä„ÇíÁÅ´ÁÇéÁì∂ÊîªÊíÉ„Å´„Çà„ÇäÂÖ®ÊªÖ„Åï„Åõ„Çâ„Çå„ÇãË°ùÊíÉÊò†ÂÉè„Åå„Åì„Å°„Çâ„Éª„Éª„Éª...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-03-07 06:48:17</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>97</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099484</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:28:20</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099485</th>\n",
       "      <td>Russia's economic sanctions hit 1 trillion yen...</td>\n",
       "      <td>ko</td>\n",
       "      <td>2022-03-09 04:20:48</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text lang_x  \\\n",
       "3099481  „Åï„Å¶\\nÔΩµÔæèÔΩ≤ÈÅî„ÅÆÂÅ•Â∫∑„Å´\\nÂΩ±Èüø„ÅÆ„ÅÇ„Çã‰∫ã„Çí„Ç™„Ç§„É©„ÅØ‰∏ÄÂøú„ÄÅÊ∞£„Å´„Åó„Å¶„ÇÑ„Å£„Å¶„ÅÑ„Çã„Éé\\n\\n„Å™„ÅÆ?(...     ja   \n",
       "3099482  RT @melt_myself: „Éü„É£„É≥„Éû„ÉºÂõΩÈÄ£Â§ß‰Ωø ÊöóÊÆ∫Ë®àÁîª„ÅÆÁ∂öÂ†±\\n„Åï„Åô„Åå„ÅÆ Myanm...     ja   \n",
       "3099483  „ÄêÁîªÂÉè„ÉªÂãïÁîª„Äë„É≠„Ç∑„Ç¢Ëªç„ÄÅÈö†„Åó„Å¶„ÅÑ„ÅüÊà¶Ëªä„ÇíÁÅ´ÁÇéÁì∂ÊîªÊíÉ„Å´„Çà„ÇäÂÖ®ÊªÖ„Åï„Åõ„Çâ„Çå„ÇãË°ùÊíÉÊò†ÂÉè„Åå„Åì„Å°„Çâ„Éª„Éª„Éª...     ja   \n",
       "3099484  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "3099485  Russia's economic sanctions hit 1 trillion yen...     ko   \n",
       "\n",
       "                 created_at tag              cat  scale  Year  Month  Day  \\\n",
       "3099481 2022-03-08 02:22:47  GN  military attack  -10.0  2022      3    8   \n",
       "3099482 2022-03-07 21:32:20  GN  military attack  -10.0  2022      3    7   \n",
       "3099483 2022-03-07 06:48:17  GN  military attack  -10.0  2022      3    7   \n",
       "3099484 2022-03-09 04:28:20  GN  military attack  -10.0  2022      3    9   \n",
       "3099485 2022-03-09 04:20:48  GN  military attack  -10.0  2022      3    9   \n",
       "\n",
       "         Month_Day  TweetNumber  Sentiment textloc label textloc_en  \n",
       "3099481         98           57         -1    None  None       None  \n",
       "3099482         97           58         -1    None  None       None  \n",
       "3099483         97           59          1    None  None       None  \n",
       "3099484         99            0         -1          None             \n",
       "3099485         99            1          1          None             "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the Year 2021\n",
    "df_2y = df[(df[\"Year\"] == 2022)]\n",
    "df_2 = df_2y.reset_index(drop = True)\n",
    "df_2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>563203</th>\n",
       "      <td>RT @Smiyahiko: „Ç¶„ÇØ„É©„Ç§„Éä‰æµÁï•ÂèçÂØæ‚ùóÔ∏è\\n„Éó„Éº„ÉÅ„É≥„Çà„ÄÅ„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆÂõΩÊ∞ë„ÄÅÂ≠ê‰æõÈÅî...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-03 12:10:44</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>123</td>\n",
       "      <td>79</td>\n",
       "      <td>-1</td>\n",
       "      <td>„É≠„Ç∑„Ç¢Ëªç</td>\n",
       "      <td>ORG</td>\n",
       "      <td>russian army</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563204</th>\n",
       "      <td>RT @Smiyahiko: „Ç¶„ÇØ„É©„Ç§„Éä‰æµÁï•ÂèçÂØæ‚ùóÔ∏è\\n„Éó„Éº„ÉÅ„É≥„Çà„ÄÅ„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆÂõΩÊ∞ë„ÄÅÂ≠ê‰æõÈÅî...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-03 11:19:04</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>123</td>\n",
       "      <td>80</td>\n",
       "      <td>-1</td>\n",
       "      <td>„É≠„Ç∑„Ç¢Ëªç</td>\n",
       "      <td>ORG</td>\n",
       "      <td>russian army</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563205</th>\n",
       "      <td>Working on confirmation here, but  just prior ...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-03 08:17:45</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>123</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>7dItzugC via @Sankei_news</td>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>7dItzugC via @Sankei_news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563206</th>\n",
       "      <td>CNN \"Retired Army major general: The Russian m...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-03 06:12:21</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>123</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>Retired Army major general: The Russian milita...</td>\n",
       "      <td>ORG</td>\n",
       "      <td>Retired Army major general: The Russian milita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563207</th>\n",
       "      <td>2022 Russia's invasion of Ukraine\\n\\nÊÖ∂ÊáâÁæ©Â°æÂ§ßÂ≠¶Ê≥ïÂ≠¶ÈÉ®...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-02 14:48:08</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>„Ç¶„ÇØ„É©„Ç§„Éä</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Ukraine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text lang_x  \\\n",
       "563203  RT @Smiyahiko: „Ç¶„ÇØ„É©„Ç§„Éä‰æµÁï•ÂèçÂØæ‚ùóÔ∏è\\n„Éó„Éº„ÉÅ„É≥„Çà„ÄÅ„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆÂõΩÊ∞ë„ÄÅÂ≠ê‰æõÈÅî...     ja   \n",
       "563204  RT @Smiyahiko: „Ç¶„ÇØ„É©„Ç§„Éä‰æµÁï•ÂèçÂØæ‚ùóÔ∏è\\n„Éó„Éº„ÉÅ„É≥„Çà„ÄÅ„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆÂõΩÊ∞ë„ÄÅÂ≠ê‰æõÈÅî...     ja   \n",
       "563205  Working on confirmation here, but  just prior ...     ja   \n",
       "563206  CNN \"Retired Army major general: The Russian m...     ja   \n",
       "563207  2022 Russia's invasion of Ukraine\\n\\nÊÖ∂ÊáâÁæ©Â°æÂ§ßÂ≠¶Ê≥ïÂ≠¶ÈÉ®...     ja   \n",
       "\n",
       "                created_at tag                cat  scale  Year  Month  Day  \\\n",
       "563203 2022-04-03 12:10:44  GN  military invasion  -11.0  2022      4    3   \n",
       "563204 2022-04-03 11:19:04  GN  military invasion  -11.0  2022      4    3   \n",
       "563205 2022-04-03 08:17:45  GN  military invasion  -11.0  2022      4    3   \n",
       "563206 2022-04-03 06:12:21  GN  military invasion  -11.0  2022      4    3   \n",
       "563207 2022-04-02 14:48:08  GN  military invasion  -11.0  2022      4    2   \n",
       "\n",
       "        Month_Day  TweetNumber  Sentiment  \\\n",
       "563203        123           79         -1   \n",
       "563204        123           80         -1   \n",
       "563205        123           81          1   \n",
       "563206        123           82          1   \n",
       "563207        122           83          1   \n",
       "\n",
       "                                                  textloc     label  \\\n",
       "563203                                               „É≠„Ç∑„Ç¢Ëªç       ORG   \n",
       "563204                                               „É≠„Ç∑„Ç¢Ëªç       ORG   \n",
       "563205                          7dItzugC via @Sankei_news  QUANTITY   \n",
       "563206  Retired Army major general: The Russian milita...       ORG   \n",
       "563207                                              „Ç¶„ÇØ„É©„Ç§„Éä       GPE   \n",
       "\n",
       "                                               textloc_en  \n",
       "563203                                       russian army  \n",
       "563204                                       russian army  \n",
       "563205                          7dItzugC via @Sankei_news  \n",
       "563206  Retired Army major general: The Russian milita...  \n",
       "563207                                            Ukraine  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the month\n",
    "df_2m = df_2[(df_2[\"Month\"] == 4)]\n",
    "df_2m2 = df_2m.reset_index(drop = True)\n",
    "df_2m2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  1, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11,\n",
       "       10,  9,  8,  7,  6,  5,  4,  3, 27, 30, 29, 28, 26])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2m2[\"Day\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70432</th>\n",
       "      <td>RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-27 08:47:08</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>„É≠„Ç∑„Ç¢</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70433</th>\n",
       "      <td>RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-27 08:46:21</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>„É≠„Ç∑„Ç¢</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70434</th>\n",
       "      <td>RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-27 07:37:21</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>„É≠„Ç∑„Ç¢</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70435</th>\n",
       "      <td>RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-27 06:15:42</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>„É≠„Ç∑„Ç¢</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70436</th>\n",
       "      <td>@Squallfang @nippon_ukuraina „ÄåMilitary assista...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-27 03:58:41</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Êó•Êú¨</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang_x  \\\n",
       "70432  RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...     ja   \n",
       "70433  RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...     ja   \n",
       "70434  RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...     ja   \n",
       "70435  RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...     ja   \n",
       "70436  @Squallfang @nippon_ukuraina „ÄåMilitary assista...     ja   \n",
       "\n",
       "               created_at tag                cat  scale  Year  Month  Day  \\\n",
       "70432 2022-04-27 08:47:08  GN  military invasion  -11.0  2022      4   27   \n",
       "70433 2022-04-27 08:46:21  GN  military invasion  -11.0  2022      4   27   \n",
       "70434 2022-04-27 07:37:21  GN  military invasion  -11.0  2022      4   27   \n",
       "70435 2022-04-27 06:15:42  GN  military invasion  -11.0  2022      4   27   \n",
       "70436 2022-04-27 03:58:41  GN  military invasion  -11.0  2022      4   27   \n",
       "\n",
       "       Month_Day  TweetNumber  Sentiment textloc label textloc_en  \n",
       "70432        147            1          1     „É≠„Ç∑„Ç¢   GPE     Russia  \n",
       "70433        147            2          1     „É≠„Ç∑„Ç¢   GPE     Russia  \n",
       "70434        147            3          1     „É≠„Ç∑„Ç¢   GPE     Russia  \n",
       "70435        147            4          1     „É≠„Ç∑„Ç¢   GPE     Russia  \n",
       "70436        147            5          1      Êó•Êú¨   GPE      Japan  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the first week\n",
    "df_w1 = df_2m2[(df_2m2[\"Day\"] > 26) & (df_2m2[\"Day\"] < 31)]\n",
    "df_week1 = df_w1.reset_index(drop = True)\n",
    "df_week1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28    23727\n",
       "29    18353\n",
       "27    14941\n",
       "30    13416\n",
       "Name: Day, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_week1[\"Day\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    49319\n",
       "es    10338\n",
       "fr     5107\n",
       "ko     1837\n",
       "ja     1833\n",
       "ar     1043\n",
       "pt      960\n",
       "Name: lang_x, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_week1[\"lang_x\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49314</th>\n",
       "      <td>The Scrap Heap of History ‚Äì The Russian Milita...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-27 00:03:01</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>8591</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49315</th>\n",
       "      <td>RT @khamenei_ir: The #UkraineWar should be con...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-27 00:01:38</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>8592</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49316</th>\n",
       "      <td>@JNotexotic @elonmusk Correct, yes and the res...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-27 00:00:45</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>8593</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49317</th>\n",
       "      <td>RT @Khamenei_m: üìùUkraine War to be considered ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-27 00:00:03</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>8594</td>\n",
       "      <td>-1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49318</th>\n",
       "      <td>Now a stay-at-home mother of two children unde...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-27 00:00:00</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>8595</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang_x  \\\n",
       "49314  The Scrap Heap of History ‚Äì The Russian Milita...     en   \n",
       "49315  RT @khamenei_ir: The #UkraineWar should be con...     en   \n",
       "49316  @JNotexotic @elonmusk Correct, yes and the res...     en   \n",
       "49317  RT @Khamenei_m: üìùUkraine War to be considered ...     en   \n",
       "49318  Now a stay-at-home mother of two children unde...     en   \n",
       "\n",
       "               created_at tag                cat  scale  Year  Month  Day  \\\n",
       "49314 2022-04-27 00:03:01  GN  military invasion  -11.0  2022      4   27   \n",
       "49315 2022-04-27 00:01:38  GN  military invasion  -11.0  2022      4   27   \n",
       "49316 2022-04-27 00:00:45  GN  military invasion  -11.0  2022      4   27   \n",
       "49317 2022-04-27 00:00:03  GN  military invasion  -11.0  2022      4   27   \n",
       "49318 2022-04-27 00:00:00  GN  military invasion  -11.0  2022      4   27   \n",
       "\n",
       "       Month_Day  TweetNumber  Sentiment textloc label textloc_en  \n",
       "49314        147         8591          0    None  None       None  \n",
       "49315        147         8592         -1    None  None       None  \n",
       "49316        147         8593          0    None  None       None  \n",
       "49317        147         8594         -1    None  None       None  \n",
       "49318        147         8595          1    None  None       None  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only Spanish\n",
    "df_week1_2 = df_week1[(df_week1[\"lang_x\"] == \"en\")]\n",
    "df_week1_2_2 = df_week1_2.reset_index(drop = True)\n",
    "df_week1_2_2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49314</th>\n",
       "      <td>RT @JuliaDavisNews: The White House unveiled a...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-28 16:09:33</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>148</td>\n",
       "      <td>3449</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49315</th>\n",
       "      <td>RT @anders_aslund: After Putin's assault on Uk...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-29 19:30:23</td>\n",
       "      <td>GN</td>\n",
       "      <td>military assault</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>149</td>\n",
       "      <td>4177</td>\n",
       "      <td>-1</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Ukraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49316</th>\n",
       "      <td>RT @KyivIndependent: ‚ö°Ô∏è Polish media: Spanish ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-28 23:22:40</td>\n",
       "      <td>GP</td>\n",
       "      <td>military assistance</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>148</td>\n",
       "      <td>16838</td>\n",
       "      <td>0</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Ukraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49317</th>\n",
       "      <td>RT @KyivIndependent: ‚ö°Ô∏è General Staff: Russia ...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-28 08:32:14</td>\n",
       "      <td>GN</td>\n",
       "      <td>military attack</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>148</td>\n",
       "      <td>33387</td>\n",
       "      <td>0</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Ukraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49318</th>\n",
       "      <td>RT @tomiahonen: Russia War Costs Thread 1/\\n\\n...</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-30 21:36:30</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>150</td>\n",
       "      <td>94461</td>\n",
       "      <td>-1</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Ukraine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang_x  \\\n",
       "49314  RT @JuliaDavisNews: The White House unveiled a...     en   \n",
       "49315  RT @anders_aslund: After Putin's assault on Uk...     en   \n",
       "49316  RT @KyivIndependent: ‚ö°Ô∏è Polish media: Spanish ...     en   \n",
       "49317  RT @KyivIndependent: ‚ö°Ô∏è General Staff: Russia ...     en   \n",
       "49318  RT @tomiahonen: Russia War Costs Thread 1/\\n\\n...     en   \n",
       "\n",
       "               created_at tag                  cat  scale  Year  Month  Day  \\\n",
       "49314 2022-04-28 16:09:33  GN    military invasion  -11.0  2022      4   28   \n",
       "49315 2022-04-29 19:30:23  GN     military assault  -10.0  2022      4   29   \n",
       "49316 2022-04-28 23:22:40  GP  military assistance    8.3  2022      4   28   \n",
       "49317 2022-04-28 08:32:14  GN      military attack  -10.0  2022      4   28   \n",
       "49318 2022-04-30 21:36:30  GN    military invasion  -11.0  2022      4   30   \n",
       "\n",
       "       Month_Day  TweetNumber  Sentiment  textloc label textloc_en  \n",
       "49314        148         3449          1     None  None       None  \n",
       "49315        149         4177         -1  Ukraine   GPE    Ukraine  \n",
       "49316        148        16838          0  Ukraine   GPE    Ukraine  \n",
       "49317        148        33387          0  Ukraine   GPE    Ukraine  \n",
       "49318        150        94461         -1  Ukraine   GPE    Ukraine  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Subset each group\n",
    "dfw1en2 = df_week1_2_2.sample(frac = 1)\n",
    "dfw1en = dfw1en2.reset_index(drop = True)\n",
    "dfw1en.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get sample\n",
    "#df_week1_sample = df_week1_2.sample(frac = 0.25)\n",
    "#dfw1en = df_week1_sample.reset_index(drop = True)\n",
    "#dfw1en.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 6: Break into English Tweets\n",
    "def English_Tweets_Func(df_gt):\n",
    "    English_Tweets = df_gt[df_gt['lang_x'] == \"en\"]\n",
    "    English_Tweets.reset_index(drop = True, inplace = True)\n",
    "    English_Tweets['TweetNumber'] = np.arange(len(English_Tweets))\n",
    "    return English_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 7: Break into Spanish Tweets\n",
    "def Spanish_Tweets_Func(df_gt):\n",
    "    Spanish_Tweets = df_gt[df_gt['lang_x'] == \"es\"]\n",
    "    Spanish_Tweets.reset_index(drop = True, inplace = True)\n",
    "    Spanish_Tweets['TweetNumber'] = np.arange(len(Spanish_Tweets))\n",
    "    return Spanish_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 8: Break into French Tweets\n",
    "def French_Tweets_Func(df_gt):\n",
    "    French_Tweets = df_gt[df_gt['lang_x'] == \"fr\"]\n",
    "    French_Tweets.reset_index(drop = True, inplace = True)\n",
    "    French_Tweets['TweetNumber'] = np.arange(len(French_Tweets))\n",
    "    return French_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 9: Break into Portuguese Tweets\n",
    "def Portuguese_Tweets_Func(df_gt):\n",
    "    Portuguese_Tweets = df_gt[df_gt['lang_x'] == \"pt\"]\n",
    "    Portuguese_Tweets.reset_index(drop = True, inplace = True)\n",
    "    Portuguese_Tweets['TweetNumber'] = np.arange(len(Portuguese_Tweets))\n",
    "    return Portuguese_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 10: Break into Arabic Tweets\n",
    "def Arabic_Tweets_Func(df_gt):\n",
    "    Arabic_Tweets = df_gt[df_gt['lang_x'] == \"ar\"]\n",
    "    Arabic_Tweets.reset_index(drop = True, inplace = True)\n",
    "    Arabic_Tweets['TweetNumber'] = np.arange(len(Arabic_Tweets))\n",
    "    return Arabic_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 11: Break into Japanese Tweets\n",
    "def Japanese_Tweets_Func(df_gt):\n",
    "    Japanese_Tweets = df_gt[df_gt['lang_x'] == \"ja\"]\n",
    "    Japanese_Tweets.reset_index(drop = True, inplace = True)\n",
    "    Japanese_Tweets['TweetNumber'] = np.arange(len(Japanese_Tweets)) \n",
    "    return Japanese_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 12: Break into Korean Tweets\n",
    "def Korean_Tweets_Func(df_gt):\n",
    "    Korean_Tweets = df_gt[df_gt['lang_x'] == \"ko\"]\n",
    "    Korean_Tweets.reset_index(drop = True, inplace = True)\n",
    "    Korean_Tweets['TweetNumber'] = np.arange(len(Korean_Tweets))\n",
    "    return Korean_Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 18: Sort Chronologically Function Sort by the Year, Month, Day, Hour, Minute, Second\n",
    "def sort_chrono(df):\n",
    "    df2 = df.sort_values(by = ['Day'], ascending = ['False'], na_position = 'first')\n",
    "    df3 = df2.reset_index(drop = True)\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0: Libraries\n",
    "#Step 3: Topics over Time Libraries\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.test.utils\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tqdm\n",
    "#Random other libraries\n",
    "import requests\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Pretty Printing Library\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Making the Plot Library\n",
    "#Graphing in Jupiter Notebook\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Importing The Stopwords\n",
    "#Step 19: English Stop Words Vector\n",
    "def stopwords_en_func():\n",
    "    stop_words_en = stopwords.words('english')\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_en = stop_words_en + custom_stop_words\n",
    "    return final_stop_words_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 20: Spanish Stop Words Vector\n",
    "def stopwords_sp_func():\n",
    "    stop_words_sp = stopwords.words('spanish')\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_sp = stop_words_sp + custom_stop_words\n",
    "    return final_stop_words_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 21: French Stop Words Vector\n",
    "def stopwords_fr_func():\n",
    "    stop_words_fr = stopwords.words('french')\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_fr = stop_words_fr + custom_stop_words\n",
    "    return final_stop_words_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 22: Portuguese Stop Words Vector\n",
    "def stopwords_pt_func():\n",
    "    stop_words_pt = stopwords.words('portuguese')\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_pt = stop_words_pt + custom_stop_words\n",
    "    return final_stop_words_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 23: Arabic Stop Words Vector\n",
    "def stopwords_ar_func():\n",
    "    stop_words_ar = stopwords.words('arabic')\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_ar = stop_words_ar + custom_stop_words\n",
    "    return final_stop_words_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 24: Get the Japanese Stopwords Vector\n",
    "def stopwords_ja_func():\n",
    "    stopwords_ja_df = pd.read_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/stopwords-ja.txt\", delimiter = \"\\t\", encoding = \"utf-8\")\n",
    "    stopwords_ja_df.columns = [\"word\"]\n",
    "    stopwords_ja = stopwords_ja_df[\"word\"].values.tolist()\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_ja = stopwords_ja + custom_stop_words\n",
    "    return final_stop_words_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 25: Import Korean Stop Words\n",
    "def stopwords_ko_func():\n",
    "    stopwords_ko_df = pd.read_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/Mockup 8/stopwords-ko.txt\", delimiter = \"\\t\", quoting=csv.QUOTE_NONE, encoding = \"utf-8\")\n",
    "    stopwords_ko_df.columns = [\"word\"]\n",
    "    stopwords_ko = stopwords_ko_df[\"word\"].values.tolist()\n",
    "    custom_stop_words = [\"http\", \"https\", \"co\", \"com\", \"app\", \"go\", \"amp\", \"RT\", \"rt\"]\n",
    "    final_stop_words_ko = stopwords_ko + custom_stop_words\n",
    "    return final_stop_words_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Removing the Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 26: Remove Stop Words English\n",
    "def stopwords_en(texts, final_stop_words_en):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_en] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 27: Remove Stop Words Spanish\n",
    "def stopwords_sp(texts, final_stop_words_sp):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_sp] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 28: Remove Stop Words French\n",
    "def stopwords_fr(texts, final_stop_words_fr):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_fr] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 29: Remove Stop Words Portuguese\n",
    "def stopwords_pt(texts, final_stop_words_pt):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_pt] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 30: Remove Stop Words Arabic\n",
    "def stopwords_ar(texts, final_stop_words_ar):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_ar] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 31: Remove Stop Words Japanese\n",
    "def stopwords_ja(texts, final_stop_words_ja):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_ja] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 32: Remove Stop Words Korean\n",
    "def stopwords_ko(texts, final_stop_words_ko):\n",
    "    return[[word for word in simple_preprocess(str(doc)) if word not in final_stop_words_ko] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Make the bigram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 33: Make bigrams of the words\n",
    "#Make sure do call this function 7 times for each of the 7 languages\n",
    "def bigrams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count = 5, threshold = 100)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Lemma the data for the languages you can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 34: Turn the words into lemmas English\n",
    "def data_lemmatization_en(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])\n",
    "    nlp.max_length = 15000000\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 35: Turn the words into lemmas Spanish\n",
    "def data_lemmatization_sp(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    nlp = spacy.load(\"es_core_news_sm\", disable = ['parser', 'ner'])\n",
    "    nlp.max_length = 15000000\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 36: Turn the words into lemmas French\n",
    "def data_lemmatization_fr(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    nlp = spacy.load(\"fr_core_news_sm\", disable = ['parser', 'ner'])\n",
    "    nlp.max_length = 15000000\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 37: Turn the words into lemmas Portuguese\n",
    "def data_lemmatization_pt(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    nlp = spacy.load(\"pt_core_news_sm\", disable = ['parser', 'ner'])\n",
    "    nlp.max_length = 15000000\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 38: Turn the words into lemmas Portuguese\n",
    "def data_lemmatization_ja(texts, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    nlp = spacy.load(\"ja_core_news_sm\", disable = ['parser', 'ner'])\n",
    "    nlp.max_length = 15000000\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Create the createlist function\n",
    "#Step 41: Create topic_id numbers, change dynamically\n",
    "#https://www.delftstack.com/howto/python/python-list-from-1-to-n/\n",
    "#topic_id = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "def createList(n):\n",
    "    lst1 = []\n",
    "    for i in range(n):\n",
    "        lst1.append(i)\n",
    "    return(lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Create the computing coherence function\n",
    "#Step 42: Detour to focus on hyperparameter tunning of the LDA Model\n",
    "#https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "#First build the function to test the hyperparameters of the number of topics (k), the document - topic density\n",
    "#(alpha) and the Word - topic density (beta), we get the coherence score for each model using the 'c_v' which is \n",
    "#measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation \n",
    "#measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
    "\n",
    "def compute_coherence_values(corpus, texts, dictionary, k, a, b):\n",
    "\n",
    "    lda_model_cv = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                         id2word = dictionary,\n",
    "                                         num_topics = k,\n",
    "                                         random_state = 101,\n",
    "                                         chunksize = 100,\n",
    "                                         passes = 10,\n",
    "                                         alpha = a,\n",
    "                                         eta = b,\n",
    "                                          per_word_topics = True,\n",
    "                                          minimum_probability = 0)\n",
    "    \n",
    "    #coherence_model_lda = CoherenceModel(model = lda_model_cv, texts = data_lemma, dictionary = id2word,\n",
    "    #                                    coherence = 'c_v')\n",
    "    coherence_model_lda = CoherenceModel(model = lda_model_cv, texts = texts, \n",
    "                                         dictionary = dictionary, coherence = 'c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 43: LDA Hyperparameter Finding function for English\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "def lda_hyperparameter_generating_en(df_en, final_stop_words_en):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_en(df_en['text'], final_stop_words_en)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram_hf = bigrams(data_words_nostops_hf)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma_hf = data_lemmatization_en(data_words_bigram_hf, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word_hf = corpora.Dictionary(data_lemma_hf)\n",
    "    #Corpus\n",
    "    texts_hf = data_lemma_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 10\n",
    "    max_topics = 11\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_lemma_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_en = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 44: Hyper Parameter Defining for English\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_en(mr_en):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_en.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_en['Topics'] == top_opt\n",
    "    mr_to = mr_en[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_en = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_en)\n",
    "    return hyper_list_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 45: Implement the stopwords, bigrams, and lemma functions English\n",
    "def build_lda_en(df_en, final_stop_words_en, hyper_list_en):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_en(df_en['text'], final_stop_words_en)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram = bigrams(data_words_nostops)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma = data_lemmatization_en(data_words_bigram, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemma)\n",
    "    #Corpus\n",
    "    texts = data_lemma\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_en = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_en[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_en[1],\n",
    "                                              eta = hyper_list_en[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_en[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_en\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_en.print_topics()\n",
    "    lenpt2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenpt2):\n",
    "        pt_list = printtopics2[i][1].split('*')\n",
    "        pt_list_words = []\n",
    "        lenptl = len(pt_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 6):\n",
    "            t_1 = pt_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            pt_list_words.append(t_3)\n",
    "        topic_label_list.append(pt_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_en = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 46: Visualization of Topics over Time English\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_en(df, hyper_list_en):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs_en = {}\n",
    "    topic_label_list_en = []\n",
    "    for i in range(0, hyper_list_en[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list_en.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs_en[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors_en = hyper_list_en[0]\n",
    "    \n",
    "    color_en = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors_en)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs_en.keys():\n",
    "        plt.plot(topic_dfs_en[i], color = color_en[i])\n",
    "    plt.xlim(26, 31)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in English Topics\")\n",
    "    plt.xlabel(\"Day\") \n",
    "    plt.ylabel(\"Average Day Topic Weight\")\n",
    "    plt.legend((topic_label_list_en))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/Output_EN_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Getting the Topic Values over Time \n",
    "def topic_value_en(df, hyper_list_en):\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_en[0]):\n",
    "        df_5 = df[df[\"topic\"] == i]\n",
    "        df_2 = df_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_2[\"topic_label\"][0])\n",
    "        topic_plots = df_2.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "    dataframe2 = pd.DataFrame(topic_dfs)\n",
    "    df2_t = dataframe2.T\n",
    "    df2_t[\"Topic_label\"] = topic_label_list\n",
    "    df2_t.to_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/DataFrames/English.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 47: LDA Hyperparameter Finding function for Spanish\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "def lda_hyperparameter_generating_sp(df_sp, final_stop_words_sp):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_sp(df_sp['text'], final_stop_words_sp)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram_hf = bigrams(data_words_nostops_hf)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma_hf = data_lemmatization_sp(data_words_bigram_hf, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word_hf = corpora.Dictionary(data_lemma_hf)\n",
    "    #Corpus\n",
    "    texts_hf = data_lemma_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 10\n",
    "    max_topics = 11\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_lemma_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_sp = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 48: Hyper Parameter Defining for Spanish\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_sp(mr_sp):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_sp.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_sp['Topics'] == top_opt\n",
    "    mr_to = mr_sp[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_sp = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_sp)\n",
    "    return hyper_list_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 50: Implement the stopwords, bigrams, and lemma functions Spanish\n",
    "def build_lda_sp(df_sp, final_stop_words_sp, hyper_list_sp):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_sp(df_sp['text'], final_stop_words_sp)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram = bigrams(data_words_nostops)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma = data_lemmatization_sp(data_words_bigram, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemma)\n",
    "    #Corpus\n",
    "    texts = data_lemma\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_sp = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_sp[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_sp[1],\n",
    "                                              eta = hyper_list_sp[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_sp[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_sp\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_sp.print_topics()\n",
    "    lenpt2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenpt2):\n",
    "        pt_list = printtopics2[i][1].split('*')\n",
    "        pt_list_words = []\n",
    "        lenptl = len(pt_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 6):\n",
    "            t_1 = pt_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            pt_list_words.append(t_3)\n",
    "        topic_label_list.append(pt_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Translate the Topic Label \n",
    "    tll_translate = []\n",
    "    lentll = len(topic_label_list)\n",
    "    for i in range(0, lentll):\n",
    "        tll_topic_line = []\n",
    "        lent_t_l = len(topic_label_list[i])\n",
    "        for j in range(0, lent_t_l):\n",
    "            text = topic_label_list[i][j]\n",
    "            translated = GoogleTranslator(source = \"spanish\", to_lang = \"english\").translate(text=text)\n",
    "            tll_topic_line.append(translated)\n",
    "        tll_translate.append(tll_topic_line)\n",
    "        \n",
    "    #Topic Label Translated \n",
    "    topic_labels_translate = tll_translate\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels_translate))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_sp = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 51: Visualization of Topics over Time Spanish\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_sp(df, hyper_list_sp):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs_sp = {}\n",
    "    topic_label_list_sp = []\n",
    "    for i in range(0, hyper_list_sp[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list_sp.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs_sp[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors_sp = hyper_list_sp[0]\n",
    "    \n",
    "    color_sp = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors_sp)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs_sp.keys():\n",
    "        plt.plot(topic_dfs_sp[i], color = color_sp[i])\n",
    "    plt.xlim(26, 31)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in Spanish Topics\")\n",
    "    plt.xlabel(\"Day\") \n",
    "    plt.ylabel(\"Average Day Topic Weight\")\n",
    "    plt.legend((topic_label_list_sp))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/Output_SP_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Getting the Topic Values over Time \n",
    "def topic_value_sp(df, hyper_list_sp):\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_sp[0]):\n",
    "        df_5 = df[df[\"topic\"] == i]\n",
    "        df_2 = df_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_2[\"topic_label\"][0])\n",
    "        topic_plots = df_2.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "    dataframe2 = pd.DataFrame(topic_dfs)\n",
    "    df2_t = dataframe2.T\n",
    "    df2_t[\"Topic_label\"] = topic_label_list\n",
    "    df2_t.to_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/DataFrames/Spanish.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 52: LDA Hyperparameter Finding function for French\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "def lda_hyperparameter_generating_fr(df_fr, final_stop_words_fr):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_fr(df_fr['text'], final_stop_words_fr)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram_hf = bigrams(data_words_nostops_hf)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma_hf = data_lemmatization_fr(data_words_bigram_hf, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word_hf = corpora.Dictionary(data_lemma_hf)\n",
    "    #Corpus\n",
    "    texts_hf = data_lemma_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 10\n",
    "    max_topics = 11\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_lemma_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_fr = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 53: Hyper Parameter Defining for French\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_fr(mr_fr):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_fr.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_fr['Topics'] == top_opt\n",
    "    mr_to = mr_fr[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_fr = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_fr)\n",
    "    return hyper_list_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 54: Implement the stopwords, bigrams, and lemma functions Spanish\n",
    "def build_lda_fr(df_fr, final_stop_words_fr, hyper_list_fr):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_fr(df_fr['text'], final_stop_words_fr)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram = bigrams(data_words_nostops)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma = data_lemmatization_fr(data_words_bigram, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemma)\n",
    "    #Corpus\n",
    "    texts = data_lemma\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_fr = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_fr[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_fr[1],\n",
    "                                              eta = hyper_list_fr[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_fr[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_fr\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_fr.print_topics()\n",
    "    lenpt2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenpt2):\n",
    "        pt_list = printtopics2[i][1].split('*')\n",
    "        pt_list_words = []\n",
    "        lenptl = len(pt_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 6):\n",
    "            t_1 = pt_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            pt_list_words.append(t_3)\n",
    "        topic_label_list.append(pt_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Translate the Topic Label \n",
    "    tll_translate = []\n",
    "    lentll = len(topic_label_list)\n",
    "    for i in range(0, lentll):\n",
    "        tll_topic_line = []\n",
    "        lent_t_l = len(topic_label_list[i])\n",
    "        for j in range(0, lent_t_l):\n",
    "            text = topic_label_list[i][j]\n",
    "            translated = GoogleTranslator(source = \"french\", to_lang = \"english\").translate(text=text)\n",
    "            tll_topic_line.append(translated)\n",
    "        tll_translate.append(tll_topic_line)\n",
    "        \n",
    "    #Topic Label Translated \n",
    "    topic_labels_translate = tll_translate\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels_translate))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_fr = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_fr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 55: Visualization of Topics over Time French\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_fr(df, hyper_list_fr):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs_fr = {}\n",
    "    topic_label_list_fr = []\n",
    "    for i in range(0, hyper_list_fr[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list_fr.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs_fr[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors_fr = hyper_list_fr[0]\n",
    "    \n",
    "    color_fr = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors_fr)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs_fr.keys():\n",
    "        plt.plot(topic_dfs_fr[i], color = color_fr[i])\n",
    "    plt.xlim(26, 31)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in French Topics\")\n",
    "    plt.xlabel(\"Day\") \n",
    "    plt.ylabel(\"Average Daily Topic Weight\")\n",
    "    plt.legend((topic_label_list_fr))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/Output_FR_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Getting the Topic Values over Time \n",
    "def topic_value_fr(df, hyper_list_fr):\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_fr[0]):\n",
    "        df_5 = df[df[\"topic\"] == i]\n",
    "        df_2 = df_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_2[\"topic_label\"][0])\n",
    "        topic_plots = df_2.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "    dataframe2 = pd.DataFrame(topic_dfs)\n",
    "    df2_t = dataframe2.T\n",
    "    df2_t[\"Topic_label\"] = topic_label_list\n",
    "    df2_t.to_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/DataFrames/French.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 56: LDA Hyperparameter Finding function for Portuguese\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "def lda_hyperparameter_generating_pt(df_pt, final_stop_words_pt):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_pt(df_pt['text'], final_stop_words_pt)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram_hf = bigrams(data_words_nostops_hf)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma_hf = data_lemmatization_pt(data_words_bigram_hf, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word_hf = corpora.Dictionary(data_lemma_hf)\n",
    "    #Corpus\n",
    "    texts_hf = data_lemma_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 10\n",
    "    max_topics = 11\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_lemma_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_pt = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 57: Hyper Parameter Defining for Portuguese\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_pt(mr_pt):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_pt.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_pt['Topics'] == top_opt\n",
    "    mr_to = mr_pt[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_pt = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_pt)\n",
    "    return hyper_list_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 58: Implement the stopwords, bigrams, and lemma functions Portuguese\n",
    "def build_lda_pt(df_pt, final_stop_words_pt, hyper_list_pt):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_pt(df_pt['text'], final_stop_words_pt)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram = bigrams(data_words_nostops)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma = data_lemmatization_pt(data_words_bigram, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemma)\n",
    "    #Corpus\n",
    "    texts = data_lemma\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_pt = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_pt[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_pt[1],\n",
    "                                              eta = hyper_list_pt[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_pt[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_pt\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_pt.print_topics()\n",
    "    lenpt2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenpt2):\n",
    "        pt_list = printtopics2[i][1].split('*')\n",
    "        pt_list_words = []\n",
    "        lenptl = len(pt_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 6):\n",
    "            t_1 = pt_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            pt_list_words.append(t_3)\n",
    "        topic_label_list.append(pt_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Translate the Topic Label \n",
    "    tll_translate = []\n",
    "    lentll = len(topic_label_list)\n",
    "    for i in range(0, lentll):\n",
    "        tll_topic_line = []\n",
    "        lent_t_l = len(topic_label_list[i])\n",
    "        for j in range(0, lent_t_l):\n",
    "            text = topic_label_list[i][j]\n",
    "            translated = GoogleTranslator(source = \"portuguese\", to_lang = \"english\").translate(text=text)\n",
    "            tll_topic_line.append(translated)\n",
    "        tll_translate.append(tll_topic_line)\n",
    "        \n",
    "    #Topic Label Translated \n",
    "    topic_labels_translate = tll_translate\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels_translate))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_pt = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 59: Visualization of Topics over Time Portuguese\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_pt(df, hyper_list_pt):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_pt[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors = hyper_list_pt[0]\n",
    "    \n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs.keys():\n",
    "        plt.plot(topic_dfs[i], color = color[i])\n",
    "    plt.xlim(26, 31)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in Portuguese Topics\")\n",
    "    plt.xlabel(\"Day\") \n",
    "    plt.ylabel(\"Average Daily Topic Weight\")\n",
    "    plt.legend((topic_label_list))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/Output_PT_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Getting the Topic Values over Time \n",
    "def topic_value_pt(df, hyper_list_pt):\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_pt[0]):\n",
    "        df_5 = df[df[\"topic\"] == i]\n",
    "        df_2 = df_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_2[\"topic_label\"][0])\n",
    "        topic_plots = df_2.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "    dataframe2 = pd.DataFrame(topic_dfs)\n",
    "    df2_t = dataframe2.T\n",
    "    df2_t[\"Topic_label\"] = topic_label_list\n",
    "    df2_t.to_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/DataFrames/Portuguese.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 60: LDA Hyperparameter Finding function for Arabic\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "#https://stackoverflow.com/questions/33229360/gensim-typeerror-doc2bow-expects-an-array-of-unicode-tokens-on-input-not-a-si\n",
    "def lda_hyperparameter_generating_ar(df_ar, final_stop_words_ar):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_ar(df_ar['text'], final_stop_words_ar)\n",
    "    #Create the bigram from the non stop words\n",
    "    #data_words_bigram_hf = str(bigrams(data_words_nostops_hf))\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    #data_lemma_hf = data_tokenization_ar(data_words_bigram_hf)\n",
    "    #data_lemma_hf = data_tokenization_ar(data_words_nostops_hf)\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    #id2word_hf = corpora.Dictionary([a.split(' ') for a in data_lemma_hf])\n",
    "    #id2word_hf = corpora.Dictionary([str(data_lemma_hf).split(' ')])\n",
    "    #id2word_hf = corpora.Dictionary([str(data_words_bigram_hf).split()])\n",
    "    id2word_hf = corpora.Dictionary(data_words_nostops_hf)\n",
    "    #Corpus\n",
    "    #texts_hf = [str(data_lemma_hf).split(' ')]\n",
    "    #texts_hf = [str(data_words_bigram_hf).split()]\n",
    "    texts_hf = data_words_nostops_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 10\n",
    "    max_topics = 11\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_words_nostops_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_ar = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_ar\n",
    "    #print(mr_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 61: Hyper Parameter Defining for Arabic\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_ar(mr_ar):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_ar.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_ar['Topics'] == top_opt\n",
    "    mr_to = mr_ar[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_ar = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_ar)\n",
    "    return hyper_list_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 62: Implement the stopwords, bigrams, and lemma functions Arabic\n",
    "def build_lda_ar(df_ar, final_stop_words_ar, hyper_list_ar):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_pt(df_ar['text'], final_stop_words_ar)\n",
    "    #Create the bigram from the non stop words\n",
    "    #data_words_bigram = str(bigrams(data_words_nostops))\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    #data_lemma = data_tokenization_ar(data_words_bigram)\n",
    "    #data_lemma = data_tokenization_ar(data_words_nostops)\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    #id2word = corpora.Dictionary([a.split(' ') for a in data_lemma])\n",
    "    id2word = corpora.Dictionary(data_words_nostops)\n",
    "    #Corpus\n",
    "    texts = data_words_nostops\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_ar = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_ar[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_ar[1],\n",
    "                                              eta = hyper_list_ar[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_ar[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_ar\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_ar.print_topics()\n",
    "    lenar2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenar2):\n",
    "        ar_list = printtopics2[i][1].split('*')\n",
    "        ar_list_words = []\n",
    "        lenarl = len(ar_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 6):\n",
    "            t_1 = ar_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            ar_list_words.append(t_3)\n",
    "        topic_label_list.append(ar_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Translate the Topic Label \n",
    "    tll_translate = []\n",
    "    lentll = len(topic_label_list)\n",
    "    for i in range(0, lentll):\n",
    "        tll_topic_line = []\n",
    "        lent_t_l = len(topic_label_list[i])\n",
    "        for j in range(0, lent_t_l):\n",
    "            text = topic_label_list[i][j]\n",
    "            translated = GoogleTranslator(source = \"arabic\", to_lang = \"english\").translate(text=text)\n",
    "            tll_topic_line.append(translated)\n",
    "        tll_translate.append(tll_topic_line)\n",
    "        \n",
    "    #Topic Label Translated \n",
    "    topic_labels_translate = tll_translate\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels_translate))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_ar = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 63: Visualization of Topics over Time Arabic\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_ar(df, hyper_list_ar):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_ar[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors = hyper_list_ar[0]\n",
    "    \n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs.keys():\n",
    "        plt.plot(topic_dfs[i], color = color[i])\n",
    "    plt.xlim(26, 31)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in Arabic Topics\")\n",
    "    plt.xlabel(\"Day\") \n",
    "    plt.ylabel(\"Average Daily Topic Weight\")\n",
    "    plt.legend((topic_label_list))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/Output_AR_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Getting the Topic Values over Time \n",
    "def topic_value_ar(df, hyper_list_ar):\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_ar[0]):\n",
    "        df_5 = df[df[\"topic\"] == i]\n",
    "        df_2 = df_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_2[\"topic_label\"][0])\n",
    "        topic_plots = df_2.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "    dataframe2 = pd.DataFrame(topic_dfs)\n",
    "    df2_t = dataframe2.T\n",
    "    df2_t[\"Topic_label\"] = topic_label_list\n",
    "    df2_t.to_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/DataFrames/Arabic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 64: LDA Hyperparameter Finding function for Japanese\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "def lda_hyperparameter_generating_ja(df_ja, final_stop_words_ja):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_ja(df_ja['text'], final_stop_words_ja)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram_hf = bigrams(data_words_nostops_hf)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma_hf = data_lemmatization_ja(data_words_bigram_hf, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word_hf = corpora.Dictionary(data_lemma_hf)\n",
    "    #Corpus\n",
    "    texts_hf = data_lemma_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 10\n",
    "    max_topics = 11\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_lemma_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_ja = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 65: Hyper Parameter Defining for Japanese\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_ja(mr_ja):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_ja.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    #print(mr3)\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_ja['Topics'] == top_opt\n",
    "    mr_to = mr_ja[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_ja = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_ja)\n",
    "    return hyper_list_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 66: Implement the stopwords, bigrams, and lemma functions Japanese\n",
    "def build_lda_ja(df_ja, final_stop_words_ja, hyper_list_ja):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_ja(df_ja['text'], final_stop_words_ja)\n",
    "    #Create the bigram from the non stop words\n",
    "    data_words_bigram = bigrams(data_words_nostops)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    data_lemma = data_lemmatization_ja(data_words_bigram, allowed_postags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"])\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word = corpora.Dictionary(data_lemma)\n",
    "    #Corpus\n",
    "    texts = data_lemma\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_ja = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_ja[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_ja[1],\n",
    "                                              eta = hyper_list_ja[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_ja[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_ja\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_ja.print_topics()\n",
    "    lenja2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenja2):\n",
    "        ja_list = printtopics2[i][1].split('*')\n",
    "        ja_list_words = []\n",
    "        lenjal = len(ja_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 6):\n",
    "            t_1 = ja_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            ja_list_words.append(t_3)\n",
    "        topic_label_list.append(ja_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Translate the Topic Label \n",
    "    tll_translate = []\n",
    "    lentll = len(topic_label_list)\n",
    "    for i in range(0, lentll):\n",
    "        tll_topic_line = []\n",
    "        lent_t_l = len(topic_label_list[i])\n",
    "        for j in range(0, lent_t_l):\n",
    "            text = topic_label_list[i][j]\n",
    "            translated = GoogleTranslator(source = \"japanese\", to_lang = \"english\").translate(text=text)\n",
    "            tll_topic_line.append(translated)\n",
    "        tll_translate.append(tll_topic_line)\n",
    "        \n",
    "    #Topic Label Translated \n",
    "    topic_labels_translate = tll_translate\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels_translate))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_ja = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 67: Visualization of Topics over Time Japanese\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_ja(df, hyper_list_ja):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_ja[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors = hyper_list_ja[0]\n",
    "    \n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs.keys():\n",
    "        plt.plot(topic_dfs[i], color = color[i])\n",
    "    plt.xlim(26, 31)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in Japanese Topics\")\n",
    "    plt.xlabel(\"Day\") \n",
    "    plt.ylabel(\"Average Daily Topic Weight\")\n",
    "    plt.legend((topic_label_list))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/Output_JA_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Getting the Topic Values over Time \n",
    "def topic_value_ja(df, hyper_list_ja):\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_ja[0]):\n",
    "        df_5 = df[df[\"topic\"] == i]\n",
    "        df_2 = df_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_2[\"topic_label\"][0])\n",
    "        topic_plots = df_2.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "    dataframe2 = pd.DataFrame(topic_dfs)\n",
    "    df2_t = dataframe2.T\n",
    "    df2_t[\"Topic_label\"] = topic_label_list\n",
    "    df2_t.to_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/DataFrames/Japanese.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 64: LDA Hyperparameter Finding function for Korean\n",
    "#https://stackoverflow.com/questions/60087463/valueerror-stop-argument-for-islice-must-be-none-or-an-integer-0-x-sys\n",
    "def lda_hyperparameter_generating_ko(df_ko, final_stop_words_ko):\n",
    "    #Remove stops words\n",
    "    data_words_nostops_hf = stopwords_ko(df_ko['text'], final_stop_words_ko)\n",
    "    #Create the bigram from the non stop words\n",
    "    #data_words_bigram_hf = bigrams(data_words_nostops_hf)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    #data_lemma_hf = data_lemmatization_ko(data_words_bigram_hf)\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    id2word_hf = corpora.Dictionary(data_words_nostops_hf)\n",
    "    #id2word_hf = corpora.Dictionary([str(data_lemma_hf).split(' ')])\n",
    "    #Corpus\n",
    "    #texts_hf = [str(data_lemma_hf).split(' ')]\n",
    "    texts_hf = data_words_nostops_hf\n",
    "    #Term Document Matrix\n",
    "    corpus_hf = [id2word_hf.doc2bow(text) for text in texts_hf]\n",
    "    \n",
    "    #Lets iterate over the function to find the optimal number for each of the hyper parameters\n",
    "    grid_hf = {}\n",
    "    grid_hf['Validation_Set'] = {}\n",
    "    \n",
    "    #Topic Range\n",
    "    min_topics = 10\n",
    "    max_topics = 11\n",
    "    step_size = 1\n",
    "    topic_range = range(min_topics, max_topics, step_size)\n",
    "    \n",
    "    #Alpha Parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "    \n",
    "    #Beta Parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "    \n",
    "    #Validation sets\n",
    "    num_of_docs = len(corpus_hf)\n",
    "    corpus_sets = [corpus_hf]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    #iterate through validation corpora:\n",
    "    for i in range(len(corpus_sets)):\n",
    "        #iterate through number of topics:\n",
    "        for k in topic_range:\n",
    "            #iterate through alpha values:\n",
    "            for a in alpha:\n",
    "                #iterate through beta values:\n",
    "                for b in beta:\n",
    "                    #Get the coherence scores for the given hyperparameters\n",
    "                    cv = compute_coherence_values(corpus = corpus_sets[i], texts = data_words_nostops_hf,\n",
    "                                                  dictionary = id2word_hf, k = k, \n",
    "                                                  a = a, b = b)\n",
    "                    #Save the Model Results \n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "    #Look at model results\n",
    "    mr_ko = pd.DataFrame(model_results)\n",
    "    \n",
    "    return mr_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 65: Hyper Parameter Defining for Japanese\n",
    "#https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group\n",
    "#https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe\n",
    "#https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-value-in-groups-using-groupby\n",
    "#https://stackoverflow.com/questions/43193880/how-to-get-row-number-in-dataframe-in-pandas\n",
    "\n",
    "def lda_hyper_define_ko(mr_ko):\n",
    "    #Find the right number of topics\n",
    "    mr2 = mr_ko.groupby(\"Topics\").max().reset_index()\n",
    "    #Find the number of topics with the highest coherence\n",
    "    max_coherence = mr2['Coherence'].max()\n",
    "    mr3_5 = mr2.loc[mr2['Coherence'] == max_coherence]\n",
    "    mr3 = mr3_5.reset_index(drop = True)\n",
    "    #Get the Number of Topics for the highest coherence\n",
    "    top_opt = mr3[\"Topics\"][0]\n",
    "    #Get the full data set of only the optimal number of topics\n",
    "    mr_top_opt = mr_ko['Topics'] == top_opt\n",
    "    mr_to = mr_ko[mr_top_opt]\n",
    "    mr_to_2 = mr_to.reset_index(drop = True)\n",
    "    #Get the hyperparameters for alpha and eta from mr_to_2 based on max coherence\n",
    "    max_co_2 = mr_to_2['Coherence'].max()\n",
    "    mr_to_3_5 = mr_to_2.loc[mr_to_2['Coherence'] == max_co_2]\n",
    "    mr_to_3 = mr_to_3_5.reset_index(drop = True)\n",
    "    #Convert mr_to_3, the optimal hyperparameters to a list \n",
    "    hyper_list_ko = [mr_to_3[\"Topics\"][0], mr_to_3[\"Alpha\"][0], mr_to_3[\"Beta\"][0]]\n",
    "    print(hyper_list_ko)\n",
    "    return hyper_list_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 66: Implement the stopwords, bigrams, and lemma functions Japanese\n",
    "def build_lda_ko(df_ko, final_stop_words_ko, hyper_list_ko):\n",
    "    #Remove stops words\n",
    "    data_words_nostops = stopwords_ko(df_ko['text'], final_stop_words_ko)\n",
    "    #Create the bigram from the non stop words\n",
    "    #data_words_bigram = bigrams(data_words_nostops)\n",
    "    #Do lemmatization keeping only noun, adj, vb, adv, the lemmatization cuts off the ends of words so they can be\n",
    "    #grouped an analyze better\n",
    "    #data_lemma = data_lemmatization_ko(data_words_bigram)\n",
    "    #Create the dictionary, corpus, and term document matrix\n",
    "    #Dictionary\n",
    "    #id2word = corpora.Dictionary([str(data_lemma).split(' ')])\n",
    "    id2word = corpora.Dictionary(data_words_nostops)\n",
    "    #Corpus\n",
    "    texts = data_words_nostops\n",
    "    #Term Document Matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    #Train the actual LDA model\n",
    "    #Watch out for too many topics\n",
    "    lda_model_ko = gensim.models.LdaMulticore(corpus = corpus,\n",
    "                                              id2word = id2word,\n",
    "                                              num_topics = hyper_list_ko[0],\n",
    "                                              random_state = 105,\n",
    "                                              chunksize = 100,\n",
    "                                              passes = 10,\n",
    "                                              alpha = hyper_list_ko[1],\n",
    "                                              eta = hyper_list_ko[2],\n",
    "                                              per_word_topics = True,\n",
    "                                              minimum_probability = 0)\n",
    "    \n",
    "    #Create the weights dataframe\n",
    "    #Extract individual document topic proportions as determined by the LDA model. Our Gensim LDA model can classify \n",
    "    #the specific relative proportions for all ten topics within each document as long as you set the minimum_probability\n",
    "    #argument to 0. If you did not do this, then some topics may be dropped from the final weighting if they did not \n",
    "    #meet the probability threshold set by default.\n",
    "    weights_output = pd.DataFrame(columns = ['topic', 'prob_weight', 'doc_id'])\n",
    "    \n",
    "    #Extraction Loop: This loop extracts the topic proportions for all five topics for every individual document and\n",
    "    #places them into a dataframe with a document-id key for merging topic proportion information with other datasets\n",
    "    #about our corpus\n",
    "    for i in range(0, len(corpus)):\n",
    "        doc_weights = lda_model_ko[corpus[i]][0]\n",
    "        weights_df = pd.DataFrame(list(doc_weights), columns = ['topic', 'prob_weight'])\n",
    "        weights_df['doc_id'] = i\n",
    "        weights_output = weights_output.append(weights_df)\n",
    "    \n",
    "    #Create the daily (or hourly) weights data\n",
    "    df2 = df_ko\n",
    "    df = weights_output\n",
    "    \n",
    "    #Create new dataset from the speechs with doc_id\n",
    "    df3 = df2.reset_index()\n",
    "    df3['doc_id'] = df3.index\n",
    "    \n",
    "    #Merge the Two Dataframe Together\n",
    "    df4 = pd.merge(df, df3[['doc_id', 'Day', 'text']], on = 'doc_id', how = 'left')\n",
    "    \n",
    "    #Get the count of the total documents by Minute\n",
    "    # This should be changed to Hour if I decide to do a full day of tweets instead\n",
    "    total_docs = df4.groupby('Day')['doc_id'].apply(lambda x: len(x.unique())).reset_index()\n",
    "    \n",
    "    #Label total_docs columns\n",
    "    total_docs.columns = ['Day', 'total_docs']\n",
    "    \n",
    "    #Get the Probability weight per Month and Topic \n",
    "    df_avg = df4.groupby(['Day', 'topic']).agg({'prob_weight': 'sum'}).reset_index()\n",
    "    \n",
    "    #Combine the prob_weight and the total_docs data frames\n",
    "    df_avg2 = df_avg.merge(total_docs, on = 'Day', how = 'left')\n",
    "    \n",
    "    #Create the Average Weight of each Day and Topic\n",
    "    df_avg2['average_weight'] = df_avg2['prob_weight'] / df_avg2['total_docs']\n",
    "    \n",
    "    #Get the Keywords from each Topics from the LDA Topic and Automatically Label them\n",
    "    printtopics2 = lda_model_ko.print_topics()\n",
    "    lenko2 = len(printtopics2)\n",
    "    topic_label_list = []\n",
    "    #For All the topics in generated by the model\n",
    "    for i in range(0, lenko2):\n",
    "        ko_list = printtopics2[i][1].split('*')\n",
    "        ko_list_words = []\n",
    "        lenkol = len(ko_list)\n",
    "        #Split the string list in the loop to get the first 3 topic words\n",
    "        for j in range(1, 6):\n",
    "            t_1 = ko_list[j]\n",
    "            t_2 = t_1.split('+')\n",
    "            t_3 = t_2[0]\n",
    "            ko_list_words.append(t_3)\n",
    "        topic_label_list.append(ko_list_words)\n",
    "        \n",
    "    #Set the Topic Labels to topic_label_list\n",
    "    topic_labels = topic_label_list\n",
    "    \n",
    "    #Translate the Topic Label \n",
    "    tll_translate = []\n",
    "    lentll = len(topic_label_list)\n",
    "    for i in range(0, lentll):\n",
    "        tll_topic_line = []\n",
    "        lent_t_l = len(topic_label_list[i])\n",
    "        for j in range(0, lent_t_l):\n",
    "            try:\n",
    "                text = topic_label_list[i][j]\n",
    "                translated = GoogleTranslator(source = \"korean\", to_lang = \"english\").translate(text=text)\n",
    "                tll_topic_line.append(translated)\n",
    "            except:\n",
    "                translated = \"\"\n",
    "                tll_topic_line.append(translated)\n",
    "        tll_translate.append(tll_topic_line)\n",
    "        \n",
    "    #Topic Label Translated \n",
    "    topic_labels_translate = tll_translate\n",
    "    \n",
    "    #Create topic_id numbers based on the createList function\n",
    "    lenpt3 = len(printtopics2)\n",
    "    topic_id = createList(lenpt3)\n",
    "    \n",
    "    #Combine the topic_id and topic_label\n",
    "    data_tuple = list(zip(topic_id, topic_labels_translate))\n",
    "    \n",
    "    #Convert into a dataframe\n",
    "    df_labels = pd.DataFrame(data_tuple, columns = ['topic', 'topic_label'])\n",
    "    \n",
    "    #Merge labels into year weights data\n",
    "    df_avg3 = df_avg2.merge(df_labels, on = 'topic')\n",
    "    \n",
    "    #Create the final per-document dataframe for broader analysis\n",
    "    #Make sure to change on = [\"Minute\"] if want to use a different time scale\n",
    "    df11_ko = pd.merge(df4, df_avg3[['Day', 'topic', 'average_weight', 'total_docs', 'topic_label']], \n",
    "                    on = ['Day', 'topic'], how = 'left')\n",
    "    \n",
    "    return df11_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 67: Visualization of Topics over Time Japanese\n",
    "#https://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib\n",
    "#https://stackoverflow.com/questions/12560600/creating-a-new-file-filename-contains-loop-variable-python\n",
    "#https://stackoverflow.com/questions/33907776/how-to-create-an-array-of-dataframes-in-python\n",
    "def viz_topic_time_ko(df, hyper_list_ko):\n",
    "    \n",
    "    #Split Data into individual topics\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_ko[0]):\n",
    "        df_1_5 = df[df[\"topic\"] == i]\n",
    "        df_1 = df_1_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_1[\"topic_label\"][0])\n",
    "        topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "  \n",
    "    #Change the size of the Plot\n",
    "    plt.rcParams['figure.figsize'] = [20, 14]\n",
    "    \n",
    "    #Get the colors for the lines\n",
    "    num_colors = hyper_list_ko[0]\n",
    "    \n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                for i in range(0, num_colors)]\n",
    "    \n",
    "    #Create the plot\n",
    "    #Change Legends based on Topic Labels, Plot the topic changes over time and colors\n",
    "    for i in topic_dfs.keys():\n",
    "        plt.plot(topic_dfs[i], color = color[i])\n",
    "    plt.xlim(26, 31)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axhline(df['average_weight'].median(), color = \"black\")\n",
    "    plt.title(\"Change in Korean Topics\")\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Average Day Topic Weight\")\n",
    "    plt.legend((topic_label_list))\n",
    "    plt.grid()\n",
    "    plt.savefig(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/Output_KO_\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Getting the Topic Values over Time \n",
    "def topic_value_ko(df, hyper_list_ko):\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_ko[0]):\n",
    "        df_5 = df[df[\"topic\"] == i]\n",
    "        df_2 = df_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_2[\"topic_label\"][0])\n",
    "        topic_plots = df_2.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "    dataframe2 = pd.DataFrame(topic_dfs)\n",
    "    df2_t = dataframe2.T\n",
    "    df2_t[\"Topic_label\"] = topic_label_list\n",
    "    df2_t.to_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 28/DataFrames/Korean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 54: The Main Function\n",
    "def main_tm(full_df2):\n",
    "    tmfull2 = full_df2\n",
    "    #tmfull2 = lang_loop(tmfull)\n",
    "    print(tmfull2.tail())\n",
    "    eng_tweet = English_Tweets_Func(tmfull2)\n",
    "    #esp_tweet = Spanish_Tweets_Func(tmfull2)\n",
    "    #frn_tweet = French_Tweets_Func(tmfull2)\n",
    "    #prt_tweet = Portuguese_Tweets_Func(tmfull2)\n",
    "    #arb_tweet = Arabic_Tweets_Func(tmfull2)\n",
    "    #jpn_tweet = Japanese_Tweets_Func(tmfull2)\n",
    "    #kor_tweet = Korean_Tweets_Func(tmfull2)\n",
    "    eng_t2 = sort_chrono(eng_tweet)\n",
    "    #esp_t2 = sort_chrono(esp_tweet)\n",
    "    #frn_t2 = sort_chrono(frn_tweet)\n",
    "    #prt_t2 = sort_chrono(prt_tweet)\n",
    "    #arb_t2 = sort_chrono(arb_tweet)\n",
    "    #jpn_t2 = sort_chrono(jpn_tweet)\n",
    "    #kor_t2 = sort_chrono(kor_tweet)\n",
    "    final_stop_words_en = stopwords_en_func()\n",
    "    #final_stop_words_sp = stopwords_sp_func()\n",
    "    #final_stop_words_fr = stopwords_fr_func()\n",
    "    #final_stop_words_pt = stopwords_pt_func()\n",
    "    #final_stop_words_ar = stopwords_ar_func()\n",
    "    #final_stop_words_ja = stopwords_ja_func()\n",
    "    #final_stop_words_ko = stopwords_ko_func()\n",
    "    mr_en = lda_hyperparameter_generating_en(eng_t2, final_stop_words_en)\n",
    "    #mr_sp = lda_hyperparameter_generating_sp(esp_t2, final_stop_words_sp)\n",
    "    #mr_fr = lda_hyperparameter_generating_fr(frn_t2, final_stop_words_fr)\n",
    "    #mr_pt = lda_hyperparameter_generating_pt(prt_t2, final_stop_words_pt)\n",
    "    #mr_ar = lda_hyperparameter_generating_ar(arb_t2, final_stop_words_ar)\n",
    "    #mr_ja = lda_hyperparameter_generating_ja(jpn_t2, final_stop_words_ja)\n",
    "    #mr_ko = lda_hyperparameter_generating_ko(kor_t2, final_stop_words_ko)\n",
    "    hyper_list_en = lda_hyper_define_en(mr_en)\n",
    "    #hyper_list_sp = lda_hyper_define_sp(mr_sp)\n",
    "    #hyper_list_fr = lda_hyper_define_fr(mr_fr)\n",
    "    #hyper_list_pt = lda_hyper_define_pt(mr_pt)\n",
    "    #hyper_list_ar = lda_hyper_define_ar(mr_ar)\n",
    "    #hyper_list_ja = lda_hyper_define_ja(mr_ja)\n",
    "    #hyper_list_ko = lda_hyper_define_ko(mr_ko)\n",
    "    en_tm_final = build_lda_en(eng_t2, final_stop_words_en, hyper_list_en)\n",
    "    #sp_tm_final = build_lda_sp(esp_t2, final_stop_words_sp, hyper_list_sp)\n",
    "    #fr_tm_final = build_lda_fr(frn_t2, final_stop_words_fr, hyper_list_fr)\n",
    "    #pt_tm_final = build_lda_pt(prt_t2, final_stop_words_pt, hyper_list_pt)\n",
    "    #ar_tm_final = build_lda_ar(arb_t2, final_stop_words_ar, hyper_list_ar)\n",
    "    #ja_tm_final = build_lda_ja(jpn_t2, final_stop_words_ja, hyper_list_ja)\n",
    "    #ko_tm_final = build_lda_ko(kor_t2, final_stop_words_ko, hyper_list_ko)\n",
    "    viz_topic_time_en(en_tm_final, hyper_list_en)\n",
    "    #viz_topic_time_sp(sp_tm_final, hyper_list_sp)\n",
    "    #viz_topic_time_fr(fr_tm_final, hyper_list_fr)\n",
    "    #viz_topic_time_pt(pt_tm_final, hyper_list_pt)\n",
    "    #viz_topic_time_ar(ar_tm_final, hyper_list_ar)\n",
    "    #viz_topic_time_ja(ja_tm_final, hyper_list_ja)\n",
    "    #viz_topic_time_ko(ko_tm_final, hyper_list_ko)\n",
    "    topic_value_en(en_tm_final, hyper_list_en)\n",
    "    #topic_value_sp(sp_tm_final, hyper_list_sp)\n",
    "    #topic_value_fr(fr_tm_final, hyper_list_fr)\n",
    "    #topic_value_pt(pt_tm_final, hyper_list_pt)\n",
    "    #topic_value_ar(ar_tm_final, hyper_list_ar)\n",
    "    #topic_value_ja(ja_tm_final, hyper_list_ja)\n",
    "    #topic_value_ko(ko_tm_final, hyper_list_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-11 17:35:47.111509\n",
      "                                                    text lang_x  \\\n",
      "49314  RT @JuliaDavisNews: The White House unveiled a...     en   \n",
      "49315  RT @anders_aslund: After Putin's assault on Uk...     en   \n",
      "49316  RT @KyivIndependent: ‚ö°Ô∏è Polish media: Spanish ...     en   \n",
      "49317  RT @KyivIndependent: ‚ö°Ô∏è General Staff: Russia ...     en   \n",
      "49318  RT @tomiahonen: Russia War Costs Thread 1/\\n\\n...     en   \n",
      "\n",
      "               created_at tag                  cat  scale  Year  Month  Day  \\\n",
      "49314 2022-04-28 16:09:33  GN    military invasion  -11.0  2022      4   28   \n",
      "49315 2022-04-29 19:30:23  GN     military assault  -10.0  2022      4   29   \n",
      "49316 2022-04-28 23:22:40  GP  military assistance    8.3  2022      4   28   \n",
      "49317 2022-04-28 08:32:14  GN      military attack  -10.0  2022      4   28   \n",
      "49318 2022-04-30 21:36:30  GN    military invasion  -11.0  2022      4   30   \n",
      "\n",
      "       Month_Day  TweetNumber  Sentiment  textloc label textloc_en  \n",
      "49314        148         3449          1     None  None       None  \n",
      "49315        149         4177         -1  Ukraine   GPE    Ukraine  \n",
      "49316        148        16838          0  Ukraine   GPE    Ukraine  \n",
      "49317        148        33387          0  Ukraine   GPE    Ukraine  \n",
      "49318        150        94461         -1  Ukraine   GPE    Ukraine  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: Dropping invalid columns in DataFrameGroupBy.max is deprecated. In a future version, a TypeError will be raised. Before calling .max, select only columns which should be valid for the function.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 0.9099999999999999, 0.61]\n",
      "2023-04-11 19:32:43.878308\n",
      "1:56:56.766799\n"
     ]
    }
   ],
   "source": [
    "#Call the Main_tm function\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "print(start)\n",
    "main_tm(dfw1en)\n",
    "end = datetime.now()\n",
    "print(end)\n",
    "diff = end - start\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang_x</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>scale</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month_Day</th>\n",
       "      <th>TweetNumber</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>textloc</th>\n",
       "      <th>label</th>\n",
       "      <th>textloc_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21113</th>\n",
       "      <td>RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-27 08:47:08</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>„É≠„Ç∑„Ç¢</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21114</th>\n",
       "      <td>RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-27 08:46:21</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>„É≠„Ç∑„Ç¢</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21115</th>\n",
       "      <td>RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-27 07:37:21</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>„É≠„Ç∑„Ç¢</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21116</th>\n",
       "      <td>RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-27 06:15:42</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>„É≠„Ç∑„Ç¢</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21117</th>\n",
       "      <td>@Squallfang @nippon_ukuraina „ÄåMilitary assista...</td>\n",
       "      <td>ja</td>\n",
       "      <td>2022-04-27 03:58:41</td>\n",
       "      <td>GN</td>\n",
       "      <td>military invasion</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>147</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Êó•Êú¨</td>\n",
       "      <td>GPE</td>\n",
       "      <td>Japan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text lang_x  \\\n",
       "21113  RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...     ja   \n",
       "21114  RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...     ja   \n",
       "21115  RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...     ja   \n",
       "21116  RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...     ja   \n",
       "21117  @Squallfang @nippon_ukuraina „ÄåMilitary assista...     ja   \n",
       "\n",
       "               created_at tag                cat  scale  Year  Month  Day  \\\n",
       "21113 2022-04-27 08:47:08  GN  military invasion  -11.0  2022      4   27   \n",
       "21114 2022-04-27 08:46:21  GN  military invasion  -11.0  2022      4   27   \n",
       "21115 2022-04-27 07:37:21  GN  military invasion  -11.0  2022      4   27   \n",
       "21116 2022-04-27 06:15:42  GN  military invasion  -11.0  2022      4   27   \n",
       "21117 2022-04-27 03:58:41  GN  military invasion  -11.0  2022      4   27   \n",
       "\n",
       "       Month_Day  TweetNumber  Sentiment textloc label textloc_en  \n",
       "21113        147            1          1     „É≠„Ç∑„Ç¢   GPE     Russia  \n",
       "21114        147            2          1     „É≠„Ç∑„Ç¢   GPE     Russia  \n",
       "21115        147            3          1     „É≠„Ç∑„Ç¢   GPE     Russia  \n",
       "21116        147            4          1     „É≠„Ç∑„Ç¢   GPE     Russia  \n",
       "21117        147            5          1      Êó•Êú¨   GPE      Japan  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only Spanish\n",
    "#df = df.drop(df[df.score < 50].index)\n",
    "df_week1_2_sp = df_week1.drop(df_week1[df_week1[\"lang_x\"] == \"en\"].index)\n",
    "df_week1_2_2_sp = df_week1_2_sp.reset_index(drop = True)\n",
    "df_week1_2_2_sp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "es    10338\n",
       "fr     5107\n",
       "ko     1837\n",
       "ja     1833\n",
       "ar     1043\n",
       "pt      960\n",
       "Name: lang_x, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_week1_2_2_sp[\"lang_x\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 54: The Main Function\n",
    "def main_tm2(full_df2):\n",
    "    tmfull2 = full_df2\n",
    "    #tmfull2 = lang_loop(tmfull)\n",
    "    print(tmfull2.tail())\n",
    "    #eng_tweet = English_Tweets_Func(tmfull2)\n",
    "    esp_tweet = Spanish_Tweets_Func(tmfull2)\n",
    "    frn_tweet = French_Tweets_Func(tmfull2)\n",
    "    prt_tweet = Portuguese_Tweets_Func(tmfull2)\n",
    "    arb_tweet = Arabic_Tweets_Func(tmfull2)\n",
    "    jpn_tweet = Japanese_Tweets_Func(tmfull2)\n",
    "    kor_tweet = Korean_Tweets_Func(tmfull2)\n",
    "    #eng_t2 = sort_chrono(eng_tweet)\n",
    "    esp_t2 = sort_chrono(esp_tweet)\n",
    "    frn_t2 = sort_chrono(frn_tweet)\n",
    "    prt_t2 = sort_chrono(prt_tweet)\n",
    "    arb_t2 = sort_chrono(arb_tweet)\n",
    "    jpn_t2 = sort_chrono(jpn_tweet)\n",
    "    kor_t2 = sort_chrono(kor_tweet)\n",
    "    #final_stop_words_en = stopwords_en_func()\n",
    "    final_stop_words_sp = stopwords_sp_func()\n",
    "    final_stop_words_fr = stopwords_fr_func()\n",
    "    final_stop_words_pt = stopwords_pt_func()\n",
    "    final_stop_words_ar = stopwords_ar_func()\n",
    "    final_stop_words_ja = stopwords_ja_func()\n",
    "    final_stop_words_ko = stopwords_ko_func()\n",
    "    #mr_en = lda_hyperparameter_generating_en(eng_t2, final_stop_words_en)\n",
    "    mr_sp = lda_hyperparameter_generating_sp(esp_t2, final_stop_words_sp)\n",
    "    mr_fr = lda_hyperparameter_generating_fr(frn_t2, final_stop_words_fr)\n",
    "    mr_pt = lda_hyperparameter_generating_pt(prt_t2, final_stop_words_pt)\n",
    "    mr_ar = lda_hyperparameter_generating_ar(arb_t2, final_stop_words_ar)\n",
    "    mr_ja = lda_hyperparameter_generating_ja(jpn_t2, final_stop_words_ja)\n",
    "    mr_ko = lda_hyperparameter_generating_ko(kor_t2, final_stop_words_ko)\n",
    "    #hyper_list_en = lda_hyper_define_en(mr_en)\n",
    "    hyper_list_sp = lda_hyper_define_sp(mr_sp)\n",
    "    hyper_list_fr = lda_hyper_define_fr(mr_fr)\n",
    "    hyper_list_pt = lda_hyper_define_pt(mr_pt)\n",
    "    hyper_list_ar = lda_hyper_define_ar(mr_ar)\n",
    "    hyper_list_ja = lda_hyper_define_ja(mr_ja)\n",
    "    hyper_list_ko = lda_hyper_define_ko(mr_ko)\n",
    "    #en_tm_final = build_lda_en(eng_t2, final_stop_words_en, hyper_list_en)\n",
    "    sp_tm_final = build_lda_sp(esp_t2, final_stop_words_sp, hyper_list_sp)\n",
    "    fr_tm_final = build_lda_fr(frn_t2, final_stop_words_fr, hyper_list_fr)\n",
    "    pt_tm_final = build_lda_pt(prt_t2, final_stop_words_pt, hyper_list_pt)\n",
    "    ar_tm_final = build_lda_ar(arb_t2, final_stop_words_ar, hyper_list_ar)\n",
    "    ja_tm_final = build_lda_ja(jpn_t2, final_stop_words_ja, hyper_list_ja)\n",
    "    ko_tm_final = build_lda_ko(kor_t2, final_stop_words_ko, hyper_list_ko)\n",
    "    #viz_topic_time_en(en_tm_final, hyper_list_en)\n",
    "    viz_topic_time_sp(sp_tm_final, hyper_list_sp)\n",
    "    viz_topic_time_fr(fr_tm_final, hyper_list_fr)\n",
    "    viz_topic_time_pt(pt_tm_final, hyper_list_pt)\n",
    "    viz_topic_time_ar(ar_tm_final, hyper_list_ar)\n",
    "    viz_topic_time_ja(ja_tm_final, hyper_list_ja)\n",
    "    viz_topic_time_ko(ko_tm_final, hyper_list_ko)\n",
    "    #topic_value_en(en_tm_final, hyper_list_en)\n",
    "    topic_value_sp(sp_tm_final, hyper_list_sp)\n",
    "    topic_value_fr(fr_tm_final, hyper_list_fr)\n",
    "    topic_value_pt(pt_tm_final, hyper_list_pt)\n",
    "    topic_value_ar(ar_tm_final, hyper_list_ar)\n",
    "    topic_value_ja(ja_tm_final, hyper_list_ja)\n",
    "    topic_value_ko(ko_tm_final, hyper_list_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-11 20:02:43.919742\n",
      "                                                    text lang_x  \\\n",
      "21113  RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...     ja   \n",
      "21114  RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...     ja   \n",
      "21115  RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...     ja   \n",
      "21116  RT @Tamama0306: „Åì„ÅÆÁî∑„ÅØ„Äå„Ç¶„ÇØ„É©„Ç§„Éä„ÅÆ„Åü„ÇÅ„Å´Êà¶„ÅÜ„Äç„Åü„ÇÅ„Å´Â®ò„Å´Âà•„Çå„ÇíÂëä„Åí„Å¶„ÅÑ„Çã...     ja   \n",
      "21117  @Squallfang @nippon_ukuraina „ÄåMilitary assista...     ja   \n",
      "\n",
      "               created_at tag                cat  scale  Year  Month  Day  \\\n",
      "21113 2022-04-27 08:47:08  GN  military invasion  -11.0  2022      4   27   \n",
      "21114 2022-04-27 08:46:21  GN  military invasion  -11.0  2022      4   27   \n",
      "21115 2022-04-27 07:37:21  GN  military invasion  -11.0  2022      4   27   \n",
      "21116 2022-04-27 06:15:42  GN  military invasion  -11.0  2022      4   27   \n",
      "21117 2022-04-27 03:58:41  GN  military invasion  -11.0  2022      4   27   \n",
      "\n",
      "       Month_Day  TweetNumber  Sentiment textloc label textloc_en  \n",
      "21113        147            1          1     „É≠„Ç∑„Ç¢   GPE     Russia  \n",
      "21114        147            2          1     „É≠„Ç∑„Ç¢   GPE     Russia  \n",
      "21115        147            3          1     „É≠„Ç∑„Ç¢   GPE     Russia  \n",
      "21116        147            4          1     „É≠„Ç∑„Ç¢   GPE     Russia  \n",
      "21117        147            5          1      Êó•Êú¨   GPE      Japan  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: Dropping invalid columns in DataFrameGroupBy.max is deprecated. In a future version, a TypeError will be raised. Before calling .max, select only columns which should be valid for the function.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 0.61, 0.01]\n",
      "[10, 0.61, 0.01]\n",
      "[10, 'asymmetric', 'symmetric']\n",
      "[10, 0.9099999999999999, 0.9099999999999999]\n",
      "[10, 'symmetric', 0.9099999999999999]\n",
      "[10, 'asymmetric', 0.9099999999999999]\n",
      "2023-04-11 20:34:07.114429\n",
      "0:31:23.194687\n"
     ]
    }
   ],
   "source": [
    "#Call the Main_tm2 function\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "print(start)\n",
    "main_tm2(df_week1_2_2_sp)\n",
    "end = datetime.now()\n",
    "print(end)\n",
    "diff = end - start\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORK AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-a0c74daf757b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test1' is not defined"
     ]
    }
   ],
   "source": [
    "test1.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dfs = {}\n",
    "topic_label_list = []\n",
    "for i in range(0, 9):\n",
    "    test1_5 = test1[test1[\"topic\"] == i]\n",
    "    df_1 = test1_5.reset_index(drop = True)\n",
    "    topic_label_list.append(df_1[\"topic_label\"][0])\n",
    "    topic_plots = df_1.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "    topic_dfs[i] = topic_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(topic_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1 = pd.DataFrame(topic_dfs)\n",
    "dataframe1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_t = dataframe1.T\n",
    "df1_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_t[\"Topic_label\"] = topic_label_list\n",
    "df1_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_value_en(df, hyper_list_en):\n",
    "    topic_dfs = {}\n",
    "    topic_label_list = []\n",
    "    for i in range(0, hyper_list_en[0]):\n",
    "        df_5 = df[df[\"topic\"] == i]\n",
    "        df_2 = df_5.reset_index(drop = True)\n",
    "        topic_label_list.append(df_2[\"topic_label\"][0])\n",
    "        topic_plots = df_2.groupby(\"Day\")[\"average_weight\"].mean()\n",
    "        topic_dfs[i] = topic_plots\n",
    "    dataframe2 = pd.DataFrame(topic_dfs)\n",
    "    df2_t = dataframe2.T\n",
    "    df2_t[\"Topic_label\"] = topic_label_list\n",
    "    df2_t.to_csv(\"/Users/johnc.burns/Documents/Documents/PhD Year Two/My Paper 4/Topic Models/Week 1/DataFrames/English.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
